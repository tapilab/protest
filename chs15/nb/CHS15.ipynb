{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains all code to reproduce results in the paper \"Analyzing e-cigarette sentiment of Twitter\", published in the Computational Health Sciences Workshop at the 6th ACM Conference on Bioinformatics, Computational Biology, and Health Informatics, 2015."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We begin with the data collected by Sherry Emery et al. <slemery@uic.edu>, which consists 4.6M tweets from 2012-10-01 to 2013-09-30. These tweets have already been classified as \"organic\" or not using an SVM classifier (see Huang, Jidong et al. \"A cross-sectional examination of marketing of electronic cigarettes on Twitter.\" Tobacco control). We restrict our analysis to those classified as organic. We will assume these data live in `/data/chs15/ecig.csv.gz`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter, defaultdict\n",
    "import cPickle\n",
    "import csv\n",
    "import datetime\n",
    "import gzip\n",
    "import itertools\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.lines import Line2D\n",
    "import numpy as np\n",
    "import re\n",
    "import requests\n",
    "from sklearn.cross_validation import cross_val_score, KFold\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tabulate import tabulate\n",
    "\n",
    "DATA = '/data/chs15'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_csv(filename, fields=['text', 'svm', 'hand_label', 'posted_time', 'real_name', 'username']):\n",
    "    f = gzip.open(filename, 'rb')\n",
    "    csvr = csv.DictReader(f, delimiter=',', quotechar='\"')\n",
    "    for row in csvr:\n",
    "        yield dict([(k, row[k]) for k in fields if k in row])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read 992633 \"organic\" tweets\n"
     ]
    }
   ],
   "source": [
    "# Read all \"organic\" tweets.\n",
    "raw_tweets = [r for r in read_csv(DATA + '/ecig.csv.gz') if r['svm'] == '-']\n",
    "print('read %d \"organic\" tweets' % len(raw_tweets))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we manually lableed 2,000 tweets into one of three categories:\n",
    "- **positive (1):** express positive sentiment toward ecigs, or indicate that the speaker uses ecigs.\n",
    "- **negative (0):** express negative sentiment toward ecigs *or* do not express sentiment, e.g., informative\n",
    "\n",
    "(We originally separated negative into two classes, but accuracy was too low to support further analysis.)\n",
    "\n",
    "We assume these data live in `/data/chs15/labeled.csv.gz`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read 2000 labeled tweets\n",
      "Label distribution=[('negative', 1296), ('positive', 704)]\n"
     ]
    }
   ],
   "source": [
    "labeled_tweets = [r for r in read_csv(DATA + '/labeled.csv.gz', fields=['text', 'sent', 'real_name', 'username'])]\n",
    "\n",
    "print('read %d labeled tweets' % len(labeled_tweets))\n",
    "# Set labels.\n",
    "\n",
    "label_map = {'-1': 'negative', '0': 'negative', '1': 'positive'}\n",
    "for t in labeled_tweets:\n",
    "    t['sent'] = label_map[t['sent']]\n",
    "             \n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(['negative', 'positive'])\n",
    "y = label_encoder.transform([t['sent'] for t in labeled_tweets])             \n",
    "print('Label distribution=%s' % Counter(t['sent'] for t in labeled_tweets).most_common(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifier Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using `labeled_tweets`, we'll train a logistic regression classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Tweet tokenizer.\n",
    "def tokenize(text):\n",
    "    punc_re = '[' + re.escape(string.punctuation) + ']'\n",
    "    text = text.lower()\n",
    "    text = re.sub('#(\\S+)', r'HASHTAG_\\1', text)\n",
    "    text = re.sub('@(\\S+)', r'MENTION_\\1', text)\n",
    "    text = re.sub('http\\S+', 'THIS_IS_A_URL', text)\n",
    "    text = re.sub(r'(.)\\1\\1\\1+', r'\\1', text)\n",
    "    text = re.sub(r'[0-9]', '9', text)\n",
    "    toks = []\n",
    "    for tok in text.split():\n",
    "        tok = re.sub(r'^(' + punc_re + '+)', r'\\1 ', tok)\n",
    "        tok = re.sub(r'(' + punc_re + '+)$', r' \\1', tok)\n",
    "        for subtok in tok.split():\n",
    "            if re.search('\\w', subtok):\n",
    "                toks.append(subtok)\n",
    "    return toks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorized 2000 tweets. Found 4824 terms.\n"
     ]
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(decode_error='ignore', ngram_range=(1, 2), max_df=1., min_df=2,\n",
    "                             use_idf=True, tokenizer=tokenize, binary=False, norm='l2')\n",
    "X = vectorizer.fit_transform(t['text'] for t in labeled_tweets)\n",
    "print('Vectorized %d tweets. Found %d terms.' % (X_labeled.shape[0], X.shape[1]))\n",
    "features = np.array(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy=0.793\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "   negative       0.84      0.83      0.84      1296\n",
      "   positive       0.70      0.72      0.71       704\n",
      "\n",
      "avg / total       0.79      0.79      0.79      2000\n",
      "\n",
      "            negative    positive\n",
      "--------  ----------  ----------\n",
      "negative        1082         214\n",
      "positive         199         505\n",
      "\n",
      "CLASS 0\n",
      "my\t5.390\n",
      "i\t4.668\n",
      "vaping\t2.160\n",
      "HASHTAG_vaping\t1.722\n",
      "HASHTAG_ecig\t1.663\n",
      "\n",
      "CLASS 1\n",
      "THIS_IS_A_URL\t4.017\n",
      "e-cigarettes\t1.940\n",
      "de\t1.171\n",
      "as\t1.164\n",
      "99\t1.112\n"
     ]
    }
   ],
   "source": [
    "def confusion(truths, preds, labels):\n",
    "    m = confusion_matrix(truths, preds)\n",
    "    m = np.vstack((labels, m))\n",
    "    m = np.hstack((np.matrix([''] + list(labels)).T, m))\n",
    "    return tabulate(m.tolist(), headers='firstrow')\n",
    "\n",
    "def top_coef(clf, vocab, n=10):\n",
    "    if len(clf.classes_) == 2:\n",
    "        coefs = [clf.coef_[0], -clf.coef_[0]]\n",
    "    else:\n",
    "        coefs = clf.coef_\n",
    "    for li, label in enumerate(clf.classes_):\n",
    "        print('\\nCLASS %s' % label)\n",
    "        coef = coefs[li]\n",
    "        top_coef_ind = np.argsort(coef)[::-1][:n]\n",
    "        top_coef_terms = vocab[top_coef_ind]\n",
    "        top_coef = coef[top_coef_ind]\n",
    "        print '\\n'.join(['%s\\t%.3f' % (term, weight) for term, weight in zip(top_coef_terms, top_coef)])\n",
    "\n",
    "def do_cv(X, y, labels, nfolds=10):\n",
    "    cv = KFold(len(y), nfolds, random_state=123456)\n",
    "    preds = []\n",
    "    truths = []\n",
    "    for train, test in cv:\n",
    "        clf = LogisticRegression(class_weight='auto')\n",
    "        clf.fit(X[train], y[train])\n",
    "        preds.extend(clf.predict(X[test]))\n",
    "        truths.extend(y[test])\n",
    "    print ('accuracy=%.3f' % (accuracy_score(truths, preds)))\n",
    "    print classification_report(truths, preds, target_names=labels)\n",
    "    print confusion(truths, preds, labels)\n",
    "    clf = LogisticRegression(class_weight='auto')\n",
    "    clf.fit(X, y)\n",
    "    return clf, truths, preds\n",
    "\n",
    "clf, truths, preds = do_cv(X, y, label_encoder.classes_, 10)\n",
    "top_coef(clf, features, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Write cross-validation table.\n",
    "def clfreport_to_tex(report, outfile):\n",
    "    \"\"\" Write a sklearn classification report as a latex table. \"\"\"\n",
    "    report = re.sub(r' \\/ total', '', report)\n",
    "    report = re.sub(r'precision', 'Prec', report)\n",
    "    report = re.sub(r'recall', 'Rec', report)\n",
    "    report = re.sub(r'f1-score', 'F1', report)\n",
    "    report = re.sub(r'support', 'N', report)\n",
    "    table = ['\\\\begin{tabular}{|r|c|c|c|c|}', '\\\\hline']\n",
    "    lines = report.split('\\n')\n",
    "    for i, line in enumerate(lines):\n",
    "        parts = line.strip().split()\n",
    "        if len(parts) > 0:\n",
    "            if i == 0:\n",
    "                parts = [''] + ['{\\\\bf %s}' % p for p in parts]\n",
    "            else:\n",
    "                parts[0] = '{\\\\bf %s}' % parts[0]\n",
    "            table.append(' & '.join(parts) + '\\\\\\\\')\n",
    "        else:\n",
    "            table.append('\\\\hline')\n",
    "    table.append('\\\\end{tabular}')\n",
    "    of = open(outfile, 'wt')\n",
    "    of.write('\\n'.join(table))\n",
    "    \n",
    "clfreport_to_tex(classification_report(truths, preds, target_names=labels), 'cv.tex')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Save top coef table.\n",
    "def clean(s):\n",
    "    s = re.sub('HASHTAG_', '\\#', s)\n",
    "    s = re.sub('MENTION_', '@', s)\n",
    "    s = re.sub('THIS_IS_A_URL', 'URL', s)\n",
    "    s = re.sub(r'_', '\\\\_', s)\n",
    "    return s\n",
    "    \n",
    "def write_top_coef(clf, vocab, labels, outf, n=20):\n",
    "    out = open(outf, 'wt')\n",
    "    coefs = [clf.coef_[0], -clf.coef_[0]]\n",
    "    for li, label in enumerate(labels):\n",
    "        coef = coefs[li]\n",
    "        top_coef_ind = np.argsort(coef)[::-1][:n]\n",
    "        top_coef_terms = vocab[top_coef_ind]\n",
    "        out.write('{\\\\bf %s} & %s\\\\\\\\\\n\\hline\\n' % (label, ', '.join(clean(s) for s in top_coef_terms)))\n",
    "        \n",
    "write_top_coef(clf, features, labels, 'coef.tex', n=20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we classify all the unlabeled tweets using the classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_raw = vectorizer.transform(t['text'] for t in raw_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted label distribution on raw tweets: [(0, 623396), (1, 369237)]\n"
     ]
    }
   ],
   "source": [
    "preds_raw = clf.predict(X_raw)\n",
    "print('predicted label distribution on raw tweets: %s' % Counter(preds_raw).most_common(2))\n",
    "for tweet, pred in zip(raw_tweets, preds_raw):\n",
    "    t['sent'] = labels[pred]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gender"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we classify each user by gender using a list of names from the census."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found 232 male and 523 female names with cutoff=75.00\n",
      "removed 6 ambiguous names, leaving 226 males and 517 females\n"
     ]
    }
   ],
   "source": [
    "def get_gender_names(cutoff=75):\n",
    "    males_url = 'http://www2.census.gov/topics/genealogy/1990surnames/dist.male.first'\n",
    "    females_url = 'http://www2.census.gov/topics/genealogy/1990surnames/dist.female.first'\n",
    "    males = set([l.split()[0].lower() for l in requests.get(males_url).text.split('\\n') if l and float(l.split()[2]) < cutoff])\n",
    "    females = set([l.split()[0].lower() for l in requests.get(females_url).text.split('\\n') if l and float(l.split()[2]) < cutoff ])\n",
    "    print('found %d male and %d female names with cutoff=%.2f' % (len(males), len(females), cutoff))\n",
    "    return remove_ambiguous_names(males, females)\n",
    "\n",
    "def remove_ambiguous_names(male_names, female_names):\n",
    "    ambiguous = male_names & female_names\n",
    "    male_names -= ambiguous\n",
    "    female_names -= ambiguous\n",
    "    print('removed %d ambiguous names, leaving %d males and %d females' % (len(ambiguous), len(male_names), len(female_names)))\n",
    "    return male_names, female_names\n",
    "    \n",
    "male_names, female_names = get_gender_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "overall gender distribution=[('unknown', 732856), ('male', 156902), ('female', 102875)]\n"
     ]
    }
   ],
   "source": [
    "def label_genders(tweets, male_names, female_names):\n",
    "    for t in tweets:\n",
    "        if len(t['real_name'])>1:\n",
    "            first = t['real_name'].split()[0].lower()\n",
    "        else:\n",
    "            first = t['real_name'].lower()\n",
    "        if first in male_names:\n",
    "            t['gender'] = 'male'\n",
    "        elif first in female_names:\n",
    "            t['gender'] = 'female'\n",
    "        else:\n",
    "            t['gender'] = 'unknown'\n",
    "label_genders(raw_tweets, male_names, female_names)\n",
    "print('overall gender distribution=%s' % Counter(t['gender'] for t in raw_tweets).most_common(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "name": "python",
   "version": "2.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
