{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains all code to reproduce results in the paper \"Analyzing e-cigarette sentiment of Twitter\", published in the Computational Health Sciences Workshop at the 6th ACM Conference on Bioinformatics, Computational Biology, and Health Informatics, 2015."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We begin with the data collected by Sherry Emery et al. <slemery@uic.edu>, which consists 4.6M tweets from 2012-10-01 to 2013-09-30. These tweets have already been classified as \"organic\" or not using an SVM classifier (see Huang, Jidong et al. \"A cross-sectional examination of marketing of electronic cigarettes on Twitter.\" Tobacco control). We restrict our analysis to those classified as organic. We will assume these data live in `/data/chs15/ecig.csv.gz`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter, defaultdict\n",
    "import cPickle\n",
    "import csv\n",
    "import datetime\n",
    "import gzip\n",
    "import itertools\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.lines import Line2D\n",
    "import numpy as np\n",
    "import re\n",
    "import requests\n",
    "from sklearn.cross_validation import cross_val_score, KFold\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tabulate import tabulate\n",
    "\n",
    "DATA = '/data/chs15'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_csv(filename, fields=['text', 'svm', 'hand_label', 'posted_time', 'real_name', 'username']):\n",
    "    f = gzip.open(filename, 'rb')\n",
    "    csvr = csv.DictReader(f, delimiter=',', quotechar='\"')\n",
    "    for row in csvr:\n",
    "        yield dict([(k, row[k]) for k in fields if k in row])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read 992633 \"organic\" tweets\n"
     ]
    }
   ],
   "source": [
    "# Read all \"organic\" tweets.\n",
    "raw_tweets = [r for r in read_csv(DATA + '/ecig.csv.gz') if r['svm'] == '-']\n",
    "print('read %d \"organic\" tweets' % len(raw_tweets))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we manually labeled 2,000 tweets into one of two categories:\n",
    "- **positive (1):** express positive sentiment toward ecigs, or indicate that the speaker uses ecigs.\n",
    "- **negative (0):** express negative sentiment toward ecigs *or* do not express sentiment, e.g., informative\n",
    "\n",
    "(We originally separated negative into two classes, but accuracy was too low to support further analysis.)\n",
    "\n",
    "We assume these data live in `/data/chs15/labeled.csv.gz`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read 2000 labeled tweets\n",
      "Label distribution=[('negative', 1296), ('positive', 704)]\n"
     ]
    }
   ],
   "source": [
    "labeled_tweets = [r for r in read_csv(DATA + '/labeled.csv.gz', fields=['text', 'sent', 'real_name', 'username'])]\n",
    "\n",
    "print('read %d labeled tweets' % len(labeled_tweets))\n",
    "# Set labels.\n",
    "\n",
    "label_map = {'-1': 'negative', '0': 'negative', '1': 'positive'}\n",
    "for t in labeled_tweets:\n",
    "    t['sent'] = label_map[t['sent']]\n",
    "             \n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(['negative', 'positive'])\n",
    "y = label_encoder.transform([t['sent'] for t in labeled_tweets])             \n",
    "print('Label distribution=%s' % Counter(t['sent'] for t in labeled_tweets).most_common(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifier Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using `labeled_tweets`, we'll train a logistic regression classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Tweet tokenizer.\n",
    "def tokenize(text):\n",
    "    punc_re = '[' + re.escape(string.punctuation) + ']'\n",
    "    text = text.lower()\n",
    "    text = re.sub('#(\\S+)', r'HASHTAG_\\1', text)\n",
    "    text = re.sub('@(\\S+)', r'MENTION_\\1', text)\n",
    "    text = re.sub('http\\S+', 'THIS_IS_A_URL', text)\n",
    "    text = re.sub(r'(.)\\1\\1\\1+', r'\\1', text)\n",
    "    text = re.sub(r'[0-9]', '9', text)\n",
    "    toks = []\n",
    "    for tok in text.split():\n",
    "        tok = re.sub(r'^(' + punc_re + '+)', r'\\1 ', tok)\n",
    "        tok = re.sub(r'(' + punc_re + '+)$', r' \\1', tok)\n",
    "        for subtok in tok.split():\n",
    "            if re.search('\\w', subtok):\n",
    "                toks.append(subtok)\n",
    "    return toks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorized 2000 tweets. Found 4824 terms.\n"
     ]
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(decode_error='ignore', ngram_range=(1, 2), max_df=1., min_df=2,\n",
    "                             use_idf=True, tokenizer=tokenize, binary=False, norm='l2')\n",
    "X = vectorizer.fit_transform(t['text'] for t in labeled_tweets)\n",
    "print('Vectorized %d tweets. Found %d terms.' % (X_labeled.shape[0], X.shape[1]))\n",
    "features = np.array(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy=0.793\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "   negative       0.84      0.83      0.84      1296\n",
      "   positive       0.70      0.72      0.71       704\n",
      "\n",
      "avg / total       0.79      0.79      0.79      2000\n",
      "\n",
      "            negative    positive\n",
      "--------  ----------  ----------\n",
      "negative        1082         214\n",
      "positive         199         505\n",
      "\n",
      "CLASS 0\n",
      "my\t5.390\n",
      "i\t4.668\n",
      "vaping\t2.160\n",
      "HASHTAG_vaping\t1.722\n",
      "HASHTAG_ecig\t1.663\n",
      "\n",
      "CLASS 1\n",
      "THIS_IS_A_URL\t4.017\n",
      "e-cigarettes\t1.940\n",
      "de\t1.171\n",
      "as\t1.164\n",
      "99\t1.112\n"
     ]
    }
   ],
   "source": [
    "def confusion(truths, preds, labels):\n",
    "    m = confusion_matrix(truths, preds)\n",
    "    m = np.vstack((labels, m))\n",
    "    m = np.hstack((np.matrix([''] + list(labels)).T, m))\n",
    "    return tabulate(m.tolist(), headers='firstrow')\n",
    "\n",
    "def top_coef(clf, vocab, n=10):\n",
    "    if len(clf.classes_) == 2:\n",
    "        coefs = [clf.coef_[0], -clf.coef_[0]]\n",
    "    else:\n",
    "        coefs = clf.coef_\n",
    "    for li, label in enumerate(clf.classes_):\n",
    "        print('\\nCLASS %s' % label)\n",
    "        coef = coefs[li]\n",
    "        top_coef_ind = np.argsort(coef)[::-1][:n]\n",
    "        top_coef_terms = vocab[top_coef_ind]\n",
    "        top_coef = coef[top_coef_ind]\n",
    "        print '\\n'.join(['%s\\t%.3f' % (term, weight) for term, weight in zip(top_coef_terms, top_coef)])\n",
    "\n",
    "def do_cv(X, y, labels, nfolds=10):\n",
    "    cv = KFold(len(y), nfolds, random_state=123456)\n",
    "    preds = []\n",
    "    truths = []\n",
    "    for train, test in cv:\n",
    "        clf = LogisticRegression(class_weight='auto')\n",
    "        clf.fit(X[train], y[train])\n",
    "        preds.extend(clf.predict(X[test]))\n",
    "        truths.extend(y[test])\n",
    "    print ('accuracy=%.3f' % (accuracy_score(truths, preds)))\n",
    "    print classification_report(truths, preds, target_names=labels)\n",
    "    print confusion(truths, preds, labels)\n",
    "    clf = LogisticRegression(class_weight='auto')\n",
    "    clf.fit(X, y)\n",
    "    return clf, truths, preds\n",
    "\n",
    "clf, truths, preds = do_cv(X, y, label_encoder.classes_, 10)\n",
    "top_coef(clf, features, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Write cross-validation results .tex table.\n",
    "def clfreport_to_tex(report, outfile):\n",
    "    \"\"\" Write a sklearn classification report as a latex table. \"\"\"\n",
    "    report = re.sub(r' \\/ total', '', report)\n",
    "    report = re.sub(r'precision', 'Prec', report)\n",
    "    report = re.sub(r'recall', 'Rec', report)\n",
    "    report = re.sub(r'f1-score', 'F1', report)\n",
    "    report = re.sub(r'support', 'N', report)\n",
    "    table = ['\\\\begin{tabular}{|r|c|c|c|c|}', '\\\\hline']\n",
    "    lines = report.split('\\n')\n",
    "    for i, line in enumerate(lines):\n",
    "        parts = line.strip().split()\n",
    "        if len(parts) > 0:\n",
    "            if i == 0:\n",
    "                parts = [''] + ['{\\\\bf %s}' % p for p in parts]\n",
    "            else:\n",
    "                parts[0] = '{\\\\bf %s}' % parts[0]\n",
    "            table.append(' & '.join(parts) + '\\\\\\\\')\n",
    "        else:\n",
    "            table.append('\\\\hline')\n",
    "    table.append('\\\\end{tabular}')\n",
    "    of = open(outfile, 'wt')\n",
    "    of.write('\\n'.join(table))\n",
    "    \n",
    "clfreport_to_tex(classification_report(truths, preds, target_names=labels), 'cv.tex')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Write top coef .tex table.\n",
    "def clean(s):\n",
    "    s = re.sub('HASHTAG_', '\\#', s)\n",
    "    s = re.sub('MENTION_', '@', s)\n",
    "    s = re.sub('THIS_IS_A_URL', 'URL', s)\n",
    "    s = re.sub(r'_', '\\\\_', s)\n",
    "    return s\n",
    "    \n",
    "def write_top_coef(clf, vocab, labels, outf, n=20):\n",
    "    out = open(outf, 'wt')\n",
    "    coefs = [clf.coef_[0], -clf.coef_[0]]\n",
    "    for li, label in enumerate(labels):\n",
    "        coef = coefs[li]\n",
    "        top_coef_ind = np.argsort(coef)[::-1][:n]\n",
    "        top_coef_terms = vocab[top_coef_ind]\n",
    "        out.write('{\\\\bf %s} & %s\\\\\\\\\\n\\hline\\n' % (label, ', '.join(clean(s) for s in top_coef_terms)))\n",
    "        \n",
    "write_top_coef(clf, features, labels, 'coef.tex', n=20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we classify all the unlabeled tweets using the classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_raw = vectorizer.transform(t['text'] for t in raw_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted label distribution on raw tweets: [(0, 623396), (1, 369237)]\n"
     ]
    }
   ],
   "source": [
    "preds_raw = clf.predict(X_raw)\n",
    "print('predicted label distribution on raw tweets: %s' % Counter(preds_raw).most_common(2))\n",
    "for tweet, pred in zip(raw_tweets, preds_raw):\n",
    "    t['sent'] = labels[pred]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gender"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we classify each user by gender using a list of names from the census."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found 232 male and 523 female names with cutoff=75.00\n",
      "removed 6 ambiguous names, leaving 226 males and 517 females\n"
     ]
    }
   ],
   "source": [
    "def get_gender_names(cutoff=75):\n",
    "    males_url = 'http://www2.census.gov/topics/genealogy/1990surnames/dist.male.first'\n",
    "    females_url = 'http://www2.census.gov/topics/genealogy/1990surnames/dist.female.first'\n",
    "    males = set([l.split()[0].lower() for l in requests.get(males_url).text.split('\\n') if l and float(l.split()[2]) < cutoff])\n",
    "    females = set([l.split()[0].lower() for l in requests.get(females_url).text.split('\\n') if l and float(l.split()[2]) < cutoff ])\n",
    "    print('found %d male and %d female names with cutoff=%.2f' % (len(males), len(females), cutoff))\n",
    "    return remove_ambiguous_names(males, females)\n",
    "\n",
    "def remove_ambiguous_names(male_names, female_names):\n",
    "    ambiguous = male_names & female_names\n",
    "    male_names -= ambiguous\n",
    "    female_names -= ambiguous\n",
    "    print('removed %d ambiguous names, leaving %d males and %d females' % (len(ambiguous), len(male_names), len(female_names)))\n",
    "    return male_names, female_names\n",
    "    \n",
    "male_names, female_names = get_gender_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "overall gender distribution=[('unknown', 732856), ('male', 156902), ('female', 102875)]\n"
     ]
    }
   ],
   "source": [
    "def label_genders(tweets, male_names, female_names):\n",
    "    for t in tweets:\n",
    "        if len(t['real_name'])>1:\n",
    "            first = t['real_name'].split()[0].lower()\n",
    "        else:\n",
    "            first = t['real_name'].lower()\n",
    "        if first in male_names:\n",
    "            t['gender'] = 'male'\n",
    "        elif first in female_names:\n",
    "            t['gender'] = 'female'\n",
    "        else:\n",
    "            t['gender'] = 'unknown'\n",
    "label_genders(raw_tweets, male_names, female_names)\n",
    "print('overall gender distribution=%s' % Counter(t['gender'] for t in raw_tweets).most_common(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "most common male names: [('Mark', 9017), ('Paul', 4531), ('John', 4393), ('Michael', 3748), ('Chris', 3662), ('Adam', 3539), ('Alex', 3215), ('Gregory', 2916), ('David', 2747), ('James', 2732)]\n",
      "most common female names: [('Sarah', 2007), ('Ashley', 1399), ('Jessica', 1320), ('Emily', 1318), ('Rachel', 1212), ('Amanda', 1208), ('Katie', 1121), ('Laura', 1116), ('Mandy', 1087), ('Lauren', 1084)]\n",
      "most common unknown names: [('Electronic', 15305), ('Dragonfly', 14279), ('The', 5898), ('Lewisville', 4837), ('g', 4588), ('Vapornine', 4105), ('Matt', 3573), ('?', 3237), ('DFW', 2904), ('Nick', 2466)]\n"
     ]
    }
   ],
   "source": [
    "print('most common male names: %s' % Counter(t['real_name'].split()[0] for t in raw_tweets if t['gender'] == 'male').most_common(10))\n",
    "print('most common female names: %s' % Counter(t['real_name'].split()[0] for t in raw_tweets if t['gender'] == 'female').most_common(10))\n",
    "print('most common unknown names: %s' % Counter(t['real_name'].split()[0] for t in raw_tweets if t['real_name'] and t['gender'] == 'unknown').most_common(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Age"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To guess age from a user's name, we use the idea from from [Nate Silver's](https://twitter.com/FiveThirtyEight) recent article on [How to Tell Someone's Age When All you Know is Her Name](http://fivethirtyeight.com/features/how-to-tell-someones-age-when-all-you-know-is-her-name/). We borrow from [this notebook](https://github.com/ramnathv/agebyname_py) by [@ramnathv](https://github.com/ramnathv)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we download a list of [baby names](http://www.ssa.gov/oact/babynames/names.zip) provided by the SSA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('/data/chs15/names.zip', <httplib.HTTPMessage instance at 0x11ed967e8>)"
      ]
     },
     "execution_count": 287,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import urllib\n",
    "names_file = DATA + '/names.zip'\n",
    "urllib.urlretrieve('http://www.ssa.gov/oact/babynames/names.zip', DATA + '/names.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>sex</th>\n",
       "      <th>n</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Mary</td>\n",
       "      <td>F</td>\n",
       "      <td>7065</td>\n",
       "      <td>1880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Anna</td>\n",
       "      <td>F</td>\n",
       "      <td>2604</td>\n",
       "      <td>1880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Emma</td>\n",
       "      <td>F</td>\n",
       "      <td>2003</td>\n",
       "      <td>1880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Elizabeth</td>\n",
       "      <td>F</td>\n",
       "      <td>1939</td>\n",
       "      <td>1880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Minnie</td>\n",
       "      <td>F</td>\n",
       "      <td>1746</td>\n",
       "      <td>1880</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        name sex     n  year\n",
       "0       Mary   F  7065  1880\n",
       "1       Anna   F  2604  1880\n",
       "2       Emma   F  2003  1880\n",
       "3  Elizabeth   F  1939  1880\n",
       "4     Minnie   F  1746  1880"
      ]
     },
     "execution_count": 336,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read list of names.\n",
    "import pandas as pd\n",
    "from zipfile import ZipFile\n",
    "\n",
    "def read_names(f, zf):\n",
    "  data = pd.read_csv(zf.open(f), header = None, names = ['name', 'sex', 'n'])\n",
    "  data['year'] = int(re.findall(r'\\d+', f)[0])\n",
    "  return data\n",
    "  \n",
    "names_zip = ZipFile(names_file)\n",
    "bnames = pd.concat([read_names(f, names_zip) for f in names_zip.namelist() if f.endswith('.txt')])\n",
    "names_ =  set(bnames.name)\n",
    "bnames.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we parse the [cohort life tables](http://www.ssa.gov/oact/NOTES/as120/LifeTables_Tbl_7.html) provided by SSA, using @ramnathv's export."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>lx</th>\n",
       "      <th>sex</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">0</th>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>F</td>\n",
       "      <td>1900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>19.1</td>\n",
       "      <td>F</td>\n",
       "      <td>1901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>38.2</td>\n",
       "      <td>F</td>\n",
       "      <td>1902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>57.3</td>\n",
       "      <td>F</td>\n",
       "      <td>1903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>76.4</td>\n",
       "      <td>F</td>\n",
       "      <td>1904</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       lx sex  year\n",
       "0 0   0.0   F  1900\n",
       "  1  19.1   F  1901\n",
       "  2  38.2   F  1902\n",
       "  3  57.3   F  1903\n",
       "  4  76.4   F  1904"
      ]
     },
     "execution_count": 337,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.interpolate import interp1d\n",
    "\n",
    "lifetables = pd.read_csv(DATA + '/lifetables.csv')\n",
    "lifetables_2014 = lifetables[lifetables['year'] + lifetables['x'] == 2014]\n",
    "lifetables_2014.head()\n",
    "\n",
    "# interpolate gaps\n",
    "def process(d, kind = 'slinear'):\n",
    "  f = interp1d(d.year, d.lx, kind)\n",
    "  year = np.arange(1900, 2011)\n",
    "  lx = f(year)\n",
    "  return pd.DataFrame({\"year\": year, \"lx\": lx, \"sex\": d.sex.iloc[1]})\n",
    "\n",
    "lifetable_2014 = lifetables_2014.\\\n",
    "  groupby('sex', as_index = False).\\\n",
    "  apply(process)\n",
    "lifetable_2014.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we use the [live births data](http://www.census.gov/statab/hist/02HS0013.xls) from the census to extrapolate the birth data to account for the correct that not all births were recorded by SSA till around 1930, since it wasn't mandatory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   year  births\n",
      "0  1909    2718\n",
      "1  1910    2777\n",
      "2  1911    2809\n",
      "3  1912    2840\n",
      "4  1913    2869\n",
      "   year       cor\n",
      "0  1909  5.316621\n",
      "1  1910  4.701051\n",
      "2  1911  4.359994\n",
      "3  1912  2.874354\n",
      "4  1913  2.523141\n"
     ]
    }
   ],
   "source": [
    "# Download the data and add a correction factor.\n",
    "urllib.urlretrieve(\"http://www.census.gov/statab/hist/02HS0013.xls\", DATA + \"/02HS0013.xls\")\n",
    "dat = pd.read_excel(DATA + '/02HS0013.xls', sheetname = 'HS-13', skiprows = range(14))\n",
    "tot_births = dat.ix[9:101,:2].reset_index(drop = True)\n",
    "tot_births.columns = ['year', 'births']\n",
    "tot_births = tot_births.convert_objects(convert_numeric = True)\n",
    "print tot_births.head()\n",
    "# Correction factors.\n",
    "cor_factors = bnames.groupby('year', as_index = False).sum().merge(tot_births)\n",
    "cor_factors['cor'] = cor_factors['births']*1000/cor_factors['n']\n",
    "cor_factors = cor_factors[['year', 'cor']]\n",
    "cor_new = pd.DataFrame({\n",
    "  'year': range(2002, 2014),\n",
    "  'cor': cor_factors.cor.iloc[-1]\n",
    "})\n",
    "cor_factors = pd.concat([cor_factors, cor_new])[['year', 'cor']]\n",
    "print cor_factors.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# For efficiency, define name as index of the table\n",
    "bname= bnames.set_index(['name'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Joseph\n",
      "    sex     n  year       cor       lx        n_cor      n_alive\n",
      "193   F     7  2007  1.076384  99351.7     7.534690     7.485842\n",
      "194   M  2820  2007  1.076384  99221.6  3035.403577  3011.775996\n",
      "195   M  2783  2008  1.076384  99256.4  2995.577360  2973.302247\n",
      "196   M  2979  2009  1.076384  99291.2  3206.548673  3183.820656\n",
      "197   M  2898  2010  1.076384  99326.0  3119.361549  3098.337052\n",
      "Irene\n",
      "    sex    n  year       cor       lx       n_cor     n_alive\n",
      "169   F  492  2006  1.076384  99324.6  529.581050  526.004259\n",
      "170   F  467  2007  1.076384  99351.7  502.671443  499.412624\n",
      "171   F  482  2008  1.076384  99378.8  518.817207  515.594315\n",
      "172   F  479  2009  1.076384  99405.9  515.588054  512.524946\n",
      "173   F  408  2010  1.076384  99433.0  439.164773  436.674709\n"
     ]
    }
   ],
   "source": [
    "# This function tells the number of births and number of people alive. \n",
    "def get_data(name):\n",
    "    dat = bnames\n",
    "    dat = dat.loc[name]\n",
    "    if type(dat) == pd.Series:\n",
    "        m = pd.DataFrame(bnames.loc[name]).transpose()\n",
    "    else:\n",
    "        m = dat\n",
    "    data =   m.\\\n",
    "            merge(cor_factors).\\\n",
    "            merge(lifetable_2014)\n",
    "    data['n_cor'] = data['n']*data['cor']\n",
    "    data['n_alive'] = data['lx']/(10**5)*data['n_cor']\n",
    "    return data\n",
    "\n",
    "print('Joseph')\n",
    "print get_data('Edward').tail()\n",
    "print('Irene')\n",
    "print get_data('Irene').tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Irene\n",
      "{'19-24': 2.768896935891503, '35-44': 5.9900770087280355, '>45': 79.96979901362864, '25-34': 5.300358368654281, '<18': 5.970868673097543}\n",
      "Michael\n",
      "{'19-24': 10.25125353909169, '35-44': 21.724002632069023, '>45': 33.05996262251043, '25-34': 21.20556963381044, '<18': 13.759211572518417}\n",
      "Payton\n",
      "{'19-24': 6.8700057813423445, '35-44': 0.4265604073936273, '>45': 0.5359894837394966, '25-34': 1.2200035112432313, '<18': 90.9474408162813}\n"
     ]
    }
   ],
   "source": [
    "# Now we can map each name to a distribution over age brackets.\n",
    "def first_name(name):\n",
    "    if len(name) > 1:\n",
    "        name = name.split()[0].lower().title()\n",
    "    else:\n",
    "        name = name.lower().title()\n",
    "    return name\n",
    "\n",
    "\n",
    "def age_brackets(name):    \n",
    "    name = first_name(name)\n",
    "    dic = defaultdict(list)\n",
    "    #\n",
    "    s=0 \n",
    "    s1=0\n",
    "    s2=0\n",
    "    s3=0\n",
    "    s4=0\n",
    "    s5=0\n",
    "    #fraction of percentages\n",
    "    f=0  \n",
    "    f1=0\n",
    "    f2=0\n",
    "    f3=0\n",
    "    f4=0    \n",
    "    f5=0\n",
    "    \n",
    "    m = get_data(name).as_matrix()\n",
    "    if len(m) != 0:\n",
    "        for t in m:\n",
    "            if t[2]>=1996: # 0-18\n",
    "                s = s+t[1]\n",
    "                f = f + t[6]\n",
    "                dic['<18'] = f\n",
    "            elif t[2] >= 1990 and t[3]<=1995: #19-24\n",
    "                s1 = s1+t[1]\n",
    "                f1 = f1 + t[6]\n",
    "                dic['19-24'] = f1\n",
    "            elif t[2] >= 1980 and t[3]<=1989: #25-34\n",
    "                s2 = s2+t[1]\n",
    "                f2 = f2 + t[6]\n",
    "                dic['25-34'] = f2\n",
    "            elif t[2] >= 1970 and t[3]<=1979: #35-44\n",
    "                s3 = s3+t[1]\n",
    "                f3 = f3 + t[6]\n",
    "                dic['35-44'] = f3\n",
    "            elif t[2] <= 1960 :#and t[2] <= 1969: #45-54\n",
    "                s4 = s4+t[1]\n",
    "                f4 = f4 + t[6]\n",
    "                dic['>45'] = f4\n",
    "        #d = pd.DataFrame(dic.values(),dic.keys(), columns=['n','n_alive'])\n",
    "        #d['fraction']= (d['n_alive']/d['n_alive'].sum())*100        \n",
    "        #print d\n",
    "        #print dic\n",
    "        total = sum(dic.values())\n",
    "        return dict([(k, 1.*v/total*100) for k, v in dic.iteritems()])\n",
    "    \n",
    "print('Irene')\n",
    "print age_brackets('Irene')\n",
    "print('Michael')\n",
    "print age_brackets('Michael')\n",
    "print('Payton')\n",
    "print age_brackets('Payton')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed 0 tweets\n",
      "processed 10000 tweets\n",
      "processed 20000 tweets\n",
      "processed 30000 tweets\n",
      "processed 40000 tweets\n",
      "processed 50000 tweets\n",
      "processed 60000 tweets\n",
      "processed 70000 tweets\n",
      "processed 80000 tweets\n",
      "processed 90000 tweets\n",
      "processed 100000 tweets\n",
      "processed 110000 tweets\n",
      "processed 120000 tweets\n",
      "processed 130000 tweets\n",
      "processed 140000 tweets\n",
      "processed 150000 tweets\n",
      "processed 160000 tweets\n",
      "processed 170000 tweets\n",
      "processed 180000 tweets\n",
      "processed 190000 tweets\n",
      "processed 200000 tweets\n",
      "processed 210000 tweets\n",
      "processed 220000 tweets\n",
      "processed 230000 tweets\n",
      "processed 240000 tweets\n",
      "processed 250000 tweets\n",
      "processed 260000 tweets\n",
      "processed 270000 tweets\n",
      "processed 280000 tweets\n",
      "processed 290000 tweets\n",
      "processed 300000 tweets\n",
      "processed 310000 tweets\n",
      "processed 320000 tweets\n",
      "processed 330000 tweets\n",
      "processed 340000 tweets\n",
      "processed 350000 tweets\n",
      "processed 360000 tweets\n",
      "processed 370000 tweets\n",
      "processed 380000 tweets\n",
      "processed 390000 tweets\n",
      "processed 400000 tweets\n",
      "processed 410000 tweets\n",
      "processed 420000 tweets\n",
      "processed 430000 tweets\n",
      "processed 440000 tweets\n",
      "processed 450000 tweets\n",
      "processed 460000 tweets\n",
      "processed 470000 tweets\n",
      "processed 480000 tweets\n",
      "processed 490000 tweets\n",
      "processed 500000 tweets\n",
      "processed 510000 tweets\n",
      "processed 520000 tweets\n",
      "processed 530000 tweets\n",
      "processed 540000 tweets\n",
      "processed 550000 tweets\n",
      "processed 560000 tweets\n",
      "processed 570000 tweets\n",
      "processed 580000 tweets\n",
      "processed 590000 tweets\n",
      "processed 600000 tweets\n",
      "processed 610000 tweets\n",
      "processed 620000 tweets\n",
      "processed 630000 tweets\n",
      "processed 640000 tweets\n",
      "processed 650000 tweets\n",
      "processed 660000 tweets\n",
      "processed 670000 tweets\n",
      "processed 680000 tweets\n",
      "processed 690000 tweets\n",
      "processed 700000 tweets\n",
      "processed 710000 tweets\n",
      "processed 720000 tweets\n",
      "processed 730000 tweets\n",
      "processed 740000 tweets\n",
      "processed 750000 tweets\n",
      "processed 760000 tweets\n",
      "processed 770000 tweets\n",
      "processed 780000 tweets\n",
      "processed 790000 tweets\n",
      "processed 800000 tweets\n",
      "processed 810000 tweets\n",
      "processed 820000 tweets\n",
      "processed 830000 tweets\n",
      "processed 840000 tweets\n",
      "processed 850000 tweets\n",
      "processed 860000 tweets\n",
      "processed 870000 tweets\n",
      "processed 880000 tweets\n",
      "processed 890000 tweets\n",
      "processed 900000 tweets\n",
      "processed 910000 tweets\n",
      "processed 920000 tweets\n",
      "processed 930000 tweets\n",
      "processed 940000 tweets\n",
      "processed 950000 tweets\n",
      "processed 960000 tweets\n",
      "processed 970000 tweets\n",
      "processed 980000 tweets\n",
      "processed 990000 tweets\n"
     ]
    }
   ],
   "source": [
    "def label_ages(tweets, valid_names):\n",
    "    name_cache = {}\n",
    "    for ti, t in enumerate(tweets):\n",
    "        name = first_name(t['real_name'])\n",
    "        if name in valid_names:\n",
    "            if name in name_cache:\n",
    "                distr = name_cache[name]\n",
    "            else:\n",
    "                distr = age_brackets(name)\n",
    "                name_cache[name] = distr\n",
    "            t['ages'] = distr\n",
    "        \n",
    "        if ti % 10000 == 0:\n",
    "            print('processed %d tweets' % ti)\n",
    "        ti += 1\n",
    "        \n",
    "label_ages(raw_tweets, names_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "overall ages:\n",
      "19-24\t0.12\n",
      ">45\t0.27\n",
      "35-44\t0.14\n",
      "<18\t0.30\n",
      "25-34\t0.16\n"
     ]
    }
   ],
   "source": [
    "def add_to_dict(d, d1):\n",
    "    for k in d1:\n",
    "        d[k] += d1[k]\n",
    "    \n",
    "all_ages = defaultdict(lambda: 0)\n",
    "for t in raw_tweets:\n",
    "    if 'ages' in t and t['ages']:\n",
    "        add_to_dict(all_ages, t['ages'])\n",
    "        \n",
    "print 'overall ages:'\n",
    "for k, v in all_ages.iteritems():\n",
    "    print('%s\\t%.2f' % (k, v / sum(all_ages.values())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Save our annotated tweets.\n",
    "cPickle.dump(raw_tweets, open(DATA + '/raw_tweets.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Figures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we create a number of figures to explore how ecig sentiment varies by time, gender, and age."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "name": "python",
   "version": "2.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
