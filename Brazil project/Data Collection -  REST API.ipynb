{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "The above code was taken from the blog: http://www.karambelkar.info/2015/01/how-to-use-twitter-search-rest-api-most.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import jsonpickle\n",
    "import os\n",
    "import tweepy\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading max 10000000 tweets\n",
      "Downloaded 100 tweets\n",
      "Downloaded 200 tweets\n",
      "Downloaded 300 tweets\n",
      "Downloaded 400 tweets\n",
      "Downloaded 500 tweets\n",
      "Downloaded 600 tweets\n",
      "Downloaded 700 tweets\n",
      "Downloaded 800 tweets\n",
      "Downloaded 900 tweets\n",
      "Downloaded 1000 tweets\n",
      "Downloaded 1100 tweets\n",
      "Downloaded 1200 tweets\n",
      "Downloaded 1300 tweets\n",
      "Downloaded 1400 tweets\n",
      "Downloaded 1500 tweets\n",
      "Downloaded 1600 tweets\n",
      "Downloaded 1700 tweets\n",
      "Downloaded 1800 tweets\n",
      "Downloaded 1900 tweets\n",
      "Downloaded 2000 tweets\n",
      "Downloaded 2100 tweets\n",
      "Downloaded 2200 tweets\n",
      "Downloaded 2300 tweets\n",
      "Downloaded 2400 tweets\n",
      "Downloaded 2500 tweets\n",
      "Downloaded 2600 tweets\n",
      "Downloaded 2700 tweets\n",
      "Downloaded 2800 tweets\n",
      "Downloaded 2900 tweets\n",
      "Downloaded 3000 tweets\n",
      "Downloaded 3100 tweets\n",
      "Downloaded 3200 tweets\n",
      "Downloaded 3300 tweets\n",
      "Downloaded 3400 tweets\n",
      "Downloaded 3500 tweets\n",
      "Downloaded 3600 tweets\n",
      "Downloaded 3700 tweets\n",
      "Downloaded 3800 tweets\n",
      "Downloaded 3900 tweets\n",
      "Downloaded 4000 tweets\n",
      "Downloaded 4100 tweets\n",
      "Downloaded 4200 tweets\n",
      "Downloaded 4300 tweets\n",
      "Downloaded 4400 tweets\n",
      "Downloaded 4500 tweets\n",
      "Downloaded 4600 tweets\n",
      "Downloaded 4700 tweets\n",
      "Downloaded 4800 tweets\n",
      "Downloaded 4900 tweets\n",
      "Downloaded 5000 tweets\n",
      "Downloaded 5100 tweets\n",
      "Downloaded 5200 tweets\n",
      "Downloaded 5300 tweets\n",
      "Downloaded 5400 tweets\n",
      "Downloaded 5500 tweets\n",
      "Downloaded 5600 tweets\n",
      "Downloaded 5700 tweets\n",
      "Downloaded 5800 tweets\n",
      "Downloaded 5900 tweets\n",
      "Downloaded 6000 tweets\n",
      "Downloaded 6100 tweets\n",
      "Downloaded 6200 tweets\n",
      "Downloaded 6300 tweets\n",
      "Downloaded 6400 tweets\n",
      "Downloaded 6500 tweets\n",
      "Downloaded 6600 tweets\n",
      "Downloaded 6700 tweets\n",
      "Downloaded 6800 tweets\n",
      "Downloaded 6900 tweets\n",
      "Downloaded 7000 tweets\n",
      "Downloaded 7100 tweets\n",
      "Downloaded 7200 tweets\n",
      "Downloaded 7300 tweets\n",
      "Downloaded 7400 tweets\n",
      "Downloaded 7500 tweets\n",
      "Downloaded 7600 tweets\n",
      "some error : Failed to parse JSON payload: Unterminated string starting at: line 1 column 509014 (char 509013)\n",
      "Downloaded 7600 tweets, Saved to tweets_foraPT.txt\n"
     ]
    }
   ],
   "source": [
    "# Replace the API_KEY and API_SECRET with your application's key and secret.\n",
    "auth = tweepy.AppAuthHandler( 'IwwzYhMQOY42t6pseaNKeTuMm'\n",
    ", 'qbmPysSls2lQ5ikuB3ruPKai9wSPacIjd8iVCsf95nQTchdwRd')\n",
    "  \n",
    "api = tweepy.API(auth, wait_on_rate_limit=True,\n",
    "                   wait_on_rate_limit_notify=True)\n",
    "  \n",
    "if (not api):\n",
    "    print (\"Can't Authenticate\")\n",
    "    sys.exit(-1)\n",
    "  \n",
    "\n",
    "searchQuery = ['#ForaPT', 'ForaPT', 'forapt']  # this is what we're searching for\n",
    "maxTweets = 10000000 # Some arbitrary large number\n",
    "tweetsPerQry = 100  # this is the max the API permits\n",
    "fName = 'tweets_foraPT.txt' # We'll store the tweets in a text file.\n",
    " \n",
    " \n",
    "# If results from a specific ID onwards are reqd, set since_id to that ID.\n",
    "# else default to no lower limit, go as far back as API allows\n",
    "sinceId = None\n",
    " \n",
    "# If results only below a specific ID are, set max_id to that ID.\n",
    "# else default to no upper limit, start from the most recent tweet matching the search query.\n",
    "max_id = -1L\n",
    " \n",
    "tweetCount = 0\n",
    "print(\"Downloading max {0} tweets\".format(maxTweets))\n",
    "with open(fName, 'w') as f:\n",
    "    while tweetCount < maxTweets:\n",
    "        try:\n",
    "            if (max_id <= 0):\n",
    "                if (not sinceId):\n",
    "                    new_tweets = api.search(q=searchQuery, count=tweetsPerQry)\n",
    "                else:\n",
    "                    new_tweets = api.search(q=searchQuery, count=tweetsPerQry,\n",
    "                                            since_id=sinceId)\n",
    "            else:\n",
    "                if (not sinceId):\n",
    "                    new_tweets = api.search(q=searchQuery, count=tweetsPerQry,\n",
    "                                            max_id=str(max_id - 1))\n",
    "                else:\n",
    "                    new_tweets = api.search(q=searchQuery, count=tweetsPerQry,\n",
    "                                            max_id=str(max_id - 1),\n",
    "                                            since_id=sinceId)\n",
    "            if not new_tweets:\n",
    "                print(\"No more tweets found\")\n",
    "                break\n",
    "            for tweet in new_tweets:\n",
    "                var = tweet._json\n",
    "                f.write(json.dumps(var, ensure_ascii=False).encode('utf8') + '\\n')\n",
    "                #f.write(jsonpickle.encode(tweet._json, unpicklable=False) +\n",
    "                  #      '\\n')\n",
    "            tweetCount += len(new_tweets)\n",
    "            print(\"Downloaded {0} tweets\".format(tweetCount))\n",
    "            max_id = new_tweets[-1].id\n",
    "        except tweepy.TweepError as e:\n",
    "            # Just exit if any error\n",
    "            print(\"some error : \" + str(e))\n",
    "            break\n",
    " \n",
    "print (\"Downloaded {0} tweets, Saved to {1}\".format(tweetCount, fName))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
