{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Search over tokenization, window, and gap sizes.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re \n",
    "import string\n",
    "import timestring\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cross_validation import cross_val_score, KFold\n",
    "import glob, os\n",
    "import numpy as np\n",
    "from sklearn import svm\n",
    "from collections import defaultdict\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "from sklearn.cross_validation import KFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from sklearn.feature_extraction import text\n",
    "from scipy.sparse import hstack\n",
    "\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Tokenizer:\n",
    "    def __init__(self, collapse_mentions=False, collapse_digits=True):\n",
    "        self.collapse_mentions = collapse_mentions\n",
    "        self.collapse_digits = collapse_digits\n",
    "        #self.stopwords = stopwords\n",
    "        \n",
    "    def tokenize(self, text):\n",
    "        punc_re = '[' + '\\\\!\\\\\"\\\\#\\\\$\\\\%\\\\&\\\\\\'\\\\(\\\\)\\\\*\\\\+\\\\,\\\\-\\\\.\\\\/\\\\:\\\\;\\\\<\\\\=\\\\>\\\\?\\\\@\\\\[\\\\\\\\\\\\]\\\\_\\\\{\\\\|\\\\}' + ']'\n",
    "        text = text.lower()\n",
    "        text = re.sub('#(\\S+)', r'HASHTAG_\\1', text)\n",
    "        if self.collapse_mentions:\n",
    "            text = re.sub('@\\S+', 'MENTION', text)\n",
    "        else:\n",
    "            text = re.sub('@\\S+', 'MENTION_\\1', text)\n",
    "        text = re.sub('http\\S+', 'THIS_IS_A_URL', text)\n",
    "        text = re.sub(r'(.)\\1\\1\\1+', r'\\1', text)\n",
    "        if self.collapse_digits: # Numbers help!\n",
    "            text = re.sub(r'[0-9]', '9', text) \n",
    "        toks = []\n",
    "        for tok in text.split():\n",
    "            tok = re.sub(r'^(' + punc_re + '+)', r'\\1 ', tok)\n",
    "            tok = re.sub(r'(' + punc_re + '+)$', r' \\1', tok)\n",
    "            for subtok in tok.split():\n",
    "                if re.search('\\w', subtok):\n",
    "                    toks.append(subtok)\n",
    "        return toks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DIR = '/data/2/protest'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['foradilma', 'fora dilma', 'forapt', 'fora pt', 'vemprarua', 'vem pra rua']"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def read_keywords(path):\n",
    "    return [s.strip().lower() for s in open(path)]\n",
    "    \n",
    "keywords = read_keywords(DIR + '/keywords.txt')\n",
    "keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def print_top_terms(clf, vec, n=100):\n",
    "    #feats = sorted(np.array(vec.get_feature_names()))\n",
    "    #print('\\n'.join(feats[np.argsort(clf.coef_[0])[::-1][:n]]))\n",
    "    #print(feats[0:30])    \n",
    "    coefs = sorted(zip(vec.get_feature_names(),clf.coef_[0]),key=lambda x:x[1])\n",
    "    #print(coefs[0:30], '\\n')\n",
    "    print(coefs[-30:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_stopwords(path):\n",
    "    return [s.strip().lower() for s in open(path)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import os, io, json, codecs\n",
    "\n",
    "def matches_keywords(text, keywords):\n",
    "    \"\"\" Return true if any keyword is a substring of this text, ignoring case. \"\"\"\n",
    "    text = text.lower()\n",
    "    for kw in keywords:\n",
    "        if kw in text:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def filename2user(fname):\n",
    "    \"\"\"Convert filename like this\n",
    "      /data/2/protest/Timeline/MandinhaSimone.txt.txt\n",
    "    into a username like\n",
    "      MandinhaSimone\n",
    "    \"\"\"\n",
    "    return re.sub(r'^([^\\.]+)\\..+', r'\\1', os.path.basename(fname))\n",
    "\n",
    "def iterate_instances(files, keywords, window_sz, gap_sz):\n",
    "    \"\"\"\n",
    "    Return an iterator over tuples containing:\n",
    "    (concatenated tweet text, label, username)\n",
    "    For each user in path, we find the first tweet containing one of the specified keywords.\n",
    "    We then create one positive instance, containing all tweets prior to the matched tweet.\n",
    "    We also create one negative instance, which is the same as the positive instance, except\n",
    "    the N most recent tweets are removed (where N is set by the negative_window parameter).\n",
    "    We additionally filter users if they use one of the keywords in one of their first `negative_window`\n",
    "    tweets. This is to we have enough tweets to make a negative example.\n",
    "    \"\"\"\n",
    "    for fname in files:\n",
    "        user = filename2user(fname)\n",
    "        lines = []\n",
    "        #print(user)\n",
    "        for i, line in enumerate(open(fname)):\n",
    "            js = json.loads(line)\n",
    "            # exclude people who use keyword within first `window` of tweets.\n",
    "            if i <= (2*window_sz + gap_sz) and matches_keywords(js['text'], keywords):\n",
    "                #print('skipping', fname, 'because uses keyword in first', negative_window, 'tweets')\n",
    "                break\n",
    "            if i > (2*window_sz + gap_sz) and matches_keywords(js['text'], keywords):\n",
    "                #yield (' '.join(lines), 1, user)\n",
    "                # yield (' '.join(lines[:-negative_window]), 0, user)\n",
    "                pos_start = max(0, i - window_sz)\n",
    "                pos_end = i+1\n",
    "                neg_start = max(0, i - gap_sz - (2 * window_sz))\n",
    "                neg_end = i - gap_sz - window_sz\n",
    "                # print('ps=%d pe=%d ns=%d ne=%d' % (pos_start, pos_end, neg_start, neg_end))\n",
    "                yield (' '.join(lines[pos_start:pos_end]), 1, user)\n",
    "                yield (' '.join(lines[neg_start:neg_end]), 0, user)\n",
    "                break\n",
    "            lines.append(js['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#parse date \n",
    "def parse_date(datestring):\n",
    "    from datetime import datetime\n",
    "\n",
    "    \"\"\" Input, e.g., Mon Aug 24 19:41:14 +0000 2015\n",
    "    Output, e.g., 24 \"\"\"\n",
    "    #Sat May 16 16:30:12 +0000 2015\n",
    "    #print datestring\n",
    "    parts = datestring.split()\n",
    "    var = datetime.strptime(parts[1]+' ' +parts[2]+' '+ parts[5],'%b %d %Y')\n",
    "    return var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def iterate_instances_changed(files, keywords, window_sz, gap_sz):\n",
    "    \"\"\"\n",
    "    Return an iterator over tuples containing:\n",
    "    (concatenated tweet text, label, username, percentage of neightbors who used hashtag)\n",
    "    For each user in path, we find the first tweet containing one of the specified keywords.\n",
    "    We then create one positive instance, containing all tweets prior to the matched tweet.\n",
    "    We also create one negative instance, which is the same as the positive instance, except\n",
    "    the N most recent tweets are removed (where N is set by the negative_window parameter).\n",
    "    We additionally filter users if they use one of the keywords in one of their first `negative_window`\n",
    "    tweets. This is to we have enough tweets to make a negative example.\n",
    "    Also we find neighbors (define as those with symetric mentions) who have also used one of the keywords \n",
    "    during positive or negative window \n",
    "    \"\"\"\n",
    "    DIR = '/home/elaine/Protest/protest/Brazil project/'\n",
    "    DIR2='/data/2/protest/mentions/'\n",
    "    dic = defaultdict(tuple)\n",
    "    pkl_file = open(DIR+'all_mentions_graph.pkl', 'rb')#open pickle file where the edges of the graph is saved\n",
    "    data1 = pickle.load(pkl_file)\n",
    "\n",
    "    for _, _, f in os.walk(DIR2): f\n",
    "        \n",
    "    #for _, _, arquivos in os.walk(files): arquivos\n",
    "        \n",
    "    for fname in files:#glob.glob(path + '/*.txt'):\n",
    "        \n",
    "        user = filename2user(fname)\n",
    "        #print('this is', user)\n",
    "        lines = []\n",
    "        countneg=0\n",
    "        countpos=0\n",
    "        #print('treino',user)\n",
    "        for i, line in enumerate(open(fname)):\n",
    "            js = json.loads(line)\n",
    "            # exclude people who use keyword within first `window` of tweets.\n",
    "            if i <= (2*window_sz + gap_sz) and matches_keywords(js['text'], keywords):\n",
    "                #print('skipping', arquivos[fname], 'because uses keyword in first', negative_window, 'tweets')\n",
    "                break\n",
    "            if i > (2*window_sz + gap_sz) and matches_keywords(js['text'], keywords):\n",
    "                #yield (' '.join(lines), 1, user)\n",
    "                # yield (' '.join(lines[:-negative_window]), 0, user)\n",
    "                pos_start = max(0, i - window_sz)\n",
    "                #print('positive window starts in',pos_start)\n",
    "                pos_end = i+1\n",
    "                #print('positive window finishes in',pos_end)\n",
    "\n",
    "                #print('lineslen: ',len(lines))\n",
    "                date1_pos = lines[pos_start][1]\n",
    "                #print('ksdjsk',lines[pos_start][1])\n",
    "                #print(pos_end)\n",
    "                date2_pos = lines[pos_end-2][1]\n",
    "\n",
    "                neg_start = max(0, i - gap_sz - (2 * window_sz))\n",
    "                #print('negative window starts in',neg_start)\n",
    "                neg_end = i - gap_sz - window_sz\n",
    "                #print('nagative window finishes in',neg_end)\n",
    "                date1_neg = lines[neg_start][1]\n",
    "                date2_neg = lines[neg_end][1]\n",
    "\n",
    "                if data1.has_node(user) == True: #if that file is in the graph\n",
    "                    #print(user, 'is in the graph!')\n",
    "                    neighbors = data1.neighbors(user) #get all neighbors of the user\n",
    "                    #print(user, 's neighbors are: ', neighbors)\n",
    "                    for n in neighbors:\n",
    "                        if n +'.txt' in f:#if that file is in the directory\n",
    "                            #print('vizinho: ', n)\n",
    "                            neighbor = open(DIR2 + n +'.txt','r')\n",
    "                            nlines = neighbor.readlines()\n",
    "                            for t in reversed(nlines):\n",
    "                                tweet_neighbor = json.loads(t) \n",
    "                                #print(tweet_neighbor['created_at'])\n",
    "                                #print(parse_date(tweet_neighbor['created_at']) ,'>', parse_date(date1_pos) )\n",
    "                                if matches_keywords(tweet_neighbor['text'],keywords):\n",
    "                                    if parse_date(tweet_neighbor['created_at']) > parse_date(date1_pos) and parse_date(tweet_neighbor['created_at']) < parse_date(date2_pos):\n",
    "                                        #print(n, 'posted keyword in', parse_date(tweet_neighbor['created_at']))\n",
    "                                        #print('entrou!!')\n",
    "                                        countpos = countpos+1\n",
    "                                        break\n",
    "                            for t in reversed(nlines):\n",
    "                                tweet_neighbor = json.loads(t) \n",
    "                                if matches_keywords(tweet_neighbor['text'],keywords):\n",
    "                                    if parse_date(tweet_neighbor['created_at']) < parse_date(date2_neg) and parse_date(tweet_neighbor['created_at']) > parse_date(date2_neg):\n",
    "                                        #print(n, 'posted negkeyword in', parse_date(tweet_neighbor['created_at']))\n",
    "                                        #print('1entrou!!')\n",
    "                                        countneg = countneg+1\n",
    "                                        break\n",
    "                            dic[user]=((float(countpos)/float(len(neighbors))),float(countneg)/float(len(neighbors)))\n",
    "                #print('diccc',dic)\n",
    "                # print('ps=%d pe=%d ns=%d ne=%d' % (pos_start, pos_end, neg_start, neg_end))\n",
    "                #print('testeee',lines[0][pos_start:pos_end])\n",
    "                testepos = \" \".join(l[0] for l in lines[pos_start:pos_end])\n",
    "                testeneg = \" \".join(l[0] for l in lines[neg_start:neg_end])\n",
    "                \n",
    "                yield (testepos, 1, user, float(countpos)/float(len(neighbors)))\n",
    "                yield (testeneg, 0, user,float(countneg)/float(len(neighbors)))\n",
    "                countneg=0\n",
    "                countpos=0\n",
    "                break\n",
    "            lines.append((js['text'],js['created_at']))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix as sk_confusion_matrix\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "#y_pred = model.predict(X)\n",
    "\n",
    "def confusion_matrix(y, y_pred):\n",
    "    cm = sk_confusion_matrix(y, y_pred)\n",
    "    cm = pd.DataFrame(data=cm, columns=[0,1], index=[0,1])\n",
    "    cm.columns.name = 'Predicted'\n",
    "    cm.index.name = 'Expected'\n",
    "    error_rate = (y_pred != y).mean()\n",
    "    print('error rate: %.2f' % error_rate)\n",
    "    return cm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def do_expt(all_files, collapse_mentions, collapse_digits, binary, ngram_range,\n",
    "            min_df, use_idf, norm, window_sz, gap_sz):\n",
    "    toker = Tokenizer(collapse_digits=collapse_digits, collapse_mentions=collapse_mentions)\n",
    "    \n",
    "    stopwords = set(read_stopwords(DIR + '/stopwords.txt'))    \n",
    "    my_stop_words = text.ENGLISH_STOP_WORDS.union(stopwords)\n",
    "    #print('stoppp',stopwords,'\\n')\n",
    "    \n",
    "    #print(my_stop_words)\n",
    "    \n",
    "    vectorizer = TfidfVectorizer(binary=binary, decode_error='ignore',ngram_range=ngram_range,\n",
    "                                 max_df=1.0, min_df=min_df, use_idf=use_idf,\n",
    "                                 tokenizer=toker.tokenize,\n",
    "                                 norm=norm, stop_words=set(my_stop_words))\n",
    "    y = []\n",
    "    users = []\n",
    "    neighbors=[]\n",
    "    iterator = iterate_instances_changed(all_files, keywords, window_sz, gap_sz)\n",
    "    X = vectorizer.fit_transform(x[0] for x in iterator if not users.append(x[2]) and not y.append(x[1]) and not neighbors.append(x[3]))\n",
    "    print('X.shape=%s' % (str(X.shape)))\n",
    "    X=hstack((X, np.array([neighbors]).T)).tocsr()\n",
    "    y = np.array(y)\n",
    "    print('X.shape=%s' % (str(X.shape)))\n",
    "    cv = KFold(len(y), 10, shuffle=False)  # Don't shuffle b/c we don't want a user in both training and testing set.\n",
    "    accuracies = []\n",
    "    model_mod = LogisticRegression(penalty='l2', C=1)\n",
    "    for train_ind, test_ind in cv:\n",
    "        model_mod.fit(X[train_ind],y[train_ind])   \n",
    "        accuracies.append(accuracy_score(y[test_ind], model_mod.predict(X[test_ind])))\n",
    "    model_mod.fit(X, y)\n",
    "    print_top_terms(model_mod, vectorizer)\n",
    "    \n",
    "        \n",
    "    from sklearn.metrics import precision_recall_fscore_support\n",
    "    y_true = y\n",
    "    y_pred = model_mod.predict(X)\n",
    "    print(precision_recall_fscore_support(y_true, y_pred))\n",
    "    \n",
    "   \n",
    "    print( np.mean(accuracies))\n",
    "    return(np.mean(accuracies))#y,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X.shape=(398, 7502)\n",
      "X.shape=(398, 7503)\n",
      "[('boa', -0.69292672146408174), ('sp', -0.40686612830855057), ('new', -0.40255358456934365), ('vamo', -0.38118001287581815), ('vida', -0.3779631159001382), ('MENTION', -0.35092272479647607), ('ferramenta', -0.33926531356820899), ('gente THIS_IS_A_URL', -0.3263607042867776), ('quarta-feira 12', -0.32567948751111686), ('fátima', -0.31653045439831617), ('obrigado', -0.31623735314122087), ('THIS_IS_A_URL foto', -0.31392556501246099), ('sigam', -0.31294227181066286), ('2013', -0.31054076234894729), ('boa tarde', -0.30563872508612511), ('motivos', -0.30303671127818377), ('final', -0.29919245020280383), ('psdb', -0.29903859572813724), ('sao', -0.29854215585500932), ('mim', -0.29821185792983274), ('mexe', -0.29573100686290982), ('MENTION THIS_IS_A_URL', -0.29041133984980738), ('HASHTAG_thevoicebrasil', -0.28848215010172457), ('médico', -0.28838941746996372), ('novo', -0.28325525918681599), ('linda', -0.28231619496354216), ('arma', -0.28167394467358314), ('pode', -0.28129531799480201), ('felicidade', -0.27504543068889359), ('HASHTAG_donaxepa', -0.2728298283200804)] \n",
      "\n",
      "[('gol', 0.28403019478477681), ('preguiça', 0.28866898631781673), ('mudar', 0.29108898523508719), ('joga', 0.29704714076484623), ('corrupção', 0.29880293449981377), ('passa', 0.3023232907037921), ('bem', 0.30234418837646609), ('amiga', 0.30251976711247525), ('tipo', 0.30276662526010445), ('p', 0.30620251267788084), ('HASHTAG_debatenaglobo', 0.30890028944781517), ('15', 0.3156056470258925), ('falando', 0.32094520434978424), ('THIS_IS_A_URL', 0.32478113342246101), ('THIS_IS_A_URL acabou', 0.33098576499136295), ('brasil', 0.34679855671429516), ('sim', 0.34847396732109015), ('globo', 0.35680576149722443), ('cruzeiro', 0.36006843170754127), ('chegando', 0.36106839891429121), ('sábado', 0.36772952645698348), ('ontem', 0.37978350458329996), ('pt', 0.38030902730805322), ('volta', 0.40004622771245751), ('photo', 0.41616196527891103), ('dilma', 0.43693433206322729), ('perfil', 0.49554946982672249), ('HASHTAG_sextadosdvcomsrtwitteiro', 0.50080071471511944), ('THIS_IS_A_URL rt', 0.50844199232137099), ('HASHTAG_sabadodetremuranosdv', 0.77039267444696269)]\n",
      "(array([ 0.96097561,  0.98963731]), array([ 0.98994975,  0.95979899]), array([ 0.97524752,  0.9744898 ]), array([199, 199]))\n",
      "0.585705128205\n"
     ]
    }
   ],
   "source": [
    "all_files = list(glob.glob(DIR + '/Timeline/*.txt'))\n",
    "acc = do_expt(all_files, collapse_mentions=True, collapse_digits=False, binary=False, ngram_range=(1,2),\n",
    "            min_df=2, use_idf=True, norm='l2', window_sz=20, gap_sz=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error rate: 0.03\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Predicted</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Expected</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>197</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8</td>\n",
       "      <td>191</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Predicted    0    1\n",
       "Expected           \n",
       "0          197    2\n",
       "1            8  191"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#model metrics of our model\n",
    "confusion_matrix(acc[0], acc[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "collapse_mentions=False  collapse_digits=False  binary=True  ngram_range=(1, 2)  min_df=2  use_idf=False  norm=None  window_sz=50  gap_sz=100\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-72-62b463ab84c3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0moptions\u001b[0m \u001b[1;32min\u001b[0m \u001b[0moption_iter\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'  '\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'%s=%s'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mopt\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mopt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margnames\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m     \u001b[0macc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdo_expt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_files\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'acc=%g'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0macc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m''\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-71-9b47dd130524>\u001b[0m in \u001b[0;36mdo_expt\u001b[1;34m(all_files, collapse_mentions, collapse_digits, binary, ngram_range, min_df, use_idf, norm, window_sz, gap_sz)\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[0mneighbors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[0miterator\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0miterate_instances_changed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_files\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeywords\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwindow_sz\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgap_sz\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m     \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0miterator\u001b[0m \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0musers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mneighbors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'X.shape=%s'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m     \u001b[0mX\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mhstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mneighbors\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtocsr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/lib64/python3.4/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   1283\u001b[0m             \u001b[0mTf\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0midf\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mweighted\u001b[0m \u001b[0mdocument\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mterm\u001b[0m \u001b[0mmatrix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1284\u001b[0m         \"\"\"\n\u001b[1;32m-> 1285\u001b[1;33m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mTfidfVectorizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1286\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_tfidf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1287\u001b[0m         \u001b[1;31m# X is already a transformed view of raw_documents so\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/lib64/python3.4/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m    802\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    803\u001b[0m         vocabulary, X = self._count_vocab(raw_documents,\n\u001b[1;32m--> 804\u001b[1;33m                                           self.fixed_vocabulary_)\n\u001b[0m\u001b[0;32m    805\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    806\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbinary\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/lib64/python3.4/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m_count_vocab\u001b[1;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[0;32m    736\u001b[0m         \u001b[0mindptr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_make_int_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    737\u001b[0m         \u001b[0mindptr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 738\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mraw_documents\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    739\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mfeature\u001b[0m \u001b[1;32min\u001b[0m \u001b[0manalyze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    740\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-71-9b47dd130524>\u001b[0m in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[0mneighbors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[0miterator\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0miterate_instances_changed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_files\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeywords\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwindow_sz\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgap_sz\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m     \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0miterator\u001b[0m \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0musers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mneighbors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'X.shape=%s'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m     \u001b[0mX\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mhstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mneighbors\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtocsr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-61-5057cb376323>\u001b[0m in \u001b[0;36miterate_instances_changed\u001b[1;34m(files, keywords, window_sz, gap_sz)\u001b[0m\n\u001b[0;32m     77\u001b[0m                                         \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     78\u001b[0m                             \u001b[1;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mreversed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnlines\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 79\u001b[1;33m                                 \u001b[0mtweet_neighbor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     80\u001b[0m                                 \u001b[1;32mif\u001b[0m \u001b[0mmatches_keywords\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtweet_neighbor\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'text'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mkeywords\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     81\u001b[0m                                     \u001b[1;32mif\u001b[0m \u001b[0mparse_date\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtweet_neighbor\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'created_at'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mparse_date\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdate2_neg\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mparse_date\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtweet_neighbor\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'created_at'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0mparse_date\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdate2_neg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/lib64/python3.4/json/__init__.py\u001b[0m in \u001b[0;36mloads\u001b[1;34m(s, encoding, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[0;32m    316\u001b[0m             \u001b[0mparse_int\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mparse_float\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    317\u001b[0m             parse_constant is None and object_pairs_hook is None and not kw):\n\u001b[1;32m--> 318\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_default_decoder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    319\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mcls\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    320\u001b[0m         \u001b[0mcls\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mJSONDecoder\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/lib64/python3.4/json/decoder.py\u001b[0m in \u001b[0;36mdecode\u001b[1;34m(self, s, _w)\u001b[0m\n\u001b[0;32m    341\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    342\u001b[0m         \"\"\"\n\u001b[1;32m--> 343\u001b[1;33m         \u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraw_decode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0midx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0m_w\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    344\u001b[0m         \u001b[0mend\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_w\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    345\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mend\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/lib64/python3.4/json/decoder.py\u001b[0m in \u001b[0;36mraw_decode\u001b[1;34m(self, s, idx)\u001b[0m\n\u001b[0;32m    357\u001b[0m         \"\"\"\n\u001b[0;32m    358\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 359\u001b[1;33m             \u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscan_once\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0midx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    360\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    361\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merrmsg\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Expecting value\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# This will run all combinations (hour or so to complete)\n",
    "from itertools import product\n",
    "\n",
    "collapse_mentions_opts = [False, True]\n",
    "collapse_digits_opts = [False, True]\n",
    "binary_opts = [True, False]\n",
    "ngrams_opts = [(1, 2), (1, 1)]\n",
    "min_df_opts = [2, 4]\n",
    "use_idf_opts = [False, True]\n",
    "norm_opts = [None, 'l2']\n",
    "window_sz_opts = [50, 20, 10, 5, 1]\n",
    "gap_opts = [100, 50, 20, 10, 1]\n",
    "\n",
    "argnames = ['collapse_mentions', 'collapse_digits', 'binary', 'ngram_range',\n",
    "            'min_df', 'use_idf', 'norm', 'window_sz', 'gap_sz']\n",
    "\n",
    "option_iter = product(collapse_mentions_opts, collapse_digits_opts,\n",
    "                      binary_opts, ngrams_opts, min_df_opts,\n",
    "                      use_idf_opts, norm_opts, window_sz_opts,\n",
    "                      gap_opts)\n",
    "    \n",
    "    \n",
    "all_files = list(glob.glob(DIR + '/Timeline/*.txt'))\n",
    "results = []\n",
    "for options in option_iter:\n",
    "    print('  '.join('%s=%s' % (name, opt) for name, opt in zip(argnames, options)))\n",
    "    acc = do_expt(all_files, *options)\n",
    "    print('acc=%g' % acc)\n",
    "    print('')\n",
    "    results.append((acc, options))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# See how this setting changes with gap size.\n",
    "xx = []\n",
    "yy = []\n",
    "for gap in np.arange(150)[::10] + 10:\n",
    "    acc = do_expt(all_files, collapse_mentions=True, collapse_digits=False, binary=False, ngram_range=(1,2),\n",
    "                min_df=2, use_idf=True, norm='l2', window_sz=gap, gap_sz=100)\n",
    "    xx.append(gap)\n",
    "    yy.append(acc)\n",
    "    #print(gap, acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-68-28a892a510f7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mxx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0myy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'bo-'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Intervalo'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mylabel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Acurácia'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/lib64/python3.4/site-packages/matplotlib/pyplot.py\u001b[0m in \u001b[0;36mplot\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   3097\u001b[0m         \u001b[0max\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhold\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhold\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3098\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3099\u001b[1;33m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0max\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3100\u001b[0m         \u001b[0mdraw_if_interactive\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3101\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/lib64/python3.4/site-packages/matplotlib/axes/_axes.py\u001b[0m in \u001b[0;36mplot\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1372\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1373\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_lines\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1374\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_line\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1375\u001b[0m             \u001b[0mlines\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1376\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/lib64/python3.4/site-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36madd_line\u001b[1;34m(self, line)\u001b[0m\n\u001b[0;32m   1502\u001b[0m             \u001b[0mline\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_clip_path\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1503\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1504\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_update_line_limits\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1505\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mline\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_label\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1506\u001b[0m             \u001b[0mline\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_label\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'_line%d'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlines\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/lib64/python3.4/site-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36m_update_line_limits\u001b[1;34m(self, line)\u001b[0m\n\u001b[0;32m   1513\u001b[0m         \u001b[0mFigures\u001b[0m \u001b[0mout\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mdata\u001b[0m \u001b[0mlimit\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mgiven\u001b[0m \u001b[0mline\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mupdating\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataLim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1514\u001b[0m         \"\"\"\n\u001b[1;32m-> 1515\u001b[1;33m         \u001b[0mpath\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mline\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_path\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1516\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvertices\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1517\u001b[0m             \u001b[1;32mreturn\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/lib64/python3.4/site-packages/matplotlib/lines.py\u001b[0m in \u001b[0;36mget_path\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    872\u001b[0m         \"\"\"\n\u001b[0;32m    873\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_invalidy\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_invalidx\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 874\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecache\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    875\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_path\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    876\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/lib64/python3.4/site-packages/matplotlib/lines.py\u001b[0m in \u001b[0;36mrecache\u001b[1;34m(self, always)\u001b[0m\n\u001b[0;32m    582\u001b[0m                 \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mma\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0myconv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    583\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 584\u001b[1;33m                 \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0myconv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    585\u001b[0m             \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    586\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/lib64/python3.4/site-packages/numpy/core/numeric.py\u001b[0m in \u001b[0;36masarray\u001b[1;34m(a, dtype, order)\u001b[0m\n\u001b[0;32m    460\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    461\u001b[0m     \"\"\"\n\u001b[1;32m--> 462\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    463\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    464\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0masanyarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: setting an array element with a sequence."
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXYAAAEACAYAAACnJV25AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nfXeCAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f794d232198>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.plot(xx, yy, 'bo-')\n",
    "plt.xlabel('Intervalo')\n",
    "plt.ylabel('Acurácia')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def iterate_Testing_instances(path, keywords, negative_window,files_test):\n",
    "    \"\"\"\n",
    "    Return an iterator over tuples containing:\n",
    "    (concatenated tweet text, label, username)\n",
    "    For each user in path, we find the first tweet containing one of the specified keywords.\n",
    "    We then create one positive instance, containing all tweets prior to the matched tweet.\n",
    "    We also create one negative instance, which is the same as the positive instance, except\n",
    "    the N most recent tweets are removed (where N is set by the negative_window parameter).\n",
    "    We additionally filter users if they use one of the keywords in one of their first `negative_window`\n",
    "    tweets. This is to we have enough tweets to make a negative example.\n",
    "    \"\"\" \n",
    "    for fname in files_test:#glob.glob(path + '/*.txt'):\n",
    "        user = filename2user(fname)\n",
    "        \n",
    "        lines = []\n",
    "        var=None\n",
    "        tweetlist=[]\n",
    "        count=0\n",
    "        for i, line in enumerate(open(path+fname)):\n",
    "            js = json.loads(line)\n",
    "            # exclude people who use keyword within first `window` of tweets.\n",
    "            if i <= negative_window and matches_keywords(js['text'], keywords):\n",
    "                #print('skipping', fname, 'because uses keyword in first', negative_window, 'tweets')\n",
    "                break\n",
    "            if i > negative_window and matches_keywords(js['text'], keywords):\n",
    "                print(user)\n",
    "                var=''\n",
    "                mylines = lines[-100:]\n",
    "                #for li, l in enumerate(lines[-100:][::-1]):  # just look at the most recent 50 tweets.\n",
    "                for li, l in enumerate(mylines):\n",
    "                    var = var + ' ' + l\n",
    "                    # yield(var,user)\n",
    "                    # yield(l,user)\n",
    "                    yield (' '.join(mylines[max(0, li-10):li]), user)\n",
    "                    count=count+1\n",
    "                    if count==1500:\n",
    "                        break\n",
    "                #tweetlist=[]\n",
    "                var=None\n",
    "                break\n",
    "            lines.append(js['text'])\n",
    "            \n",
    "y_test = []\n",
    "users_test = []\n",
    "negative_window = 10\n",
    "# The loop below iterates over each instance and vectorizes the text.\n",
    "# Simulataneously, we append to the y (labels) and users lists.\n",
    "# We do this to avoid having to store all the text in memory at once and to \n",
    "# only require one loop through the files.\n",
    "files_test=[]\n",
    "for _, _, arquivos in os.walk(DIR + '/Timeline/'): arquivos#list of all files in the directory\n",
    "for f in arquivos:\n",
    "    if f not in set(files_train):\n",
    "        files_test.append(f)\n",
    "\n",
    "print(len(arquivos))\n",
    "print(len(set(files_test)))\n",
    "print(len(files_train))\n",
    "iterator1 = iterate_Testing_instances(DIR + '/Timeline/', keywords, negative_window, files_test)\n",
    "X_ = vectorizer.transform(x[0] for x in iterator1 if not users_test.append(x[1]))\n",
    "print('read %d instances into X matrix with shape %s' % (len(users_test), str(X_.shape)))\n",
    "#print('label distribution=', Counter(y))\n",
    "#y = np.array(y)\n",
    "#users = np.array(users)\n",
    "#print(type(y))\n",
    "iterator1 = iterate_Testing_instances(DIR + '/Timeline/', keywords, negative_window,files_test)\n",
    "iter1=list(iterator1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pred=model_mod.predict_proba(X_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "count =0\n",
    "dictv = defaultdict(list)\n",
    "for o in zip(iter1,pred):\n",
    "    #print(o[0][1])\n",
    "    dictv[o[0][1]].append(o[1][1])\n",
    "print(len(dictv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%pylab inline\n",
    "def is_interesting(vals):\n",
    "    # if there's a significant jump between adjacent cells.\n",
    "    for vi, vj in zip(vals[:-1], vals[1:]):\n",
    "        if abs(vj - vi) > .00:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "fig = plt.figure(figsize=(40,10))\n",
    "for k,v in dictv.items():\n",
    "    if is_interesting(dictv[k]):\n",
    "        plt.plot(dictv[k])\n",
    "print(len(dictv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pickle \n",
    "\n",
    "output = open('predmodel.pkl', 'wb')\n",
    "pickle.dump(pred, output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def plot_probs(dictv):\n",
    "    %pylab inline\n",
    "    fig = plt.figure(figsize=(40,10))\n",
    "    \n",
    "    for k, v in dictv.items():\n",
    "        if is_interesting(v):\n",
    "            i=1\n",
    "            y=[]\n",
    "            x=[]\n",
    "            t = 1/len(dictv[k])\n",
    "            for g in dictv[k]:\n",
    "                y.append(g)\n",
    "                x.append(i*t)\n",
    "                i = i+1\n",
    "            plt.xlabel('% of total tweets')\n",
    "            plt.ylabel('Probability')\n",
    "            plt.plot(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_probs(dictv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'module' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-112-2c0afae24453>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0municodedata\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'não'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: 'module' object is not callable"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['agora', 'my_stop_word', 'string', 'test', 'um']"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction import text\n",
    "my_words = set(['my_stop_word_1','my_stop_word_2','hi','para','de','kkk'])\n",
    "my_stop_words = text.ENGLISH_STOP_WORDS.union(my_words)\n",
    "vectorizer = text.TfidfVectorizer(stop_words=set(my_stop_words),max_features=15000)\n",
    "XX = vectorizer.fit_transform([\"Hi, this is a test string my_stop_word_2 Hi kkk my_stop_word agora para de um\"])\n",
    "vectorizer.get_feature_names()\n",
    "#[u'hi', u'my_stop_word', u'string', u'test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
