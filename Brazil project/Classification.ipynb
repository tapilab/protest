{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re \n",
    "import string\n",
    "import timestring\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    punc_re = '[' + '\\\\!\\\\\"\\\\#\\\\$\\\\%\\\\&\\\\\\'\\\\(\\\\)\\\\*\\\\+\\\\,\\\\-\\\\.\\\\/\\\\:\\\\;\\\\<\\\\=\\\\>\\\\?\\\\@\\\\[\\\\\\\\\\\\]\\\\_\\\\{\\\\|\\\\}' + ']'\n",
    "    #print punc_re\n",
    "    text = text.lower()\n",
    "    text = re.sub('#(\\S+)', r'HASHTAG_\\1', text)\n",
    "    text = re.sub('@(\\S+)', r'MENTION_\\1', text)\n",
    "    text = re.sub('http\\S+', 'THIS_IS_A_URL', text)\n",
    "    text = re.sub(r'(.)\\1\\1\\1+', r'\\1', text)\n",
    "    text = re.sub(r'[0-9]', '9', text)\n",
    "    toks = []\n",
    "    for tok in text.split():\n",
    "        tok = re.sub(r'^(' + punc_re + '+)', r'\\1 ', tok)\n",
    "        #print '1',tok\n",
    "        tok = re.sub(r'(' + punc_re + '+)$', r' \\1', tok)\n",
    "        #print '2',tok\n",
    "\n",
    "        for subtok in tok.split():\n",
    "            #print '3',subtok\n",
    "            if re.search('\\w', subtok):\n",
    "                toks.append(subtok)\n",
    "    return toks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "DIR = 'D:/TCC/Data/Timeline/vemprarua/'\n",
    "import os, io, json, codecs\n",
    "\n",
    "os.chdir(DIR)\n",
    "date_ative = []\n",
    "countuser = 0\n",
    "user = []\n",
    "\n",
    "#for file in glob.glob(\"*.txt\"):\n",
    " #   print 'file ', file, '\\n'\n",
    "with io.open('CleuzaBapti.txt','r',encoding=\"utf-8\") as f, codecs.open(\"CleuzaBapti - Copy.txt\", 'w+', encoding='utf-8') as fi:\n",
    "    for line in  reversed(f.readlines()):  \n",
    "        tweet= json.loads(line)\n",
    "        tweet['tokened_text'] = tokenize(tweet['text'])\n",
    "        fi.write(json.dumps(tweet,ensure_ascii=False,encoding='utf-8')+'\\n')\n",
    "\n",
    "        #if countuser == 100:\n",
    "         #   break\n",
    "        countuser+=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweet created before 7 days: \n",
      " tweet date 2015-08-08 00:00:00 < 2015-08-09 00:00:00\n"
     ]
    }
   ],
   "source": [
    "DIR= 'C:/Users/Elaine/Documents/Data/Timeline/vemprarua/'\n",
    "os.chdir(DIR)\n",
    "\n",
    "#for file in glob.glob(\"*.txt\"):\n",
    "    #print 'file ', file, '\\n'\n",
    "positive_tweets = []\n",
    "easy_negative_sample = []\n",
    "hard_negative_sample = []\n",
    "N=5\n",
    "\n",
    "with io.open(DIR+'TatianaRCosta.txt.txt','r',encoding=\"utf-8\") as f:\n",
    "    lines = f.readlines()\n",
    "    for i in xrange(len(lines)):\n",
    "        tweet= json.loads(lines[i])\n",
    "        if 'vempraRua' in tweet['text'] or 'vemprarua' in tweet['text'] or 'vem pra rua' in tweet['text']or 'VEMPRARUA' in tweet['text'] or 'VemPraRua' in tweet['text']:\n",
    "            positive_tweets.append(tweet)\n",
    "            date = timestring.Date(tweet['time_tweet'])\n",
    "            i=i-1\n",
    "            j=i-1\n",
    "            tweet_date = date\n",
    "            while  tweet_date >= date - '7 day': \n",
    "                tweet= json.loads(lines[i])                \n",
    "                if timestring.Date(tweet['time_tweet']) < date - '7 day':\n",
    "                    print 'Tweet created before 7 days: \\n tweet date',timestring.Date(tweet['time_tweet']) ,'<', date - '7 day'\n",
    "                    break\n",
    "                else:\n",
    "                    easy_negative_sample.append(tweet)\n",
    "                    i=i-1\n",
    "                    tweet_date = timestring.Date(tweet['time_tweet'])\n",
    "                if i == 1:\n",
    "                    print 'comeco do arquivo'\n",
    "                    break\n",
    "                    \n",
    "            while len(hard_negative_sample)<N:\n",
    "                tweet= json.loads(lines[j])    \n",
    "                #print tweet['time_tweet']\n",
    "                hard_negative_sample.append(tweet)\n",
    "                j=j-1\n",
    "                if j == 1:\n",
    "                    print 'comeco do arquivo'\n",
    "                    break\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Sun Aug 16 19:19:25 +0000 2015 #vemprarua #forapt #londrinanaluta hellen_ukstin @ PraÃ§a Da Bandeira https://t.co/CI1yduR61x\n",
      "7\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "print len(positive_tweets), positive_tweets[0]['created_at'],positive_tweets[0]['text']\n",
    "print len(easy_negative_sample)\n",
    "\n",
    "#for f in easy_negative_sample:\n",
    " #   print f['time_tweet']\n",
    "    \n",
    "print len(hard_negative_sample)\n",
    "#for f in hard_negative_sample:\n",
    " #   print f['time_tweet']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13\n"
     ]
    }
   ],
   "source": [
    "all_tweets = easy_negative_sample+positive_tweets+hard_negative_sample\n",
    "print len(all_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "UnicodeEncodeError",
     "evalue": "'ascii' codec can't encode character u'\\xed' in position 25: ordinal not in range(128)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnicodeEncodeError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-232-fd590a211dda>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m vectorizer = TfidfVectorizer(decode_error='ignore', ngram_range=(1, 2), max_df=1., min_df=2,\n\u001b[0;32m      2\u001b[0m                              use_idf=True, tokenizer=tokenize, binary=False, norm='l2')\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'text'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mall_tweets\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;32mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Vectorized %d tweets. Found %d terms.'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mfeatures\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvectorizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_feature_names\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Elaine\\Anaconda\\lib\\site-packages\\sklearn\\feature_extraction\\text.pyc\u001b[0m in \u001b[0;36mfit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   1280\u001b[0m             \u001b[0mTf\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0midf\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mweighted\u001b[0m \u001b[0mdocument\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mterm\u001b[0m \u001b[0mmatrix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1281\u001b[0m         \"\"\"\n\u001b[1;32m-> 1282\u001b[1;33m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mTfidfVectorizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1283\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_tfidf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1284\u001b[0m         \u001b[1;31m# X is already a transformed view of raw_documents so\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Elaine\\Anaconda\\lib\\site-packages\\sklearn\\feature_extraction\\text.pyc\u001b[0m in \u001b[0;36mfit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m    815\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    816\u001b[0m         vocabulary, X = self._count_vocab(raw_documents,\n\u001b[1;32m--> 817\u001b[1;33m                                           self.fixed_vocabulary_)\n\u001b[0m\u001b[0;32m    818\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    819\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbinary\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Elaine\\Anaconda\\lib\\site-packages\\sklearn\\feature_extraction\\text.pyc\u001b[0m in \u001b[0;36m_count_vocab\u001b[1;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[0;32m    746\u001b[0m         \u001b[0mindptr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    747\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mraw_documents\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 748\u001b[1;33m             \u001b[1;32mfor\u001b[0m \u001b[0mfeature\u001b[0m \u001b[1;32min\u001b[0m \u001b[0manalyze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    749\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    750\u001b[0m                     \u001b[0mj_indices\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvocabulary\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfeature\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Elaine\\Anaconda\\lib\\site-packages\\sklearn\\feature_extraction\\text.pyc\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(doc)\u001b[0m\n\u001b[0;32m    232\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    233\u001b[0m             return lambda doc: self._word_ngrams(\n\u001b[1;32m--> 234\u001b[1;33m                 tokenize(preprocess(self.decode(doc))), stop_words)\n\u001b[0m\u001b[0;32m    235\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    236\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-228-b9aad16baf6f>\u001b[0m in \u001b[0;36mtokenize\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0mpunc_re\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'['\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'\\\\!\\\\\"\\\\#\\\\$\\\\%\\\\&\\\\\\'\\\\(\\\\)\\\\*\\\\+\\\\,\\\\-\\\\.\\\\/\\\\:\\\\;\\\\<\\\\=\\\\>\\\\?\\\\@\\\\[\\\\\\\\\\\\]\\\\_\\\\{\\\\|\\\\}'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m']'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[1;31m#print punc_re\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0mtext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'utf-8'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m     \u001b[0mtext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mre\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msub\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'#(\\S+)'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34mr'HASHTAG_\\1'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mtext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mre\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msub\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'@(\\S+)'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34mr'MENTION_\\1'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Elaine\\Anaconda\\lib\\encodings\\utf_8.pyc\u001b[0m in \u001b[0;36mdecode\u001b[1;34m(input, errors)\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'strict'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mcodecs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutf_8_decode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mIncrementalEncoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcodecs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIncrementalEncoder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mUnicodeEncodeError\u001b[0m: 'ascii' codec can't encode character u'\\xed' in position 25: ordinal not in range(128)"
     ]
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(decode_error='ignore', ngram_range=(1, 2), max_df=1., min_df=2,\n",
    "                             use_idf=True, tokenizer=tokenize, binary=False, norm='l2')\n",
    "X = vectorizer.fit_transform(t['text'] for t in all_tweets)\n",
    "print('Vectorized %d tweets. Found %d terms.' % (X.shape[0], X.shape[1]))\n",
    "features = np.array(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'HASHTAG_bomdia' u'HASHTAG_bomdia HASHTAG_deusnocomando'\n",
      " u'HASHTAG_criancaesperanca' u'HASHTAG_deusnocomando'\n",
      " u'HASHTAG_deusnocomando HASHTAG_meuanjomeguarda'\n",
      " u'HASHTAG_deusnocomando HASHTAG_vivendoeaprendendo'\n",
      " u'HASHTAG_for\\xe7anaperuca'\n",
      " u'HASHTAG_for\\xe7anaperuca HASHTAG_vamoqvamo\\u2026'\n",
      " u'HASHTAG_mariapassanaftrente'\n",
      " u'HASHTAG_mariapassanaftrente HASHTAG_for\\xe7anaperuca'\n",
      " u'HASHTAG_meuanjomeguarda'\n",
      " u'HASHTAG_meuanjomeguarda HASHTAG_mariapassanaftrente'\n",
      " u'HASHTAG_meuanjomeguarda\\u2026'\n",
      " u'HASHTAG_meuanjomeguarda\\u2026 THIS_IS_A_URL' u'HASHTAG_s\\xf3acho'\n",
      " u'HASHTAG_s\\xf3acho HASHTAG_criancaesperanca' u'HASHTAG_vamoqvamo\\u2026'\n",
      " u'HASHTAG_vamoqvamo\\u2026 THIS_IS_A_URL' u'HASHTAG_vivendoeaprendendo'\n",
      " u'HASHTAG_vivendoeaprendendo HASHTAG_meuanjomeguarda\\u2026'\n",
      " u'MENTION_mjournalstudio' u'MENTION_mjournalstudio THIS_IS_A_URL'\n",
      " u'THIS_IS_A_URL' u'THIS_IS_A_URL quer' u'a' u'a brilhar' u'a cantar'\n",
      " u'alegria' u'alegria bom' u'amor' u'amor o' u'artista' u'artista fabio'\n",
      " u'as' u'as doa\\xe7\\xf5es' u'aves' u'aves a' u'bom' u'bom dia' u'brilhar'\n",
      " u'brilhar \\u2600\\ufe0fas' u'cantar' u'cantar THIS_IS_A_URL' u'certo'\n",
      " u'certo HASHTAG_deusnocomando' u'com' u'com alegria' u'com amor'\n",
      " u'come\\xe7a' u'come\\xe7a com' u'de' u'de recursos' u'destinadas'\n",
      " u'destinadas \\xe0s' u'dia' u'dia come\\xe7a' u'do' u'do artista'\n",
      " u'doa\\xe7\\xf5es' u'doa\\xe7\\xf5es n\\xe3o' u'd\\xe1' u'd\\xe1 tudo' u'escolas'\n",
      " u'escolas p\\xfablicas' u'fabio' u'fabio kawallys' u'kawallys'\n",
      " u'kawallys THIS_IS_A_URL' u'mais' u'mais \\xfatil' u'mas' u'mas sempre'\n",
      " u'muito' u'muito mais' u'multifuncionais' u'multifuncionais nas' u'nas'\n",
      " u'nas escolas' u'n\\xe3o' u'n\\xe3o s\\xe3o' u'o' u'o sol' u'pq' u'pq as'\n",
      " u'p\\xf4ster' u'p\\xf4ster do' u'p\\xfablicas' u'p\\xfablicas muito' u'quer'\n",
      " u'quer um' u'recursos' u'recursos multifuncionais' u'rt'\n",
      " u'rt MENTION_mjournalstudio' u'salas' u'salas de' u'sempre'\n",
      " u'sempre d\\xe1' u'sol' u'sol a' u's\\xe3o' u's\\xe3o destinadas' u'tudo'\n",
      " u'tudo certo' u'um' u'um p\\xf4ster' u'\\xe0s' u'\\xe0s salas' u'\\xfatil'\n",
      " u'\\xfatil HASHTAG_s\\xf3acho' u'\\u2600\\ufe0fas' u'\\u2600\\ufe0fas aves']\n"
     ]
    }
   ],
   "source": [
    "print features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
