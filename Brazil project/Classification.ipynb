{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re \n",
    "import string\n",
    "import timestring\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cross_validation import cross_val_score, KFold\n",
    "import glob, os\n",
    "import numpy as np\n",
    "from sklearn import svm\n",
    "from collections import defaultdict\n",
    "from scipy.sparse import csr_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    punc_re = '[' + '\\\\!\\\\\"\\\\#\\\\$\\\\%\\\\&\\\\\\'\\\\(\\\\)\\\\*\\\\+\\\\,\\\\-\\\\.\\\\/\\\\:\\\\;\\\\<\\\\=\\\\>\\\\?\\\\@\\\\[\\\\\\\\\\\\]\\\\_\\\\{\\\\|\\\\}' + ']'\n",
    "    text = text.lower()\n",
    "    text = re.sub('#(\\S+)', r'HASHTAG_\\1', text)\n",
    "    text = re.sub('@(\\S+)', r'MENTION_\\1', text)\n",
    "    text = re.sub('http\\S+', 'THIS_IS_A_URL', text)\n",
    "    text = re.sub(r'(.)\\1\\1\\1+', r'\\1', text)\n",
    "    text = re.sub(r'[0-9]', '9', text)\n",
    "    toks = []\n",
    "    for tok in text.split():\n",
    "        tok = re.sub(r'^(' + punc_re + '+)', r'\\1 ', tok)\n",
    "        tok = re.sub(r'(' + punc_re + '+)$', r' \\1', tok)\n",
    "        for subtok in tok.split():\n",
    "            if re.search('\\w', subtok):\n",
    "                toks.append(subtok)\n",
    "    return toks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(decode_error='ignore', ngram_range=(1, 2), max_df=1., min_df=2,\n",
    "                             use_idf=True, tokenizer=tokenize, binary=False, norm='l2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DIR = '/data/2/protest'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['foradilma', 'fora dilma', 'forapt', 'fora pt', 'vemprarua', 'vem pra rua']"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def read_keywords(path):\n",
    "    return [s.strip().lower() for s in open(path)]\n",
    "    \n",
    "keywords = read_keywords(DIR + '/keywords.txt')\n",
    "keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipping /data/2/protest/Timeline/MariaFeistauer.txt.txt because uses keyword in first 10 tweets\n",
      "skipping /data/2/protest/Timeline/Matredamandio.txt.txt because uses keyword in first 10 tweets\n",
      "skipping /data/2/protest/Timeline/edmarmbastos.txt.txt because uses keyword in first 10 tweets\n",
      "skipping /data/2/protest/Timeline/cajoso1.txt.txt because uses keyword in first 10 tweets\n",
      "skipping /data/2/protest/Timeline/FOFURA1055.txt.txt because uses keyword in first 10 tweets\n",
      "skipping /data/2/protest/Timeline/usiuva.txt.txt because uses keyword in first 10 tweets\n",
      "skipping /data/2/protest/Timeline/ElMarriachi43.txt.txt because uses keyword in first 10 tweets\n",
      "skipping /data/2/protest/Timeline/OslecMac74.txt.txt because uses keyword in first 10 tweets\n",
      "skipping /data/2/protest/Timeline/CSobania.txt.txt because uses keyword in first 10 tweets\n",
      "skipping /data/2/protest/Timeline/wmedsantos.txt.txt because uses keyword in first 10 tweets\n",
      "skipping /data/2/protest/Timeline/herminioaneto.txt.txt because uses keyword in first 10 tweets\n",
      "skipping /data/2/protest/Timeline/marchelaum.txt.txt because uses keyword in first 10 tweets\n",
      "skipping /data/2/protest/Timeline/jeff_ms.txt.txt because uses keyword in first 10 tweets\n",
      "skipping /data/2/protest/Timeline/ClauLBusch.txt.txt because uses keyword in first 10 tweets\n",
      "skipping /data/2/protest/Timeline/marcioadriano79.txt.txt because uses keyword in first 10 tweets\n",
      "skipping /data/2/protest/Timeline/angelobgu.txt.txt because uses keyword in first 10 tweets\n",
      "skipping /data/2/protest/Timeline/byadiniz35.txt.txt because uses keyword in first 10 tweets\n",
      "skipping /data/2/protest/Timeline/Rodrigo_Cunha81.txt.txt because uses keyword in first 10 tweets\n",
      "skipping /data/2/protest/Timeline/LuttiLippe.txt.txt because uses keyword in first 10 tweets\n",
      "skipping /data/2/protest/Timeline/LUCIANO_DAVID.txt.txt because uses keyword in first 10 tweets\n",
      "skipping /data/2/protest/Timeline/ElaineAlmeida.txt.txt because uses keyword in first 10 tweets\n",
      "skipping /data/2/protest/Timeline/cabral1956.txt.txt because uses keyword in first 10 tweets\n",
      "skipping /data/2/protest/Timeline/0686966667ff496.txt.txt because uses keyword in first 10 tweets\n",
      "skipping /data/2/protest/Timeline/Edisoaress.txt.txt because uses keyword in first 10 tweets\n",
      "skipping /data/2/protest/Timeline/LelexDutra.txt.txt because uses keyword in first 10 tweets\n",
      "skipping /data/2/protest/Timeline/felixponto30.txt.txt because uses keyword in first 10 tweets\n",
      "skipping /data/2/protest/Timeline/caranovanocongr.txt.txt because uses keyword in first 10 tweets\n",
      "skipping /data/2/protest/Timeline/FernandaMixalhs.txt.txt because uses keyword in first 10 tweets\n",
      "skipping /data/2/protest/Timeline/jwbranda.txt.txt because uses keyword in first 10 tweets\n",
      "skipping /data/2/protest/Timeline/najinhas.txt.txt because uses keyword in first 10 tweets\n",
      "skipping /data/2/protest/Timeline/iabsilva2.txt.txt because uses keyword in first 10 tweets\n",
      "skipping /data/2/protest/Timeline/Vicente_Q_Filho.txt.txt because uses keyword in first 10 tweets\n",
      "skipping /data/2/protest/Timeline/claudinarosa.txt.txt because uses keyword in first 10 tweets\n",
      "skipping /data/2/protest/Timeline/tnasc86.txt.txt because uses keyword in first 10 tweets\n",
      "skipping /data/2/protest/Timeline/_marilda.txt.txt because uses keyword in first 10 tweets\n",
      "skipping /data/2/protest/Timeline/MovBrasillivre.txt.txt because uses keyword in first 10 tweets\n",
      "read 570 instances into X matrix with shape (570, 1509078)\n",
      "label distribution= Counter({0: 285, 1: 285})\n"
     ]
    }
   ],
   "source": [
    "import os, io, json, codecs\n",
    "\n",
    "def matches_keywords(text, keywords):\n",
    "    \"\"\" Return true if any keyword is a substring of this text, ignoring case. \"\"\"\n",
    "    text = text.lower()\n",
    "    for kw in keywords:\n",
    "        if kw in text:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def filename2user(fname):\n",
    "    \"\"\"Convert filename like this\n",
    "      /data/2/protest/Timeline/MandinhaSimone.txt.txt\n",
    "    into a username like\n",
    "      MandinhaSimone\n",
    "    \"\"\"\n",
    "    return re.sub(r'^([^\\.]+)\\..+', r'\\1', os.path.basename(fname))\n",
    "\n",
    "def iterate_instances(path, keywords, negative_window):\n",
    "    \"\"\"\n",
    "    Return an iterator over tuples containing:\n",
    "    (concatenated tweet text, label, username)\n",
    "    For each user in path, we find the first tweet containing one of the specified keywords.\n",
    "    We then create one positive instance, containing all tweets prior to the matched tweet.\n",
    "    We also create one negative instance, which is the same as the positive instance, except\n",
    "    the N most recent tweets are removed (where N is set by the negative_window parameter).\n",
    "    We additionally filter users if they use one of the keywords in one of their first `negative_window`\n",
    "    tweets. This is to we have enough tweets to make a negative example.\n",
    "    \"\"\"\n",
    "    for fname in glob.glob(path + '/*.txt'):\n",
    "        user = filename2user(fname)\n",
    "        lines = []\n",
    "        for i, line in enumerate(open(fname)):\n",
    "            js = json.loads(line)\n",
    "            # exclude people who use keyword within first `window` of tweets.\n",
    "            if i <= negative_window and matches_keywords(js['text'], keywords):\n",
    "                print('skipping', fname, 'because uses keyword in first', negative_window, 'tweets')\n",
    "                break\n",
    "            if i > negative_window and matches_keywords(js['text'], keywords):\n",
    "                yield (' '.join(lines), 1, user)\n",
    "                yield (' '.join(lines[:-negative_window]), 0, user)\n",
    "                break\n",
    "            lines.append(js['text'])\n",
    "            \n",
    "y = []\n",
    "users = []\n",
    "negative_window = 10\n",
    "# The loop below iterates over each instance and vectorizes the text.\n",
    "# Simulataneously, we append to the y (labels) and users lists.\n",
    "# We do this to avoid having to store all the text in memory at once and to \n",
    "# only require one loop through the files.\n",
    "iterator = iterate_instances(DIR + '/Timeline', keywords, negative_window)\n",
    "X = vectorizer.fit_transform(x[0] for x in iterator if not users.append(x[2]) and not y.append(x[1]))\n",
    "print('read %d instances into X matrix with shape %s' % (len(users), str(X.shape)))\n",
    "print('label distribution=', Counter(y))\n",
    "y = np.array(y)\n",
    "users = np.array(users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average 10-fold cross validation accuracy=0.5070 (std=0.02)\n",
      "accuracy on training data=0.5912\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cross_validation import cross_val_score\n",
    "from sklearn.cross_validation import KFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "model_mod = LogisticRegression(penalty='l2', C=2.6)\n",
    "model_mod.fit(X, y)\n",
    "\n",
    "# 10 Cross-validation accuracy\n",
    "cv = KFold(len(y), 10, shuffle=False)  # Don't shuffle b/c we don't want a user in both training and testing set.\n",
    "accuracies = []\n",
    "for train_ind, test_ind in cv:\n",
    "    model_mod.fit(X[train_ind],y[train_ind])   \n",
    "    accuracies.append(accuracy_score(y[test_ind], model_mod.predict(X[test_ind])))\n",
    "    \n",
    "print('Average 10-fold cross validation accuracy=%.4f (std=%.2f)' % (np.mean(accuracies), np.std(accuracies)))\n",
    "\n",
    "predicted = model_mod.predict(X)\n",
    "print('accuracy on training data=%.4f' % accuracy(y, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('MENTION_vixxavier acredito', -0.45075759660025527),\n",
      " ('MENTION_vixxavier', -0.41407313570559506),\n",
      " ('MENTION_blogdopim ninguem', -0.15640751111880824),\n",
      " ('MENTION_ingdoc MENTION_lobaoeletrico', -0.15640751111880824),\n",
      " ('ninguem gosta', -0.15640751111880824),\n",
      " ('um comunista', -0.14475965064777133),\n",
      " ('acredito', -0.14364939133552279),\n",
      " ('rt MENTION_ingdoc', -0.13806387255262972),\n",
      " ('THIS_IS_A_URL MENTION_blogdopim', -0.13745794246007784),\n",
      " ('MENTION_ingdoc', -0.13162607663537565),\n",
      " ('gosta mais', -0.12903761300571143),\n",
      " ('MENTION_coralnet', -0.118599809289055),\n",
      " ('vica', -0.11830596568532399),\n",
      " ('MENTION_lobaoeletrico THIS_IS_A_URL', -0.1127486502840931),\n",
      " ('MENTION_exame_com', -0.11026014328283816),\n",
      " ('comunista', -0.10503150541381466),\n",
      " ('via', -0.09271825578922839),\n",
      " ('dinheiro do', -0.088981057340257963),\n",
      " ('no', -0.086119766487644606),\n",
      " ('THIS_IS_A_URL via', -0.086096073671798098),\n",
      " ('da', -0.085941107074687467),\n",
      " ('MENTION_david_gds', -0.085875762703050251),\n",
      " ('MENTION_david_gds MENTION_coralnet', -0.085875762703050251),\n",
      " ('MENTION_givestyls', -0.085875762703050251),\n",
      " ('MENTION_italo_filho THIS_IS_A_URL', -0.085875762703050251),\n",
      " ('THIS_IS_A_URL MENTION_italo_filho', -0.085875762703050251),\n",
      " ('rt MENTION_david_gds', -0.085875762703050251),\n",
      " ('MENTION_turquim9', -0.083371475086543717),\n",
      " ('MENTION_stanleyburburin', -0.0813858064055928),\n",
      " ('sua', -0.07807298684386027)]\n",
      "[('fracos', 0.17386996677213548),\n",
      " ('todos às', 0.17805818747090377),\n",
      " ('pt', 0.17901032261248795),\n",
      " ('MENTION_matheusbrazmatt', 0.18117715082220229),\n",
      " ('o', 0.18147717149795675),\n",
      " ('MENTION_tvrevolta THIS_IS_A_URL', 0.18257242743513843),\n",
      " ('MENTION_genpeternelli', 0.19043534326101313),\n",
      " ('MENTION_jornaloglobo', 0.19333433036741368),\n",
      " ('HASHTAG_sabadodetremuranosdv', 0.19402330943626081),\n",
      " ('MENTION_xerosadabarra99', 0.19692529055026495),\n",
      " ('bundões', 0.19949359872228689),\n",
      " ('fracos incompetentes', 0.19949359872228689),\n",
      " ('MENTION_nascimento9999', 0.2111600392986385),\n",
      " ('MENTION_aecioneves', 0.21348140245458608),\n",
      " ('MENTION_edgaarcia', 0.21405477881704402),\n",
      " ('HASHTAG_vaiadilma', 0.21412239151374421),\n",
      " ('e', 0.22231492878000475),\n",
      " ('MENTION_tode_olho', 0.24609152452606453),\n",
      " ('panelinhas', 0.25128035061347931),\n",
      " ('MENTION_candidaz', 0.25588553383064705),\n",
      " ('MENTION_yuliyagro', 0.26094563877581856),\n",
      " ('a', 0.30172121497800558),\n",
      " ('dilma', 0.33383334436112461),\n",
      " ('MENTION_tvrevolta', 0.33990765380663884),\n",
      " ('rt MENTION_tvrevolta', 0.33990765380663884),\n",
      " ('rt', 0.34562015091708936),\n",
      " ('THIS_IS_A_URL rt', 0.4202850964829995),\n",
      " ('THIS_IS_A_URL', 0.44337540143639426),\n",
      " ('HASHTAG_lulanacadeia', 0.45837841798697371),\n",
      " ('MENTION_folhapolitica', 0.66675955416228594)]\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint \n",
    "coefs = sorted(zip(vectorizer.get_feature_names(), model_mod.coef_[0]),key=lambda x:x[1])\n",
    "pprint(coefs[:30])\n",
    "pprint(coefs[-30:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results:\n",
    "\n",
    "| negative_window      |   train acc  | test acc | n users |\n",
    "| -------------------- | ------------ | -------- | ------- |\n",
    "| 10                   | .591         | .507     | 285     |\n",
    "| 20                   | .610         | .509     | 274     |\n",
    "| 30                   | .614         | .515     | 264     |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
