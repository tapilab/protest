{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re \n",
    "import string\n",
    "import timestring\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cross_validation import cross_val_score, KFold\n",
    "import glob, os\n",
    "import numpy as np\n",
    "from sklearn import svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Aug 16 16:41:54 +0000 2015\n",
      "start of the file\n"
     ]
    }
   ],
   "source": [
    "#DIR = 'C:/Users/Elaine/Desktop/TCC/all timeline/'\n",
    "import os, io, json, codecs\n",
    "\n",
    "os.chdir(DIR)\n",
    "date_ative = []\n",
    "countuser = 0\n",
    "user = []\n",
    "positive_sample=[]\n",
    "negative_sample = []\n",
    "\n",
    "#for file in glob.glob(\"*.txt\"):\n",
    " #   print 'file ', file, '\\n'\n",
    "with io.open(DIR+'DaviWesler.txt.txt','r',encoding=\"utf-8\") as f:#, codecs.open(\"CleuzaBapti - Copy.txt\", 'w+', encoding='utf-8') as fi:\n",
    "    lines = f.readlines()\n",
    "    i=0\n",
    "    for i in xrange(len(lines)):\n",
    "        tweet= json.loads(lines[i])        \n",
    "        if 'foraDILMA' in tweet['text'] or 'foradilma' in tweet['text'] or 'Fora Dilma' in tweet['text'] or 'foraDilma' in tweet['text'] or 'FORADILMA' in tweet['text'] or 'fora Dilma' in tweet['text'] or 'FORA DILMA' in tweet['text'] or 'ForaDilma' in tweet['text'] or'foraPT' in tweet['text'] or 'FORAPT' in tweet['text'] or 'fora pt' in tweet['text']or 'FORA PT' in tweet['text'] or 'ForaPT' in tweet['text'] or'vempraRua' in tweet['text'] or 'vemprarua' in tweet['text'] or 'vem pra rua' in tweet['text']or 'VEMPRARUA' in tweet['text'] or 'VemPraRua' in tweet['text']:\n",
    "            print tweet['created_at']\n",
    "            j=i-1\n",
    "            while j>1:\n",
    "                #print j\n",
    "                tweet1= json.loads(lines[j])    \n",
    "                #print tweet['time_tweet']\n",
    "                positive_sample.append(tweet1)\n",
    "                j=j-1\n",
    "                if j == 1:\n",
    "                    print 'start of the file'\n",
    "                    break\n",
    "            negative_sample = positive_sample[10:]\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RT @curiosiddades: Timão e Pumba, são vcs? Esse filhote de javali e esse suricato viraram grandes amigos em zoológico da Inglaterra. http:/…\n"
     ]
    }
   ],
   "source": [
    "for p in positive_sample:\n",
    "    print p['text']\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorized 236 tweets. Found 403 terms.\n"
     ]
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(decode_error='ignore', ngram_range=(1, 2), max_df=1., min_df=2,\n",
    "                             use_idf=True, binary=False, norm='l2')\n",
    "X = vectorizer.fit_transform(t['text'] for t in positive_sample)\n",
    "print('Vectorized %d tweets. Found %d terms.' % (X.shape[0], X.shape[1]))\n",
    "features = np.array(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([u'100', u'104', u'104 anos', u'11', u'2015', u'29', u'84',\n",
       "       u'84 anos', u'aben\\xe7oe', u'acesse', u'acesse http', u'acho',\n",
       "       u'acho que', u'acidente', u'acidente de', u'acompanhe',\n",
       "       u'acompanhe http', u'acordei', u'adbrasil', u'afffmanu', u'agora',\n",
       "       u'ainda', u'alagoas', u'alerta', u'amigo', u'amigos', u'amor',\n",
       "       u'ano', u'ano no', u'anos', u'anos http', u'anos que', u'anuncia',\n",
       "       u'anunciai_', u'ao', u'ao seu', u'ao vivo', u'aos', u'apenas',\n",
       "       u'app', u'ap\\xf3s', u'ap\\xf3s acidente', u'aqui', u'ara\\xfajo',\n",
       "       u'ara\\xfajo morre', u'argentina', u'as', u'as pessoas',\n",
       "       u'assembleia', u'assembleia de', u'assembl\\xe9ia',\n",
       "       u'assembl\\xe9ia de', u'assim', u'assista', u'assista ao', u'at\\xe9',\n",
       "       u'aula', u'banda', u'bar\\xe7a', u'bar\\xe7a championsleaguefinal',\n",
       "       u'bem', u'boa', u'boa noite', u'bom', u'bom dia', u'brasil',\n",
       "       u'brasileira', u'brincadero', u'cada', u'campe\\xf5es', u'cantor',\n",
       "       u'carro', u'casa', u'ceia', u'celular', u'championsleaguefinal',\n",
       "       u'chegar', u'chuva', u'cidade', u'co', u'coca', u'com', u'comemora',\n",
       "       u'como', u'cong', u'congresso', u'congresso dos', u'contra',\n",
       "       u'copa', u'copa do', u'coronel', u'cristiano',\n",
       "       u'cristiano ara\\xfajo', u'cristo', u'cristofobia', u'culto',\n",
       "       u'culto de', u'da', u'da argentina', u'da assembl\\xe9ia',\n",
       "       u'da maconha', u'da verdade', u'daqui', u'das', u'daviwesler',\n",
       "       u'de', u'de aula', u'de carro', u'de deus', u'de f\\xedsica',\n",
       "       u'de jovens', u'de julho', u'de matem\\xe1tica', u'de novo',\n",
       "       u'de santa', u'de semana', u'de tornado', u'define', u'deus',\n",
       "       u'deus aben\\xe7oe', u'deus http', u'dia', u'dia da', u'dia de',\n",
       "       u'dia do', u'dia internacional', u'dialetos', u'dias', u'divulgue',\n",
       "       u'diz', u'diz que', u'do', u'do ano', u'do bar\\xe7a', u'do estado',\n",
       "       u'do mundo', u'do que', u'dormir', u'dos', u'dos gide\\xf5es',\n",
       "       u'duz\\xe3o', u'd\\xe1', u'd\\xfavida', u'ebd', u'ele', u'em',\n",
       "       u'em todo', u'em zool\\xf3gico', u'entram', u'ent\\xe3o', u'espero',\n",
       "       u'esp\\xedrito', u'esp\\xedrito santo', u'essa', u'esse', u'estado',\n",
       "       u'estado de', u'estar', u'estava', u'est\\xe1', u'eu', u'exatamente',\n",
       "       u'facebook', u'fam\\xedlia', u'faz', u'faz sorrir', u'fazer', u'fim',\n",
       "       u'final', u'final de', u'fleck', u'foi', u'folha', u'foto',\n",
       "       u'fotos', u'frio', u'friozinho', u'f\\xedsica', u'g1', u'g1 http',\n",
       "       u'g1nacopa', u'galera', u'gays', u'gideoes', u'gideoes assista',\n",
       "       u'gide\\xf5es', u'gide\\xf5es 2015', u'gol', u'gooool', u'gooool do',\n",
       "       u'gospelprime', u'governador', u'gra\\xe7a', u'gra\\xe7a paz', u'gt',\n",
       "       u'gt http', u'hist\\xf3ria', u'hoje', u'homem', u'horas',\n",
       "       u'hor\\xe1rio', u'http', u'http co', u'https', u'https co',\n",
       "       u'huehuehue', u'huehuehue http', u'idade', u'iead', u'iead cong',\n",
       "       u'igreja', u'imaginando', u'infelizmente', u'instagram',\n",
       "       u'internacional', u'ir', u'israel', u'israel http', u'isso',\n",
       "       u'jos\\xe9', u'jos\\xe9 neco', u'jovens', u'jovens da', u'julho',\n",
       "       u'j\\xe1', u'j\\xfapiter', u'kkkkk', u'legalizar', u'luto', u'l\\xe1',\n",
       "       u'l\\xedder', u'maconha', u'mais', u'manh\\xe3', u'manu',\n",
       "       u'marcofeliciano', u'marcofeliciano parab\\xe9ns', u'marisa_lobo',\n",
       "       u'mas', u'matem\\xe1tica', u'me', u'me faz', u'melhor', u'mesmo',\n",
       "       u'meu', u'morre', u'morre aos', u'muito', u'mundo', u'm\\xfasica',\n",
       "       u'm\\xfasica boa', u'na', u'na ebd', u'nada', u'nada http',\n",
       "       u'namorou', u'neco', u'neco http', u'nem', u'nesta', u'no',\n",
       "       u'noite', u'nos', u'nosso', u'nova', u'novo', u'nunca', u'n\\xe3o',\n",
       "       u'n\\xe3o sei', u'n\\xf3s', u'odialetos', u'olhos', u'ontem', u'os',\n",
       "       u'ou', u'ouvir', u'para', u'para legalizar', u'parab\\xe9ns',\n",
       "       u'parab\\xe9ns assembl\\xe9ia', u'paran\\xe1', u'participe', u'partir',\n",
       "       u'partir de', u'pastor', u'paz', u'paz m\\xfasica', u'pela',\n",
       "       u'perder', u'pessoa', u'pessoas', u'pior', u'pior do',\n",
       "       u'planejando', u'planeta', u'pode', u'por', u'por l\\xe1', u'porque',\n",
       "       u'portalr7', u'pouco', u'pouco da', u'pr', u'pra', u'precisa',\n",
       "       u'presidente', u'primeiro', u'pro', u'prova', u'prova de',\n",
       "       u'quando', u'que', u'que deus', u'que me', u'quem', u'quero',\n",
       "       u'quinta', u'regi\\xe3o', u'rt', u'rt adbrasil', u'rt anunciai_',\n",
       "       u'rt brincadero', u'rt dialetos', u'rt folha', u'rt g1',\n",
       "       u'rt gideoes', u'rt gospelprime', u'rt marcofeliciano',\n",
       "       u'rt marisa_lobo', u'rt namorou', u'rt odialetos', u'rt portalr7',\n",
       "       u'rt uolnoticias', u'rt veja', u'rt vocenaosabiaq', u'sair',\n",
       "       u'samuel', u'santa', u'santo', u'saymon', u'se', u'segundo', u'sei',\n",
       "       u'seja', u'selfie', u'semana', u'semana foi', u'senhor',\n",
       "       u'senhor venha', u'ser', u'seu', u'silvio', u'sl', u'snap',\n",
       "       u'snap wsldavi', u'sofre', u'sorrir', u'sou', u'sua', u'suas',\n",
       "       u'surpreende', u's\\xe3o', u's\\xf3', u'tamb\\xe9m', u'tamb\\xe9m mas',\n",
       "       u'tava', u'te', u'tem', u'tempo', u'todo', u'todos', u'tomando',\n",
       "       u'tomando uma', u'tornado', u'tr\\xeas', u'tudo', u't\\xe1', u't\\xf4',\n",
       "       u'um', u'um pouco', u'uma', u'uns', u'uol', u'uolnoticias', u'vai',\n",
       "       u'vc', u'veja', u'vejo', u'venha', u'verdade', u'vez', u'vezes',\n",
       "       u'vezes no', u'vida', u'vivo', u'vocenaosabiaq',\n",
       "       u'vocenaosabiaq hoje', u'voc\\xea', u'v\\xe3o', u'wsldavi', u'xadrez',\n",
       "       u'zool\\xf3gico', u'\\xfanico'], \n",
       "      dtype='<U26')"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with codecs.open(\"C:/Users/Elaine/\"+'file_test', 'w+', encoding='utf-8') as fi:\n",
    "       for f in features:\n",
    "            fi.write(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    punc_re = '[' + '\\\\!\\\\\"\\\\#\\\\$\\\\%\\\\&\\\\\\'\\\\(\\\\)\\\\*\\\\+\\\\,\\\\-\\\\.\\\\/\\\\:\\\\;\\\\<\\\\=\\\\>\\\\?\\\\@\\\\[\\\\\\\\\\\\]\\\\_\\\\{\\\\|\\\\}' + ']'\n",
    "    #print punc_re\n",
    "    text = text.lower()\n",
    "    text = re.sub('#(\\S+)', r'HASHTAG_\\1', text)\n",
    "    text = re.sub('@(\\S+)', r'MENTION_\\1', text)\n",
    "    text = re.sub('http\\S+', 'THIS_IS_A_URL', text)\n",
    "    text = re.sub(r'(.)\\1\\1\\1+', r'\\1', text)\n",
    "    text = re.sub(r'[0-9]', '9', text)\n",
    "    toks = []\n",
    "    for tok in text.split():\n",
    "        tok = re.sub(r'^(' + punc_re + '+)', r'\\1 ', tok)\n",
    "        #print '1',tok\n",
    "        tok = re.sub(r'(' + punc_re + '+)$', r' \\1', tok)\n",
    "        #print '2',tok\n",
    "\n",
    "        for subtok in tok.split():\n",
    "            #print '3',subtok\n",
    "            if re.search('\\w', subtok):\n",
    "                toks.append(subtok)\n",
    "    return toks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
