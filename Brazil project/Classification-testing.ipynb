{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re \n",
    "import string\n",
    "import timestring\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cross_validation import cross_val_score, KFold\n",
    "import glob, os\n",
    "import numpy as np\n",
    "from sklearn import svm\n",
    "from collections import defaultdict\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "from sklearn.cross_validation import KFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "from scipy.sparse import hstack\n",
    "import json\n",
    "from scipy import sparse\n",
    "from sklearn.feature_extraction import text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DIR = '/data/2/protest'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Tokenizer:\n",
    "    def __init__(self, collapse_mentions=False, collapse_digits=True):\n",
    "        self.collapse_mentions = collapse_mentions\n",
    "        self.collapse_digits = collapse_digits\n",
    "        \n",
    "    def tokenize(self, text):\n",
    "        punc_re = '[' + '\\\\!\\\\\"\\\\#\\\\$\\\\%\\\\&\\\\\\'\\\\(\\\\)\\\\*\\\\+\\\\,\\\\-\\\\.\\\\/\\\\:\\\\;\\\\<\\\\=\\\\>\\\\?\\\\@\\\\[\\\\\\\\\\\\]\\\\_\\\\{\\\\|\\\\}' + ']'\n",
    "        text = text.lower()\n",
    "        text = re.sub('#(\\S+)', r'HASHTAG_\\1', text)\n",
    "        if self.collapse_mentions:\n",
    "            text = re.sub('@\\S+', 'MENTION', text)\n",
    "        else:\n",
    "            text = re.sub('@\\S+', 'MENTION_\\1', text)\n",
    "        text = re.sub('http\\S+', 'THIS_IS_A_URL', text)\n",
    "        text = re.sub(r'(.)\\1\\1\\1+', r'\\1', text)\n",
    "        if self.collapse_digits: # Numbers help!\n",
    "            text = re.sub(r'[0-9]', '9', text) \n",
    "        toks = []\n",
    "        for tok in text.split():\n",
    "            tok = re.sub(r'^(' + punc_re + '+)', r'\\1 ', tok)\n",
    "            tok = re.sub(r'(' + punc_re + '+)$', r' \\1', tok)\n",
    "            for subtok in tok.split():\n",
    "                if re.search('\\w', subtok):\n",
    "                    toks.append(subtok)\n",
    "        return toks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def print_top_terms(clf, vec, n=100):\n",
    "    coefs = sorted(zip(vec.get_feature_names(),clf.coef_[0]),key=lambda x:x[1])\n",
    "    print(coefs[0:30], '\\n')\n",
    "    print(coefs[-30:])\n",
    "    #feats = np.array(vec.get_feature_names())\n",
    "    #print('\\n'.join(feats[np.argsort(clf.coef_[0])[::-1][:n]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#read file with stopwords\n",
    "def read_stopwords(path):\n",
    "    return [s.strip().lower() for s in open(path)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#parse date from tweets\n",
    "def parse_date(datestring):\n",
    "    from datetime import datetime\n",
    "    \"\"\" Input, e.g., Mon Aug 24 19:41:14 +0000 2015\n",
    "    Output, e.g., datetime.datetime(2015, 8, 24, 0, 0)\"\"\"\n",
    "    \n",
    "    parts = datestring.split()\n",
    "    var = datetime.strptime(parts[1]+' ' +parts[2]+' '+ parts[5],'%b %d %Y')\n",
    "    return var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def filename2user(fname):\n",
    "    \"\"\"Convert filename like this\n",
    "      /data/2/protest/Timeline/MandinhaSimone.txt.txt\n",
    "    into a username like\n",
    "      MandinhaSimone\n",
    "    \"\"\"\n",
    "    return re.sub(r'^([^\\.]+)\\..+', r'\\1', os.path.basename(fname))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def matches_keywords(text, keywords):\n",
    "    \"\"\" Return true if any keyword is a substring of this text, ignoring case. \"\"\"\n",
    "    text = text.lower()\n",
    "    for kw in keywords:\n",
    "        if kw in text:\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def iterate_instances_changed(files, keywords, window_sz, gap_sz,files_train):\n",
    "    \"\"\"\n",
    "    Return an iterator over tuples containing:\n",
    "    (concatenated tweet text, label, username, percentage of neightbors who used one of the keywords)\n",
    "    For each user in path, we find the first tweet containing one of the specified keywords.\n",
    "    We then create one positive instance, defined as window = N tweets prior to the matched tweet.\n",
    "    We also create one negative instance, which is the same size as positive, but defined after a gap=g.\n",
    "    We additionally filter users if they use one of the keywords in one of their first (2*window + gap)\n",
    "    tweets. \n",
    "    Also we find neighbors (define as those with symetric mentions) who have also used one of the keywords \n",
    "    during positive or negative window \n",
    "    \"\"\"\n",
    "    DIR = '/home/elaine/Protest/protest/Brazil project/'\n",
    "    DIR2='/data/2/protest/mentions/'\n",
    "    dic = defaultdict(tuple)\n",
    "    pkl_file = open(DIR+'all_mentions_graph.pkl', 'rb')#open pickle file where the edges of the graph is saved\n",
    "    data1 = pickle.load(pkl_file)\n",
    "    for _, _, f in os.walk(DIR2): f    \n",
    "    for _, _, arquivos in os.walk(files): arquivos\n",
    "        \n",
    "    for fname in files_train:\n",
    "        \n",
    "        user = filename2user(arquivos[fname])\n",
    "        #print('this is', user)\n",
    "        lines = []\n",
    "        #countneg=0\n",
    "        #countpos=0\n",
    "        #print('treino',user)\n",
    "        for i, line in enumerate(open(files+arquivos[fname])):\n",
    "            js = json.loads(line)\n",
    "                # exclude people who use keyword within first `window` of tweets.\n",
    "            if i <= (2*window_sz + gap_sz) and matches_keywords(js['text'], keywords):\n",
    "                #print('skipping', arquivos[fname], 'because uses keyword in first', negative_window, 'tweets')\n",
    "                break\n",
    "            if i > (2*window_sz + gap_sz) and matches_keywords(js['text'], keywords):\n",
    "                #yield (' '.join(lines), 1, user)\n",
    "                # yield (' '.join(lines[:-negative_window]), 0, user)\n",
    "                pos_start = max(0, i - window_sz)\n",
    "                #print('positive window starts in',pos_start)\n",
    "                pos_end = i+1\n",
    "                #print('positive window finishes in',pos_end)\n",
    "\n",
    "                #print('lineslen: ',len(lines))\n",
    "                date1_pos = lines[pos_start][1]\n",
    "                #print('ksdjsk',lines[pos_start][1])\n",
    "                #print(pos_end)\n",
    "                date2_pos = lines[pos_end-2][1]\n",
    "\n",
    "                neg_start = max(0, i - gap_sz - (2 * window_sz))\n",
    "                #print('negative window starts in',neg_start)\n",
    "                neg_end = i - gap_sz - window_sz\n",
    "                #print('nagative window finishes in',neg_end)\n",
    "                date1_neg = lines[neg_start][1]\n",
    "                date2_neg = lines[neg_end][1]\n",
    "                \n",
    "                if data1.has_node(user) == True: #if that file is in the graph\n",
    "                    result1, result2,result3 = neighbors_hashtag_use(user, DIR2, date1_pos, date2_pos, date1_neg, date2_neg)\n",
    "                #print(result1, result2,result3)\n",
    "                \n",
    "                #dic[user]=((float(countpos)/float(len(neighbors))),float(countneg)/float(len(neighbors)))\n",
    "                #print('diccc',dic)\n",
    "                # print('ps=%d pe=%d ns=%d ne=%d' % (pos_start, pos_end, neg_start, neg_end))\n",
    "                #print('testeee',lines[0][pos_start:pos_end])\n",
    "                testepos = \" \".join(l[0] for l in lines[pos_start:pos_end])\n",
    "                testeneg = \" \".join(l[0] for l in lines[neg_start:neg_end])\n",
    "                \n",
    "                yield (testepos, 1, user, float(result1)/float(result3))\n",
    "                yield (testeneg, 0, user, float(result2)/float(result3))\n",
    "                \n",
    "                #yield (testepos, 1, user, float(countpos)/float(len(neighbors)))\n",
    "                #yield (testeneg, 0, user,float(countneg)/float(len(neighbors)))\n",
    "                #countneg=0\n",
    "                #countpos=0\n",
    "                break\n",
    "            lines.append((js['text'],js['created_at']))\n",
    "    #print(dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def iterate_Testing_instances(path,files_test):\n",
    "    \n",
    "    \"\"\"\n",
    "    Return an iterator over tuples containing:\n",
    "    (N concatenated tweets, user, percentage of neighbors who used one of the keywords)\n",
    "    For each user in path, we find the first tweet containing one of the specified keywords. \n",
    "    Then each 20 tweets are concatenated and also we get the percentage of neighbors who used one of the keywords\n",
    "    during the same window time as those 20 tweets.\n",
    "    \"\"\"\n",
    "    for _, _, arquivos in os.walk(path): arquivos\n",
    "    DIR = '/home/elaine/Protest/protest/Brazil project/'\n",
    "    pkl_file = open(DIR+'all_mentions_graph.pkl', 'rb')#open pickle file where the edges of the graph is saved\n",
    "    data1 = pickle.load(pkl_file)\n",
    "    DIR2='/data/2/protest/mentions/'\n",
    "\n",
    "    for fname in files_test:\n",
    "        \n",
    "        user = filename2user(arquivos[fname])\n",
    "        lines = []\n",
    "        var=None\n",
    "        count=0\n",
    "        date1=None\n",
    "        date2=None\n",
    "        \n",
    "        for i, line in enumerate(open(path+arquivos[fname])):\n",
    "            js = json.loads(line)\n",
    "            \n",
    "            if matches_keywords(js['text'], keywords):\n",
    "                #print('test',user)\n",
    "                v=''\n",
    "                count=0\n",
    "                #print('len of lines', len(lines))\n",
    "                for k,l in enumerate(lines[:][::-1]):\n",
    "                    #print(l)\n",
    "                    v=v+l[0]\n",
    "                    count=count+1\n",
    "                    if count == 1:\n",
    "                        date1 = l[1]\n",
    "                        #print('date1',date1)\n",
    "                    if count==20:\n",
    "                        date2 = l[1]\n",
    "                        #print('date2', date2)\n",
    "                        if data1.has_node(user) == True:\n",
    "                            result1,result2=neighbors_hashtag_use(user, DIR2, date2, date1)\n",
    "                        #print('count',count)\n",
    "                        count=0\n",
    "                        yield(v,user, float(result1)/float(result2))\n",
    "                        v=None\n",
    "                        v=\"\"\n",
    "                    elif(count<20 and k == (len(lines)-1)):  #PS.: if first tweet of user is with keyword this user is not counted!\n",
    "                        #print('count',count)\n",
    "                        date2 = l[1]\n",
    "                        if data1.has_node(user) == True:\n",
    "                            result1,result2=neighbors_hashtag_use(user, DIR2, date2, date1)\n",
    "                        count=0\n",
    "                        yield(v,user, float(result1)/float(result2))\n",
    "                        v=None\n",
    "                        v=\"\"\n",
    "                break\n",
    "            lines.append((js['text'],js['created_at']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def neighbors_hashtag_use(user, DIR2,date1_pos, date2_pos, date1_neg=None, date2_neg=None):\n",
    "    \n",
    "    \"\"\"\n",
    "    neighors of each user are checked for the use of any of the keywords during same the window time defined of positive \n",
    "    and negative instancees (define as the time each set starts and finishes)\n",
    "    \"\"\"\n",
    "    \n",
    "    countpos=0\n",
    "    countneg=0\n",
    "    DIR = '/home/elaine/Protest/protest/Brazil project/'\n",
    "    pkl_file = open(DIR+'all_mentions_graph.pkl', 'rb')#open pickle file where the edges of the graph is saved\n",
    "    data1 = pickle.load(pkl_file)\n",
    "    for _, _, f in os.walk(DIR2): f\n",
    "    neighbors=[]\n",
    "    #print(user, 'is in the graph!')\n",
    "    neighbors = data1.neighbors(user) #get all neighbors of the user\n",
    "    #print(user, 's neighbors are: ', neighbors)\n",
    "    \n",
    "    for n in neighbors:\n",
    "        if n +'.txt' in f:#if that file is in the directory\n",
    "            #print('vizinho: ', n)\n",
    "            neighbor = open(DIR2 + n +'.txt','r')\n",
    "            nlines = neighbor.readlines()\n",
    "            for t in reversed(nlines):\n",
    "                tweet_neighbor = json.loads(t) \n",
    "                #print(tweet_neighbor['created_at'])\n",
    "                #print(parse_date(tweet_neighbor['created_at']) ,'>', parse_date(date1_pos) )\n",
    "                if matches_keywords(tweet_neighbor['text'],keywords):\n",
    "                    if parse_date(tweet_neighbor['created_at']) > parse_date(date1_pos) and parse_date(tweet_neighbor['created_at']) < parse_date(date2_pos):\n",
    "                        #if date2_neg == None and date1_neg ==None:\n",
    "                            #print('entrou2')\n",
    "                            #print(user, 'has', neighbors)\n",
    "                            #print(n, 'posted keyword')\n",
    "                        #print(n, 'posted keyword in', parse_date(tweet_neighbor['created_at']))\n",
    "                        #print('entrou!!')\n",
    "                        countpos = countpos+1\n",
    "                        break\n",
    "            if date2_neg != None and date1_neg !=None:\n",
    "                for t in reversed(nlines):\n",
    "                    tweet_neighbor = json.loads(t) \n",
    "                    if matches_keywords(tweet_neighbor['text'],keywords):\n",
    "                        if parse_date(tweet_neighbor['created_at']) < parse_date(date2_neg) and parse_date(tweet_neighbor['created_at']) > parse_date(date2_neg):\n",
    "                            #print(n, 'posted negkeyword in', parse_date(tweet_neighbor['created_at']))\n",
    "                            #print('1entrou!!')\n",
    "                            countneg = countneg+1\n",
    "                            break\n",
    "    if date2_neg != None and date1_neg !=None:                        \n",
    "        return countpos, countneg, len(neighbors)\n",
    "    else:\n",
    "        return countpos, len(neighbors)\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#define vectorizer \n",
    "toker = Tokenizer(collapse_digits=False, collapse_mentions=True)\n",
    "\n",
    "stopwords = set(read_stopwords(DIR + '/stopwords.txt'))    \n",
    "my_stop_words = text.ENGLISH_STOP_WORDS.union(stopwords)\n",
    "\n",
    "vectorizer = TfidfVectorizer(binary=False, decode_error='ignore',ngram_range=(1,2),\n",
    "                                 max_df=1.0, min_df=2, use_idf=True,\n",
    "                                 tokenizer=toker.tokenize,\n",
    "                                 norm='l2', stop_words=set(my_stop_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['foradilma', 'fora dilma', 'forapt', 'fora pt', 'vemprarua', 'vem pra rua']"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def read_keywords(path):\n",
    "    return [s.strip().lower() for s in open(path)]\n",
    "    \n",
    "keywords = read_keywords(DIR + '/keywords.txt')\n",
    "keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def plot_probs(dictv,name):\n",
    "    %pylab inline\n",
    "    fig = plt.figure(figsize=(40,10))\n",
    " \n",
    "    for k, v in dictv.items():\n",
    "        i=1\n",
    "        y=[]\n",
    "        x=[]\n",
    "        t = 1/len(dictv[k])\n",
    "        for g in dictv[k]:\n",
    "            y.append(g)\n",
    "            x.append(i*t)\n",
    "            i = i+1\n",
    "        plt.xlabel('% of total tweets')\n",
    "        plt.ylabel('Probability')\n",
    "        plt.plot(x,y)\n",
    "    plt.savefig(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "288 33\n",
      "read 362 instances into X training matrix with shape (362, 6851)\n",
      "label distribution= Counter({0: 181, 1: 181})\n",
      "read 1068 instances into X_ testing matrix with shape (1068, 6851)\n",
      "Populating the interactive namespace from numpy and matplotlib\n",
      "Populating the interactive namespace from numpy and matplotlib\n",
      "289 32\n",
      "read 356 instances into X training matrix with shape (356, 6770)\n",
      "label distribution= Counter({0: 178, 1: 178})\n",
      "read 1843 instances into X_ testing matrix with shape (1843, 6770)\n",
      "Populating the interactive namespace from numpy and matplotlib\n",
      "Populating the interactive namespace from numpy and matplotlib\n",
      "289 32\n",
      "read 360 instances into X training matrix with shape (360, 6865)"
     ]
    }
   ],
   "source": [
    "# cross validation process\n",
    "\"\"\"\n",
    "Each time of the loop, diferent sets for training and testing are obtained, in order to have all users tested. \n",
    "We start calling iterate_instances_changed(), then it is vectorized as training set.\n",
    "After we call iterate_Testing_instances(), to get testing set and predict it.\n",
    "And then, we plot the results, as probability X % of tweets for each user.\n",
    "\"\"\"\n",
    "\n",
    "dictv = defaultdict(list)\n",
    "newdictv = defaultdict(list)\n",
    "\n",
    "model_mod = LogisticRegression(penalty='l2', C=1)\n",
    "\n",
    "treino_y=[]\n",
    "plots=[]\n",
    "t=0\n",
    "allusers=[]\n",
    "\n",
    "for _, _, arquivos in os.walk(DIR + '/Timeline/'): arquivos\n",
    "\n",
    "cv = KFold(len(arquivos), 10,shuffle=False)\n",
    "preds=None\n",
    "for train_ind, test_ind in cv: \n",
    "    print(len(train_ind), len(test_ind))\n",
    "    #files, keywords, window_sz, gap_sz,files_train\n",
    "    iterator = iterate_instances_changed(DIR + '/Timeline/', keywords, 20,100, train_ind)\n",
    "    treino_y=[]\n",
    "    users=[]\n",
    "    neighbors=[]\n",
    "    treino_X = vectorizer.fit_transform(x[0] for x in iterator if not users.append(x[2]) and not treino_y.append(x[1]) and not neighbors.append(x[3]))\n",
    "    treino_X = sparse.hstack((treino_X, np.array([neighbors]).T)).tocsr()\n",
    "    print('read %d instances into X training matrix with shape %s' % (len(users), str(treino_X.shape)))\n",
    "    print('label distribution=', Counter(treino_y))\n",
    "\n",
    "    iterator1 = iterate_Testing_instances(DIR + '/Timeline/', test_ind)\n",
    "    users1=[]\n",
    "    neighbors1=[]\n",
    "    X_ = vectorizer.transform(x[0] for x in iterator1 if not users1.append(x[1]) and not neighbors1.append(x[2]))\n",
    "    X_= sparse.hstack((X_,np.array([neighbors1]).T)).tocsr()\n",
    "    print('read %d instances into X_ testing matrix with shape %s' % (len(users1), str(X_.shape)))\n",
    "\n",
    "\n",
    "    treino_y = np.array(treino_y)\n",
    "    model_mod.fit(treino_X,treino_y)   \n",
    "    preds = model_mod.predict_proba(X_)\n",
    "    #print(len(preds))\n",
    "    \n",
    "    #iterator1 = iterate_Testing_instances(DIR + '/Timeline/', test_ind)\n",
    "    #iter1=list(iterator1)\n",
    "    #print(users1)\n",
    "    for o in zip(users1, preds):\n",
    "        #print(o[1][1])\n",
    "        #allusers.append(o[0])\n",
    "        dictv[(o[0])].append(o[1][1])\n",
    "        newdictv[(o[0])].append(o[1][1])\n",
    "\n",
    "    t=t+1\n",
    "    #print('users1',len(users1))\n",
    "\n",
    "    #print('dictv',len(dictv))\n",
    "    name = ['a','b','c','d','e','f','g','h','i','j']#names for each plot\n",
    "    name_sep = ['aa','bb','cc','dd','ee','ff','gg','hh','ii','jj']\n",
    "    plot_probs(dictv,name[t-1])\n",
    "    plot_probs(newdictv,name_sep[t-1])\n",
    "    #print('newdcit', len(newdictv))\n",
    "    newdictv = defaultdict(list)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_files = list(glob.glob(DIR + '/Timeline/*.txt'))\n",
    "window_sz=20  \n",
    "gap_sz=100\n",
    "users = []\n",
    "y=[]\n",
    "\n",
    "iterator = iterate_instances(all_files, keywords, window_sz, gap_sz)\n",
    "X = vectorizer.fit_transform(x[0] for x in iterator if not users.append(x[2]) and not y.append(x[1]))\n",
    "y = np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average 10-fold cross validation accuracy=0.5784 (std=0.05)\n",
      "accuracy on training data=0.9382\n"
     ]
    }
   ],
   "source": [
    "#y = np.array(y)\n",
    "model_mod = LogisticRegression(penalty='l2', C=1)\n",
    "model_mod.fit(X, y)\n",
    "\n",
    "\n",
    "# Compute accuracy\n",
    "def accuracy(truth, predicted):\n",
    "    return (1. * len([1 for tr, pr in zip(truth, predicted) if tr == pr]) / len(truth))\n",
    "\n",
    "# 10 Cross-validation accuracy\n",
    "cv = KFold(len(y), 10, shuffle=False)  # Don't shuffle b/c we don't want a user in both training and testing set.\n",
    "accuracies = []\n",
    "for train_ind, test_ind in cv:\n",
    "    model_mod.fit(X[train_ind],y[train_ind])   \n",
    "    accuracies.append(accuracy_score(y[test_ind], model_mod.predict(X[test_ind])))\n",
    "    \n",
    "print('Average 10-fold cross validation accuracy=%.4f (std=%.2f)' % (np.mean(accuracies), np.std(accuracies)))\n",
    "\n",
    "predicted = model_mod.predict(X)\n",
    "print('accuracy on training data=%.4f' % accuracy(y, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os, io, json, codecs\n",
    "\n",
    "def matches_keywords(text, keywords):\n",
    "    \"\"\" Return true if any keyword is a substring of this text, ignoring case. \"\"\"\n",
    "    text = text.lower()\n",
    "    for kw in keywords:\n",
    "        if kw in text:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def filename2user(fname):\n",
    "    \"\"\"Convert filename like this\n",
    "      /data/2/protest/Timeline/MandinhaSimone.txt.txt\n",
    "    into a username like\n",
    "      MandinhaSimone\n",
    "    \"\"\"\n",
    "    return re.sub(r'^([^\\.]+)\\..+', r'\\1', os.path.basename(fname))\n",
    "\n",
    "def iterate_instances(files, keywords, window_sz, gap_sz,files_train):\n",
    "    \"\"\"\n",
    "    Return an iterator over tuples containing:\n",
    "    (concatenated tweet text, label, username)\n",
    "    For each user in path, we find the first tweet containing one of the specified keywords.\n",
    "    We then create one positive instance, containing all tweets prior to the matched tweet.\n",
    "    We also create one negative instance, which is the same as the positive instance, except\n",
    "    the N most recent tweets are removed (where N is set by the negative_window parameter).\n",
    "    We additionally filter users if they use one of the keywords in one of their first `negative_window`\n",
    "    tweets. This is to we have enough tweets to make a negative example.\n",
    "    \"\"\"\n",
    "    for _, _, arquivos in os.walk(path): arquivos\n",
    "        \n",
    "    for fname in files_train:#glob.glob(path + '/*.txt'):\n",
    "        \n",
    "        user = filename2user(arquivos[fname])\n",
    "        lines = []\n",
    "        #print('treino',user)\n",
    "        for i, line in enumerate(open(path+arquivos[fname])):\n",
    "            js = json.loads(line)\n",
    "            # exclude people who use keyword within first `window` of tweets.\n",
    "            if i <= (2*window_sz + gap_sz) and matches_keywords(js['text'], keywords):\n",
    "                #print('skipping', fname, 'because uses keyword in first', negative_window, 'tweets')\n",
    "                break\n",
    "            if i > (2*window_sz + gap_sz) and matches_keywords(js['text'], keywords):\n",
    "                #yield (' '.join(lines), 1, user)\n",
    "                # yield (' '.join(lines[:-negative_window]), 0, user)\n",
    "                pos_start = max(0, i - window_sz)\n",
    "                pos_end = i+1\n",
    "                neg_start = max(0, i - gap_sz - (2 * window_sz))\n",
    "                neg_end = i - gap_sz - window_sz\n",
    "                # print('ps=%d pe=%d ns=%d ne=%d' % (pos_start, pos_end, neg_start, neg_end))\n",
    "                yield (' '.join(lines[pos_start:pos_end]), 1, user)\n",
    "                yield (' '.join(lines[neg_start:neg_end]), 0, user)\n",
    "                break\n",
    "            lines.append(js['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([ 0.94527363,  0.95431472]), array([ 0.95477387,  0.94472362]), array([ 0.95      ,  0.94949495]), array([199, 199]))\n"
     ]
    }
   ],
   "source": [
    "#precision, recall and F1 measures for both classes\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "y_true = y\n",
    "y_pred = model_mod.predict(X)\n",
    "print(precision_recall_fscore_support(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getRandomFile(path):\n",
    "  \"\"\"\n",
    "  Returns a random filename, chosen among the files of the given path.\n",
    "  \"\"\"\n",
    "  files = os.listdir(path)\n",
    "  index = random.randrange(0, len(files))\n",
    "  return files[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "myn\n",
      "2\n",
      "3\n",
      "4\n",
      "mlp\n",
      "5\n",
      "6\n",
      "7\n",
      "gob\n",
      "8\n",
      "9\n",
      "10\n",
      "less\n"
     ]
    }
   ],
   "source": [
    "ab=['d','v','b','o','g','p','l','m','n','y','m']\n",
    "v=''\n",
    "count=0\n",
    "for k,l in enumerate(ab[::-1]):\n",
    "    #print(l)\n",
    "    v=v+l\n",
    "    count=count+1\n",
    "    if count==3:\n",
    "        count=0\n",
    "        print(v)\n",
    "        v=None\n",
    "        v=\"\"\n",
    "    print(k)\n",
    "    if (count<3 and k==(len(ab)-1)):\n",
    "        print('less')\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['marcocanavarro',\n",
       " 'marcocanavarro',\n",
       " 'marcocanavarro',\n",
       " 'marcocanavarro',\n",
       " 'marcocanavarro',\n",
       " 'marcocanavarro',\n",
       " 'marcocanavarro',\n",
       " 'marcocanavarro',\n",
       " 'AugustoCampos3',\n",
       " 'AugustoCampos3',\n",
       " 'AugustoCampos3',\n",
       " 'AugustoCampos3',\n",
       " 'AugustoCampos3',\n",
       " 'edsondomingues2',\n",
       " 'edsondomingues2',\n",
       " 'edsondomingues2',\n",
       " 'edsondomingues2',\n",
       " 'edsondomingues2',\n",
       " 'edsondomingues2',\n",
       " 'edsondomingues2',\n",
       " 'edsondomingues2',\n",
       " 'edsondomingues2',\n",
       " 'edsondomingues2',\n",
       " 'edsondomingues2',\n",
       " 'edsondomingues2',\n",
       " 'edsondomingues2',\n",
       " 'edsondomingues2',\n",
       " 'edsondomingues2',\n",
       " 'edsondomingues2',\n",
       " 'edsondomingues2',\n",
       " 'edsondomingues2',\n",
       " 'edsondomingues2',\n",
       " 'edsondomingues2',\n",
       " 'edsondomingues2',\n",
       " 'edsondomingues2',\n",
       " 'edsondomingues2',\n",
       " 'edsondomingues2',\n",
       " 'edsondomingues2',\n",
       " 'edsondomingues2',\n",
       " 'edsondomingues2',\n",
       " 'edsondomingues2',\n",
       " 'edsondomingues2',\n",
       " 'edsondomingues2',\n",
       " 'edsondomingues2',\n",
       " 'ClaraCamargoL',\n",
       " 'ClaraCamargoL',\n",
       " 'ClaraCamargoL',\n",
       " 'ClaraCamargoL',\n",
       " 'ClaraCamargoL',\n",
       " 'ClaraCamargoL',\n",
       " 'walisonsr',\n",
       " 'walisonsr',\n",
       " 'walisonsr',\n",
       " 'walisonsr',\n",
       " 'walisonsr',\n",
       " 'walisonsr',\n",
       " 'walisonsr',\n",
       " 'walisonsr',\n",
       " 'walisonsr',\n",
       " 'walisonsr',\n",
       " 'walisonsr',\n",
       " 'walisonsr',\n",
       " 'walisonsr',\n",
       " 'walisonsr',\n",
       " 'walisonsr',\n",
       " 'walisonsr',\n",
       " 'walisonsr',\n",
       " 'walisonsr',\n",
       " 'walisonsr',\n",
       " 'walisonsr',\n",
       " 'walisonsr',\n",
       " 'walisonsr',\n",
       " 'walisonsr',\n",
       " 'walisonsr',\n",
       " 'walisonsr',\n",
       " 'walisonsr',\n",
       " 'walisonsr',\n",
       " 'walisonsr',\n",
       " 'walisonsr',\n",
       " 'walisonsr',\n",
       " 'walisonsr',\n",
       " 'walisonsr',\n",
       " 'walisonsr',\n",
       " 'walisonsr',\n",
       " 'walisonsr',\n",
       " 'walisonsr',\n",
       " 'walisonsr',\n",
       " 'walisonsr',\n",
       " 'walisonsr',\n",
       " 'walisonsr',\n",
       " 'walisonsr',\n",
       " 'walisonsr',\n",
       " 'walisonsr',\n",
       " 'walisonsr',\n",
       " 'walisonsr',\n",
       " 'walisonsr',\n",
       " 'walisonsr',\n",
       " 'walisonsr',\n",
       " 'walisonsr',\n",
       " 'walisonsr',\n",
       " 'walisonsr',\n",
       " 'walisonsr',\n",
       " 'walisonsr',\n",
       " 'walisonsr',\n",
       " 'walisonsr',\n",
       " 'walisonsr',\n",
       " 'martinhagatosa',\n",
       " 'martinhagatosa',\n",
       " 'martinhagatosa',\n",
       " 'martinhagatosa',\n",
       " 'martinhagatosa',\n",
       " 'martinhagatosa',\n",
       " 'martinhagatosa',\n",
       " 'martinhagatosa',\n",
       " 'martinhagatosa',\n",
       " 'martinhagatosa',\n",
       " 'martinhagatosa',\n",
       " 'martinhagatosa',\n",
       " 'martinhagatosa',\n",
       " 'martinhagatosa',\n",
       " 'martinhagatosa',\n",
       " 'martinhagatosa',\n",
       " 'martinhagatosa',\n",
       " 'martinhagatosa',\n",
       " 'martinhagatosa',\n",
       " 'martinhagatosa',\n",
       " 'martinhagatosa',\n",
       " 'martinhagatosa',\n",
       " 'martinhagatosa',\n",
       " 'martinhagatosa',\n",
       " 'martinhagatosa',\n",
       " 'martinhagatosa',\n",
       " 'martinhagatosa',\n",
       " 'martinhagatosa',\n",
       " 'martinhagatosa',\n",
       " 'IgorYamada',\n",
       " 'IgorYamada',\n",
       " 'IgorYamada',\n",
       " 'IgorYamada',\n",
       " 'IgorYamada',\n",
       " 'IgorYamada',\n",
       " 'IgorYamada',\n",
       " 'IgorYamada',\n",
       " 'IgorYamada',\n",
       " 'IgorYamada',\n",
       " 'IgorYamada',\n",
       " 'IgorYamada',\n",
       " 'IgorYamada',\n",
       " 'IgorYamada',\n",
       " 'IgorYamada',\n",
       " 'IgorYamada',\n",
       " 'IgorYamada',\n",
       " 'IgorYamada',\n",
       " 'IgorYamada',\n",
       " 'IgorYamada',\n",
       " 'IgorYamada',\n",
       " 'IgorYamada',\n",
       " 'IgorYamada',\n",
       " 'IgorYamada',\n",
       " 'IgorYamada',\n",
       " 'IgorYamada',\n",
       " 'IgorYamada',\n",
       " 'IgorYamada',\n",
       " 'IgorYamada',\n",
       " 'IgorYamada',\n",
       " 'IgorYamada',\n",
       " 'IgorYamada',\n",
       " 'IgorYamada',\n",
       " 'IgorYamada',\n",
       " 'IgorYamada',\n",
       " 'IgorYamada',\n",
       " 'IgorYamada',\n",
       " 'IgorYamada',\n",
       " 'IgorYamada',\n",
       " 'IgorYamada',\n",
       " 'IgorYamada',\n",
       " 'IgorYamada',\n",
       " 'IgorYamada',\n",
       " 'IgorYamada',\n",
       " 'IgorYamada',\n",
       " 'IgorYamada',\n",
       " 'IgorYamada',\n",
       " 'IgorYamada',\n",
       " 'IgorYamada',\n",
       " 'IgorYamada',\n",
       " 'IgorYamada',\n",
       " 'IgorYamada',\n",
       " 'IgorYamada',\n",
       " 'IgorYamada',\n",
       " 'IgorYamada',\n",
       " 'IgorYamada',\n",
       " 'IgorYamada',\n",
       " 'IgorYamada',\n",
       " 'IgorYamada',\n",
       " 'IgorYamada',\n",
       " 'IgorYamada',\n",
       " 'IgorYamada',\n",
       " 'IgorYamada',\n",
       " 'IgorYamada',\n",
       " 'IgorYamada',\n",
       " 'IgorYamada',\n",
       " 'IgorYamada',\n",
       " 'IgorYamada',\n",
       " 'IgorYamada',\n",
       " 'IgorYamada',\n",
       " 'IgorYamada',\n",
       " 'IgorYamada',\n",
       " 'IgorYamada',\n",
       " 'IgorYamada',\n",
       " 'IgorYamada',\n",
       " 'IgorYamada',\n",
       " 'mcberinguy',\n",
       " 'mcberinguy',\n",
       " 'mcberinguy',\n",
       " 'mcberinguy',\n",
       " 'mcberinguy',\n",
       " 'mcberinguy',\n",
       " 'mcberinguy',\n",
       " 'mcberinguy',\n",
       " 'mcberinguy',\n",
       " 'mcberinguy',\n",
       " 'mcberinguy',\n",
       " 'mcberinguy',\n",
       " 'mcberinguy',\n",
       " 'mcberinguy',\n",
       " 'mcberinguy',\n",
       " 'mcberinguy',\n",
       " 'mcberinguy',\n",
       " 'mcberinguy',\n",
       " 'mcberinguy',\n",
       " 'mcberinguy',\n",
       " 'mcberinguy',\n",
       " 'mcberinguy',\n",
       " 'mcberinguy',\n",
       " 'mcberinguy',\n",
       " 'mcberinguy',\n",
       " 'mcberinguy',\n",
       " 'mcberinguy',\n",
       " 'mcberinguy',\n",
       " 'mcberinguy',\n",
       " 'mcberinguy',\n",
       " 'mcberinguy',\n",
       " 'mcberinguy',\n",
       " 'mcberinguy',\n",
       " 'mcberinguy',\n",
       " 'mcberinguy',\n",
       " 'mcberinguy',\n",
       " 'mcberinguy',\n",
       " 'mcberinguy',\n",
       " 'mcberinguy',\n",
       " 'mcberinguy',\n",
       " 'mcberinguy',\n",
       " 'mcberinguy',\n",
       " 'mcberinguy',\n",
       " 'mcberinguy',\n",
       " 'mcberinguy',\n",
       " 'mcberinguy',\n",
       " 'mcberinguy',\n",
       " 'mcberinguy',\n",
       " 'mcberinguy',\n",
       " 'mcberinguy',\n",
       " 'mcberinguy',\n",
       " 'mcberinguy',\n",
       " 'mcberinguy',\n",
       " 'mcberinguy',\n",
       " 'mcberinguy',\n",
       " 'mcberinguy',\n",
       " 'mcberinguy',\n",
       " 'mcberinguy',\n",
       " 'mcberinguy',\n",
       " 'mcberinguy',\n",
       " 'mcberinguy',\n",
       " 'mcberinguy',\n",
       " 'mcberinguy',\n",
       " 'mcberinguy',\n",
       " 'mcberinguy',\n",
       " 'mcberinguy',\n",
       " 'mcberinguy',\n",
       " 'mcberinguy',\n",
       " 'mcberinguy',\n",
       " 'mcberinguy',\n",
       " 'mcberinguy',\n",
       " 'mcberinguy',\n",
       " 'mcberinguy',\n",
       " 'mcberinguy',\n",
       " 'mcberinguy',\n",
       " 'mcberinguy',\n",
       " 'mcberinguy',\n",
       " 'mcberinguy',\n",
       " 'mcberinguy',\n",
       " 'mcberinguy',\n",
       " 'mcberinguy',\n",
       " 'mcberinguy',\n",
       " 'mcberinguy',\n",
       " 'mcberinguy',\n",
       " 'mcberinguy',\n",
       " 'mcberinguy',\n",
       " 'mcberinguy',\n",
       " 'mcberinguy',\n",
       " 'mcberinguy',\n",
       " 'mcberinguy',\n",
       " 'mcberinguy',\n",
       " 'mcberinguy',\n",
       " 'mcberinguy',\n",
       " 'mcberinguy',\n",
       " 'mcberinguy',\n",
       " 'mcberinguy',\n",
       " 'mcberinguy',\n",
       " 'mcberinguy',\n",
       " 'mcberinguy',\n",
       " 'mcberinguy',\n",
       " 'mcberinguy',\n",
       " 'mcberinguy',\n",
       " 'mcberinguy',\n",
       " 'mcberinguy',\n",
       " 'mcberinguy',\n",
       " 'mcberinguy',\n",
       " 'MandinhaSimone',\n",
       " 'MandinhaSimone',\n",
       " 'MandinhaSimone',\n",
       " 'MandinhaSimone',\n",
       " 'MandinhaSimone',\n",
       " 'MandinhaSimone',\n",
       " 'MandinhaSimone',\n",
       " 'MandinhaSimone',\n",
       " 'MandinhaSimone',\n",
       " 'MandinhaSimone',\n",
       " 'MandinhaSimone',\n",
       " 'MandinhaSimone',\n",
       " 'MandinhaSimone',\n",
       " 'MandinhaSimone',\n",
       " 'MandinhaSimone',\n",
       " 'MandinhaSimone',\n",
       " 'MandinhaSimone',\n",
       " 'MandinhaSimone',\n",
       " 'MandinhaSimone',\n",
       " 'MandinhaSimone',\n",
       " 'MandinhaSimone',\n",
       " 'MandinhaSimone',\n",
       " 'MandinhaSimone',\n",
       " 'MandinhaSimone',\n",
       " 'MandinhaSimone',\n",
       " 'MandinhaSimone',\n",
       " 'MandinhaSimone',\n",
       " 'MandinhaSimone',\n",
       " 'MandinhaSimone',\n",
       " 'MandinhaSimone',\n",
       " 'MandinhaSimone',\n",
       " 'MandinhaSimone',\n",
       " 'MandinhaSimone',\n",
       " 'MandinhaSimone',\n",
       " 'MandinhaSimone',\n",
       " 'MandinhaSimone',\n",
       " 'MandinhaSimone',\n",
       " 'MandinhaSimone',\n",
       " 'MandinhaSimone',\n",
       " 'MandinhaSimone',\n",
       " 'MandinhaSimone',\n",
       " 'MandinhaSimone',\n",
       " 'MandinhaSimone',\n",
       " 'MandinhaSimone',\n",
       " 'MandinhaSimone',\n",
       " 'MandinhaSimone',\n",
       " 'MandinhaSimone',\n",
       " 'MandinhaSimone',\n",
       " 'MandinhaSimone',\n",
       " 'MandinhaSimone',\n",
       " 'MandinhaSimone',\n",
       " 'MandinhaSimone',\n",
       " 'MandinhaSimone',\n",
       " 'MandinhaSimone',\n",
       " 'MandinhaSimone',\n",
       " 'MandinhaSimone',\n",
       " 'MandinhaSimone',\n",
       " 'MandinhaSimone',\n",
       " 'MandinhaSimone',\n",
       " 'MandinhaSimone',\n",
       " 'MandinhaSimone',\n",
       " 'MandinhaSimone',\n",
       " 'MandinhaSimone',\n",
       " 'MandinhaSimone',\n",
       " 'MandinhaSimone',\n",
       " 'MandinhaSimone',\n",
       " 'MandinhaSimone',\n",
       " 'MandinhaSimone',\n",
       " 'MandinhaSimone',\n",
       " 'MandinhaSimone',\n",
       " 'MandinhaSimone',\n",
       " 'Verazattar',\n",
       " 'Verazattar',\n",
       " 'Verazattar',\n",
       " 'Sandoval73',\n",
       " 'erasmo7silva',\n",
       " 'erasmo7silva',\n",
       " 'erasmo7silva',\n",
       " 'erasmo7silva',\n",
       " 'erasmo7silva',\n",
       " 'erasmo7silva',\n",
       " 'erasmo7silva',\n",
       " 'erasmo7silva',\n",
       " 'erasmo7silva',\n",
       " 'erasmo7silva',\n",
       " 'erasmo7silva',\n",
       " 'erasmo7silva',\n",
       " 'erasmo7silva',\n",
       " 'erasmo7silva',\n",
       " 'erasmo7silva',\n",
       " 'erasmo7silva',\n",
       " 'erasmo7silva',\n",
       " 'erasmo7silva',\n",
       " 'erasmo7silva',\n",
       " 'erasmo7silva',\n",
       " 'erasmo7silva',\n",
       " 'erasmo7silva',\n",
       " 'erasmo7silva',\n",
       " 'erasmo7silva',\n",
       " 'erasmo7silva',\n",
       " 'erasmo7silva',\n",
       " 'erasmo7silva',\n",
       " 'erasmo7silva',\n",
       " 'erasmo7silva',\n",
       " 'erasmo7silva',\n",
       " 'erasmo7silva',\n",
       " 'erasmo7silva',\n",
       " 'erasmo7silva',\n",
       " 'erasmo7silva',\n",
       " 'erasmo7silva',\n",
       " 'erasmo7silva',\n",
       " 'erasmo7silva',\n",
       " 'erasmo7silva',\n",
       " 'erasmo7silva',\n",
       " 'erasmo7silva',\n",
       " 'erasmo7silva',\n",
       " 'erasmo7silva',\n",
       " 'erasmo7silva',\n",
       " 'erasmo7silva',\n",
       " 'erasmo7silva',\n",
       " 'erasmo7silva',\n",
       " 'erasmo7silva',\n",
       " 'erasmo7silva',\n",
       " 'erasmo7silva',\n",
       " 'erasmo7silva',\n",
       " 'erasmo7silva',\n",
       " 'erasmo7silva',\n",
       " 'erasmo7silva',\n",
       " 'erasmo7silva',\n",
       " 'erasmo7silva',\n",
       " 'erasmo7silva',\n",
       " 'erasmo7silva',\n",
       " 'SANDRO_A',\n",
       " 'SANDRO_A',\n",
       " 'SANDRO_A',\n",
       " 'SANDRO_A',\n",
       " 'SANDRO_A',\n",
       " 'SANDRO_A',\n",
       " 'SANDRO_A',\n",
       " 'SANDRO_A',\n",
       " 'SANDRO_A',\n",
       " 'SANDRO_A',\n",
       " 'SANDRO_A',\n",
       " 'SANDRO_A',\n",
       " 'SANDRO_A',\n",
       " 'SANDRO_A',\n",
       " 'SANDRO_A',\n",
       " 'SANDRO_A',\n",
       " 'SANDRO_A',\n",
       " 'SANDRO_A',\n",
       " 'SANDRO_A',\n",
       " 'SANDRO_A',\n",
       " 'SANDRO_A',\n",
       " 'SANDRO_A',\n",
       " 'SANDRO_A',\n",
       " 'SANDRO_A',\n",
       " 'SANDRO_A',\n",
       " 'SANDRO_A',\n",
       " 'SANDRO_A',\n",
       " 'SANDRO_A',\n",
       " 'SANDRO_A',\n",
       " 'SANDRO_A',\n",
       " 'SANDRO_A',\n",
       " 'SANDRO_A',\n",
       " 'SANDRO_A',\n",
       " 'SANDRO_A',\n",
       " 'SANDRO_A',\n",
       " 'SANDRO_A',\n",
       " 'SANDRO_A',\n",
       " 'rafavnt',\n",
       " 'rafavnt',\n",
       " 'rafavnt',\n",
       " 'rafavnt',\n",
       " 'rafavnt',\n",
       " 'rafavnt',\n",
       " 'rafavnt',\n",
       " 'rafavnt',\n",
       " 'rafavnt',\n",
       " 'rafavnt',\n",
       " 'rafavnt',\n",
       " 'rafavnt',\n",
       " 'rafavnt',\n",
       " 'rafavnt',\n",
       " 'rafavnt',\n",
       " 'rafavnt',\n",
       " 'rafavnt',\n",
       " 'rafavnt',\n",
       " 'rafavnt',\n",
       " 'rafavnt',\n",
       " 'rafavnt',\n",
       " 'rafavnt',\n",
       " 'rafavnt',\n",
       " 'rafavnt',\n",
       " 'rafavnt',\n",
       " 'rafavnt',\n",
       " 'rafavnt',\n",
       " 'rafavnt',\n",
       " 'rafavnt',\n",
       " 'rafavnt',\n",
       " 'rafavnt',\n",
       " 'rafavnt',\n",
       " 'rafavnt',\n",
       " 'rafavnt',\n",
       " 'rafavnt',\n",
       " 'rafavnt',\n",
       " 'rafavnt',\n",
       " 'rafavnt',\n",
       " 'rafavnt',\n",
       " 'rafavnt',\n",
       " 'rafavnt',\n",
       " 'rafavnt',\n",
       " 'rafavnt',\n",
       " 'rafavnt',\n",
       " 'rafavnt',\n",
       " 'rafavnt',\n",
       " 'rafavnt',\n",
       " 'rafavnt',\n",
       " 'rafavnt',\n",
       " 'rafavnt',\n",
       " 'rafavnt',\n",
       " 'rafavnt',\n",
       " 'rafavnt',\n",
       " 'rafavnt',\n",
       " 'rafavnt',\n",
       " 'rafavnt',\n",
       " 'rafavnt',\n",
       " 'rafavnt',\n",
       " 'rafavnt',\n",
       " 'rafavnt',\n",
       " 'rafavnt',\n",
       " 'rafavnt',\n",
       " 'rafavnt',\n",
       " 'rafavnt',\n",
       " 'rafavnt',\n",
       " 'rafavnt',\n",
       " 'rafavnt',\n",
       " 'rafavnt',\n",
       " 'rafavnt',\n",
       " 'rafavnt',\n",
       " 'rafavnt',\n",
       " 'rafavnt',\n",
       " 'rafavnt',\n",
       " 'rafavnt',\n",
       " 'rafavnt',\n",
       " 'rafavnt',\n",
       " 'rafavnt',\n",
       " 'rafavnt',\n",
       " 'rafavnt',\n",
       " 'rafavnt',\n",
       " 'rafavnt',\n",
       " 'rafavnt',\n",
       " 'rafavnt',\n",
       " 'rafavnt',\n",
       " 'rafavnt',\n",
       " 'rafavnt',\n",
       " 'rafavnt',\n",
       " 'rafavnt',\n",
       " 'rafavnt',\n",
       " 'rafavnt',\n",
       " 'rafavnt',\n",
       " 'rafavnt',\n",
       " 'rafavnt',\n",
       " 'rafavnt',\n",
       " 'rafavnt',\n",
       " 'rafavnt',\n",
       " 'rafavnt',\n",
       " 'rafavnt',\n",
       " 'samanthaeandre',\n",
       " 'samanthaeandre',\n",
       " 'samanthaeandre',\n",
       " 'samanthaeandre',\n",
       " 'samanthaeandre',\n",
       " 'samanthaeandre',\n",
       " 'samanthaeandre',\n",
       " 'samanthaeandre',\n",
       " 'samanthaeandre',\n",
       " 'samanthaeandre',\n",
       " 'samanthaeandre',\n",
       " 'samanthaeandre',\n",
       " 'samanthaeandre',\n",
       " 'samanthaeandre',\n",
       " 'samanthaeandre',\n",
       " 'samanthaeandre',\n",
       " 'samanthaeandre',\n",
       " 'samanthaeandre',\n",
       " 'samanthaeandre',\n",
       " 'samanthaeandre',\n",
       " 'samanthaeandre',\n",
       " 'samanthaeandre',\n",
       " 'samanthaeandre',\n",
       " 'samanthaeandre',\n",
       " 'samanthaeandre',\n",
       " 'samanthaeandre',\n",
       " 'samanthaeandre',\n",
       " 'samanthaeandre',\n",
       " 'samanthaeandre',\n",
       " 'samanthaeandre',\n",
       " 'samanthaeandre',\n",
       " 'samanthaeandre',\n",
       " 'samanthaeandre',\n",
       " 'samanthaeandre',\n",
       " 'samanthaeandre',\n",
       " 'samanthaeandre',\n",
       " 'samanthaeandre',\n",
       " 'samanthaeandre',\n",
       " 'samanthaeandre',\n",
       " 'samanthaeandre',\n",
       " 'samanthaeandre',\n",
       " 'samanthaeandre',\n",
       " 'samanthaeandre',\n",
       " 'samanthaeandre',\n",
       " 'samanthaeandre',\n",
       " 'samanthaeandre',\n",
       " 'samanthaeandre',\n",
       " 'samanthaeandre',\n",
       " 'samanthaeandre',\n",
       " 'samanthaeandre',\n",
       " 'samanthaeandre',\n",
       " 'samanthaeandre',\n",
       " 'samanthaeandre',\n",
       " 'samanthaeandre',\n",
       " 'samanthaeandre',\n",
       " 'samanthaeandre',\n",
       " 'samanthaeandre',\n",
       " 'samanthaeandre',\n",
       " 'samanthaeandre',\n",
       " 'samanthaeandre',\n",
       " 'samanthaeandre',\n",
       " 'samanthaeandre',\n",
       " 'samanthaeandre',\n",
       " 'samanthaeandre',\n",
       " 'samanthaeandre',\n",
       " 'samanthaeandre',\n",
       " 'samanthaeandre',\n",
       " 'samanthaeandre',\n",
       " 'samanthaeandre',\n",
       " 'samanthaeandre',\n",
       " 'samanthaeandre',\n",
       " 'samanthaeandre',\n",
       " 'samanthaeandre',\n",
       " 'samanthaeandre',\n",
       " 'samanthaeandre',\n",
       " 'samanthaeandre',\n",
       " 'samanthaeandre',\n",
       " 'barroschitao',\n",
       " 'barroschitao',\n",
       " 'barroschitao',\n",
       " 'barroschitao',\n",
       " 'barroschitao',\n",
       " 'barroschitao',\n",
       " 'barroschitao',\n",
       " 'barroschitao',\n",
       " 'barroschitao',\n",
       " 'barroschitao',\n",
       " 'barroschitao',\n",
       " 'barroschitao',\n",
       " 'barroschitao',\n",
       " 'barroschitao',\n",
       " 'barroschitao',\n",
       " 'barroschitao',\n",
       " 'barroschitao',\n",
       " 'barroschitao',\n",
       " 'bangsppk',\n",
       " 'bangsppk',\n",
       " 'bangsppk',\n",
       " 'bangsppk',\n",
       " 'bangsppk',\n",
       " 'bangsppk',\n",
       " 'bangsppk',\n",
       " 'bangsppk',\n",
       " 'bangsppk',\n",
       " 'bangsppk',\n",
       " 'bangsppk',\n",
       " 'bangsppk',\n",
       " 'bangsppk',\n",
       " 'bangsppk',\n",
       " 'bangsppk',\n",
       " 'bangsppk',\n",
       " 'bangsppk',\n",
       " 'bangsppk',\n",
       " 'bangsppk',\n",
       " 'bangsppk',\n",
       " 'bangsppk',\n",
       " 'bangsppk',\n",
       " 'bangsppk',\n",
       " 'bangsppk',\n",
       " 'bangsppk',\n",
       " 'ggauchinho',\n",
       " 'ggauchinho',\n",
       " 'ggauchinho',\n",
       " 'ggauchinho',\n",
       " 'ggauchinho',\n",
       " 'ggauchinho',\n",
       " 'ggauchinho',\n",
       " 'ggauchinho',\n",
       " 'ggauchinho',\n",
       " 'ggauchinho',\n",
       " 'ggauchinho',\n",
       " 'ggauchinho',\n",
       " 'ggauchinho',\n",
       " 'ggauchinho',\n",
       " 'ggauchinho',\n",
       " 'ggauchinho',\n",
       " 'ggauchinho',\n",
       " 'ggauchinho',\n",
       " 'ggauchinho',\n",
       " 'ggauchinho',\n",
       " 'ggauchinho',\n",
       " 'ggauchinho',\n",
       " 'ggauchinho',\n",
       " 'ggauchinho',\n",
       " 'ggauchinho',\n",
       " 'ggauchinho',\n",
       " 'ggauchinho',\n",
       " 'ggauchinho',\n",
       " 'ggauchinho',\n",
       " 'ggauchinho',\n",
       " 'ggauchinho',\n",
       " 'ggauchinho',\n",
       " 'ggauchinho',\n",
       " 'ggauchinho',\n",
       " 'ggauchinho',\n",
       " 'ggauchinho',\n",
       " 'ggauchinho',\n",
       " 'ggauchinho',\n",
       " 'ggauchinho',\n",
       " 'ggauchinho',\n",
       " 'ggauchinho',\n",
       " 'ggauchinho',\n",
       " 'ggauchinho',\n",
       " 'ggauchinho',\n",
       " 'ggauchinho',\n",
       " 'ggauchinho',\n",
       " 'ggauchinho',\n",
       " 'ggauchinho',\n",
       " 'ggauchinho',\n",
       " 'ggauchinho',\n",
       " 'ggauchinho',\n",
       " 'ggauchinho',\n",
       " 'ggauchinho',\n",
       " 'ggauchinho',\n",
       " 'GusttavoGrossi',\n",
       " 'mariavaltenice',\n",
       " 'mariavaltenice',\n",
       " 'mariavaltenice',\n",
       " 'FredSteca',\n",
       " 'FredSteca',\n",
       " 'FredSteca',\n",
       " 'FredSteca',\n",
       " 'FredSteca',\n",
       " 'casa_dos_pobres',\n",
       " 'casa_dos_pobres',\n",
       " 'casa_dos_pobres',\n",
       " 'casa_dos_pobres',\n",
       " 'bemdeboavsf',\n",
       " 'bemdeboavsf',\n",
       " 'bemdeboavsf',\n",
       " 'bemdeboavsf',\n",
       " 'bemdeboavsf',\n",
       " 'bemdeboavsf',\n",
       " 'bemdeboavsf',\n",
       " 'bemdeboavsf',\n",
       " 'bemdeboavsf',\n",
       " 'bemdeboavsf',\n",
       " 'bemdeboavsf',\n",
       " 'bemdeboavsf',\n",
       " 'bemdeboavsf',\n",
       " 'bemdeboavsf',\n",
       " 'bemdeboavsf',\n",
       " 'bemdeboavsf',\n",
       " 'bemdeboavsf',\n",
       " 'bemdeboavsf',\n",
       " 'bemdeboavsf',\n",
       " 'bemdeboavsf',\n",
       " 'bemdeboavsf',\n",
       " 'bemdeboavsf',\n",
       " 'bemdeboavsf',\n",
       " 'bemdeboavsf',\n",
       " 'bemdeboavsf',\n",
       " 'bemdeboavsf',\n",
       " 'bemdeboavsf',\n",
       " 'bemdeboavsf',\n",
       " 'bemdeboavsf',\n",
       " 'bemdeboavsf',\n",
       " 'bemdeboavsf',\n",
       " 'bemdeboavsf',\n",
       " 'bemdeboavsf',\n",
       " 'bemdeboavsf',\n",
       " 'bemdeboavsf',\n",
       " 'bemdeboavsf',\n",
       " 'bemdeboavsf',\n",
       " 'bemdeboavsf',\n",
       " 'bemdeboavsf',\n",
       " 'bemdeboavsf',\n",
       " 'bemdeboavsf',\n",
       " 'bemdeboavsf',\n",
       " 'bemdeboavsf',\n",
       " 'bemdeboavsf',\n",
       " 'bemdeboavsf',\n",
       " 'bemdeboavsf',\n",
       " 'bemdeboavsf',\n",
       " 'bemdeboavsf',\n",
       " 'bemdeboavsf',\n",
       " 'bemdeboavsf',\n",
       " 'bemdeboavsf',\n",
       " 'bemdeboavsf',\n",
       " 'bemdeboavsf',\n",
       " 'bemdeboavsf',\n",
       " 'bemdeboavsf',\n",
       " 'bemdeboavsf',\n",
       " 'bemdeboavsf',\n",
       " 'bemdeboavsf',\n",
       " 'bemdeboavsf',\n",
       " 'bemdeboavsf',\n",
       " 'bemdeboavsf',\n",
       " 'bemdeboavsf',\n",
       " 'bemdeboavsf',\n",
       " 'bemdeboavsf',\n",
       " 'bemdeboavsf',\n",
       " 'bemdeboavsf',\n",
       " 'bemdeboavsf',\n",
       " 'bemdeboavsf',\n",
       " 'bemdeboavsf',\n",
       " 'bemdeboavsf',\n",
       " 'bemdeboavsf',\n",
       " 'bemdeboavsf',\n",
       " 'bemdeboavsf',\n",
       " 'bemdeboavsf',\n",
       " 'bemdeboavsf',\n",
       " 'bemdeboavsf',\n",
       " 'bemdeboavsf',\n",
       " 'bemdeboavsf',\n",
       " 'bemdeboavsf',\n",
       " 'bemdeboavsf',\n",
       " 'bemdeboavsf',\n",
       " 'bemdeboavsf',\n",
       " 'bemdeboavsf',\n",
       " 'bemdeboavsf',\n",
       " 'bemdeboavsf',\n",
       " 'bemdeboavsf',\n",
       " 'bemdeboavsf',\n",
       " 'bemdeboavsf',\n",
       " 'bemdeboavsf',\n",
       " 'bemdeboavsf',\n",
       " 'bemdeboavsf',\n",
       " 'bemdeboavsf',\n",
       " 'bemdeboavsf',\n",
       " 'bemdeboavsf',\n",
       " 'bemdeboavsf',\n",
       " 'bemdeboavsf',\n",
       " 'bemdeboavsf',\n",
       " 'bemdeboavsf',\n",
       " 'bemdeboavsf',\n",
       " 'bemdeboavsf',\n",
       " 'bemdeboavsf',\n",
       " 'bemdeboavsf',\n",
       " 'bemdeboavsf',\n",
       " 'bemdeboavsf',\n",
       " 'bemdeboavsf',\n",
       " 'bemdeboavsf',\n",
       " 'bemdeboavsf',\n",
       " 'bemdeboavsf',\n",
       " 'bemdeboavsf',\n",
       " 'bemdeboavsf',\n",
       " 'bemdeboavsf',\n",
       " 'bemdeboavsf',\n",
       " 'bemdeboavsf',\n",
       " 'bemdeboavsf',\n",
       " 'bemdeboavsf',\n",
       " 'bemdeboavsf',\n",
       " 'bemdeboavsf',\n",
       " 'bemdeboavsf',\n",
       " 'bemdeboavsf',\n",
       " 'bemdeboavsf',\n",
       " 'bemdeboavsf',\n",
       " 'bemdeboavsf',\n",
       " 'bemdeboavsf',\n",
       " 'bemdeboavsf',\n",
       " 'bemdeboavsf',\n",
       " 'SONIARMALHEIROS',\n",
       " 'SONIARMALHEIROS',\n",
       " 'SONIARMALHEIROS',\n",
       " 'fernandasdn',\n",
       " 'fernandasdn',\n",
       " 'fernandasdn',\n",
       " 'fernandasdn',\n",
       " 'fernandasdn',\n",
       " 'fernandasdn',\n",
       " 'fernandasdn',\n",
       " 'fernandasdn',\n",
       " 'fernandasdn',\n",
       " 'fernandasdn',\n",
       " 'fernandasdn',\n",
       " 'fernandasdn',\n",
       " 'fernandasdn',\n",
       " 'fernandasdn',\n",
       " 'fernandasdn',\n",
       " 'fernandasdn',\n",
       " 'fernandasdn',\n",
       " 'fernandasdn',\n",
       " 'fernandasdn',\n",
       " 'fernandasdn',\n",
       " 'fernandasdn',\n",
       " 'fernandasdn',\n",
       " 'fernandasdn',\n",
       " 'fernandasdn',\n",
       " 'fernandasdn',\n",
       " 'fernandasdn',\n",
       " 'fernandasdn',\n",
       " 'fernandasdn',\n",
       " 'fernandasdn',\n",
       " 'fernandasdn',\n",
       " 'fernandasdn',\n",
       " 'fernandasdn',\n",
       " 'fernandasdn',\n",
       " 'fernandasdn',\n",
       " 'fernandasdn',\n",
       " 'fernandasdn',\n",
       " 'fernandasdn',\n",
       " 'fernandasdn',\n",
       " 'fernandasdn',\n",
       " 'fernandasdn',\n",
       " 'fernandasdn',\n",
       " 'fernandasdn',\n",
       " 'fernandasdn',\n",
       " 'fernandasdn',\n",
       " 'fernandasdn',\n",
       " 'fernandasdn',\n",
       " 'fernandasdn',\n",
       " 'fernandasdn',\n",
       " 'fernandasdn',\n",
       " 'fernandasdn',\n",
       " 'fernandasdn',\n",
       " 'fernandasdn',\n",
       " 'fernandasdn',\n",
       " 'fernandasdn',\n",
       " 'fernandasdn',\n",
       " 'fernandasdn',\n",
       " 'fernandasdn',\n",
       " 'fernandasdn',\n",
       " 'fernandasdn',\n",
       " 'fernandasdn',\n",
       " 'fernandasdn',\n",
       " 'fernandasdn',\n",
       " 'fernandasdn',\n",
       " 'fernandasdn',\n",
       " 'fernandasdn',\n",
       " 'fernandasdn',\n",
       " 'fernandasdn',\n",
       " 'fernandasdn',\n",
       " 'fernandasdn',\n",
       " 'fernandasdn',\n",
       " 'fernandasdn',\n",
       " 'fernandasdn',\n",
       " 'fernandasdn',\n",
       " 'fernandasdn',\n",
       " 'fernandasdn',\n",
       " 'fernandasdn',\n",
       " 'fernandasdn',\n",
       " 'fernandasdn',\n",
       " 'fernandasdn',\n",
       " 'fernandasdn',\n",
       " 'fernandasdn',\n",
       " 'fernandasdn',\n",
       " 'fernandasdn',\n",
       " 'fernandasdn',\n",
       " 'fernandasdn',\n",
       " 'fernandasdn',\n",
       " 'fernandasdn',\n",
       " 'fernandasdn',\n",
       " 'fernandasdn',\n",
       " 'fernandasdn',\n",
       " 'fernandasdn',\n",
       " 'fernandasdn',\n",
       " 'fernandasdn',\n",
       " 'fernandasdn',\n",
       " 'fernandasdn',\n",
       " 'fernandasdn',\n",
       " 'fernandasdn',\n",
       " 'fernandasdn',\n",
       " 'fernandasdn',\n",
       " 'fernandasdn',\n",
       " 'fernandasdn',\n",
       " ...]"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "allusers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def iterate_Testing_instances(path,files_test):\n",
    "    \"\"\"\n",
    "    Return an iterator over tuples containing:\n",
    "    (concatenated tweet text, label, username)\n",
    "    For each user in path, we find the first tweet containing one of the specified keywords.\n",
    "    We then create one positive instance, containing all tweets prior to the matched tweet.\n",
    "    We also create one negative instance, which is the same as the positive instance, except\n",
    "    the N most recent tweets are removed (where N is set by the negative_window parameter).\n",
    "    We additionally filter users if they use one of the keywords in one of their first `negative_window`\n",
    "    tweets. This is to we have enough tweets to make a negative example.\n",
    "    \"\"\" \n",
    "    for _, _, arquivos in os.walk(path): arquivos\n",
    "        \n",
    "    for fname in files_test:\n",
    "        \n",
    "        user = filename2user(arquivos[fname])\n",
    "        lines = []\n",
    "        var=None\n",
    "        count=0\n",
    "        \n",
    "        for i, line in enumerate(open(path+arquivos[fname])):\n",
    "            js = json.loads(line)\n",
    "            \n",
    "            if matches_keywords(js['text'], keywords):\n",
    "                #print('test',user)\n",
    "                v=''\n",
    "                count=0\n",
    "                #print('len of lines', len(lines))\n",
    "                for k,l in enumerate(lines[:][::-1]):\n",
    "                    #print(l)\n",
    "                    v=v+l\n",
    "                    count=count+1\n",
    "                    if count==20:\n",
    "                        #print('count',count)\n",
    "                        count=0\n",
    "                        yield(v,user)\n",
    "                        v=None\n",
    "                        v=\"\"\n",
    "                    #PS.: if first tweet of user is with keyword this user is not counted!\n",
    "                    elif(count<20 and k == (len(lines)-1)):  \n",
    "                        count=0\n",
    "                        yield(v,user)\n",
    "                        v=None\n",
    "                        v=\"\"\n",
    "                break\n",
    "            lines.append(js['text'])\n",
    "            "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
