{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re \n",
    "import string\n",
    "import timestring\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cross_validation import cross_val_score, KFold\n",
    "import glob, os\n",
    "import numpy as np\n",
    "from sklearn import svm\n",
    "from collections import defaultdict\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "from sklearn.cross_validation import KFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "from scipy.sparse import hstack\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DIR = '/data/2/protest'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Tokenizer:\n",
    "    def __init__(self, collapse_mentions=False, collapse_digits=True):\n",
    "        self.collapse_mentions = collapse_mentions\n",
    "        self.collapse_digits = collapse_digits\n",
    "        #self.stopwords = stopwords\n",
    "        \n",
    "    def tokenize(self, text):\n",
    "        punc_re = '[' + '\\\\!\\\\\"\\\\#\\\\$\\\\%\\\\&\\\\\\'\\\\(\\\\)\\\\*\\\\+\\\\,\\\\-\\\\.\\\\/\\\\:\\\\;\\\\<\\\\=\\\\>\\\\?\\\\@\\\\[\\\\\\\\\\\\]\\\\_\\\\{\\\\|\\\\}' + ']'\n",
    "        text = text.lower()\n",
    "        text = re.sub('#(\\S+)', r'HASHTAG_\\1', text)\n",
    "        if self.collapse_mentions:\n",
    "            text = re.sub('@\\S+', 'MENTION', text)\n",
    "        else:\n",
    "            text = re.sub('@\\S+', 'MENTION_\\1', text)\n",
    "        text = re.sub('http\\S+', 'THIS_IS_A_URL', text)\n",
    "        text = re.sub(r'(.)\\1\\1\\1+', r'\\1', text)\n",
    "        if self.collapse_digits: # Numbers help!\n",
    "            text = re.sub(r'[0-9]', '9', text) \n",
    "        toks = []\n",
    "        for tok in text.split():\n",
    "            tok = re.sub(r'^(' + punc_re + '+)', r'\\1 ', tok)\n",
    "            tok = re.sub(r'(' + punc_re + '+)$', r' \\1', tok)\n",
    "            for subtok in tok.split():\n",
    "                if re.search('\\w', subtok):\n",
    "                    toks.append(subtok)\n",
    "        return toks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def print_top_terms(clf, vec, n=100):\n",
    "    coefs = sorted(zip(vec.get_feature_names(),clf.coef_[0]),key=lambda x:x[1])\n",
    "    print(coefs[0:30], '\\n')\n",
    "    print(coefs[-30:])\n",
    "    #feats = np.array(vec.get_feature_names())\n",
    "    #print('\\n'.join(feats[np.argsort(clf.coef_[0])[::-1][:n]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_stopwords(path):\n",
    "    return [s.strip().lower() for s in open(path)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#parse date \n",
    "def parse_date(datestring):\n",
    "    from datetime import datetime\n",
    "\n",
    "    \"\"\" Input, e.g., Mon Aug 24 19:41:14 +0000 2015\n",
    "    Output, e.g., 24 \"\"\"\n",
    "    #Sat May 16 16:30:12 +0000 2015\n",
    "    #print datestring\n",
    "    parts = datestring.split()\n",
    "    var = datetime.strptime(parts[1]+' ' +parts[2]+' '+ parts[5],'%b %d %Y')\n",
    "    return var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def filename2user(fname):\n",
    "    \"\"\"Convert filename like this\n",
    "      /data/2/protest/Timeline/MandinhaSimone.txt.txt\n",
    "    into a username like\n",
    "      MandinhaSimone\n",
    "    \"\"\"\n",
    "    return re.sub(r'^([^\\.]+)\\..+', r'\\1', os.path.basename(fname))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def matches_keywords(text, keywords):\n",
    "    \"\"\" Return true if any keyword is a substring of this text, ignoring case. \"\"\"\n",
    "    text = text.lower()\n",
    "    for kw in keywords:\n",
    "        if kw in text:\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def iterate_instances_changed(files, keywords, window_sz, gap_sz,files_train):\n",
    "    \"\"\"\n",
    "    Return an iterator over tuples containing:\n",
    "    (concatenated tweet text, label, username, percentage of neightbors who used hashtag)\n",
    "    For each user in path, we find the first tweet containing one of the specified keywords.\n",
    "    We then create one positive instance, containing all tweets prior to the matched tweet.\n",
    "    We also create one negative instance, which is the same as the positive instance, except\n",
    "    the N most recent tweets are removed (where N is set by the negative_window parameter).\n",
    "    We additionally filter users if they use one of the keywords in one of their first `negative_window`\n",
    "    tweets. This is to we have enough tweets to make a negative example.\n",
    "    Also we find neighbors (define as those with symetric mentions) who have also used one of the keywords \n",
    "    during positive or negative window \n",
    "    \"\"\"\n",
    "    DIR = '/home/elaine/Protest/protest/Brazil project/'\n",
    "    DIR2='/data/2/protest/mentions/'\n",
    "    dic = defaultdict(tuple)\n",
    "    pkl_file = open(DIR+'all_mentions_graph.pkl', 'rb')#open pickle file where the edges of the graph is saved\n",
    "    data1 = pickle.load(pkl_file)\n",
    "\n",
    "    for _, _, f in os.walk(DIR2): f\n",
    "        \n",
    "    for _, _, arquivos in os.walk(files): arquivos\n",
    "        \n",
    "    for fname in files_train:#glob.glob(path + '/*.txt'):\n",
    "        \n",
    "        user = filename2user(arquivos[fname])\n",
    "        #print('this is', user)\n",
    "        lines = []\n",
    "        countneg=0\n",
    "        countpos=0\n",
    "        #print('treino',user)\n",
    "        for i, line in enumerate(open(files+arquivos[fname])):\n",
    "            js = json.loads(line)\n",
    "            # exclude people who use keyword within first `window` of tweets.\n",
    "            if i <= (2*window_sz + gap_sz) and matches_keywords(js['text'], keywords):\n",
    "                #print('skipping', arquivos[fname], 'because uses keyword in first', negative_window, 'tweets')\n",
    "                break\n",
    "            if i > (2*window_sz + gap_sz) and matches_keywords(js['text'], keywords):\n",
    "                #yield (' '.join(lines), 1, user)\n",
    "                # yield (' '.join(lines[:-negative_window]), 0, user)\n",
    "                pos_start = max(0, i - window_sz)\n",
    "                #print('positive window starts in',pos_start)\n",
    "                pos_end = i+1\n",
    "                #print('positive window finishes in',pos_end)\n",
    "\n",
    "                #print('lineslen: ',len(lines))\n",
    "                date1_pos = lines[pos_start][1]\n",
    "                #print('ksdjsk',lines[pos_start][1])\n",
    "                #print(pos_end)\n",
    "                date2_pos = lines[pos_end-2][1]\n",
    "\n",
    "                neg_start = max(0, i - gap_sz - (2 * window_sz))\n",
    "                #print('negative window starts in',neg_start)\n",
    "                neg_end = i - gap_sz - window_sz\n",
    "                #print('nagative window finishes in',neg_end)\n",
    "                date1_neg = lines[neg_start][1]\n",
    "                date2_neg = lines[neg_end][1]\n",
    "\n",
    "                if data1.has_node(user) == True: #if that file is in the graph\n",
    "                    #print(user, 'is in the graph!')\n",
    "                    neighbors = data1.neighbors(user) #get all neighbors of the user\n",
    "                    #print(user, 's neighbors are: ', neighbors)\n",
    "                    for n in neighbors:\n",
    "                        if n +'.txt' in f:#if that file is in the directory\n",
    "                            #print('vizinho: ', n)\n",
    "                            neighbor = open(DIR2 + n +'.txt','r')\n",
    "                            nlines = neighbor.readlines()\n",
    "                            for t in reversed(nlines):\n",
    "                                tweet_neighbor = json.loads(t) \n",
    "                                #print(tweet_neighbor['created_at'])\n",
    "                                #print(parse_date(tweet_neighbor['created_at']) ,'>', parse_date(date1_pos) )\n",
    "                                if matches_keywords(tweet_neighbor['text'],keywords):\n",
    "                                    if parse_date(tweet_neighbor['created_at']) > parse_date(date1_pos) and parse_date(tweet_neighbor['created_at']) < parse_date(date2_pos):\n",
    "                                        #print(n, 'posted keyword in', parse_date(tweet_neighbor['created_at']))\n",
    "                                        #print('entrou!!')\n",
    "                                        countpos = countpos+1\n",
    "                                        break\n",
    "                            for t in reversed(nlines):\n",
    "                                tweet_neighbor = json.loads(t) \n",
    "                                if matches_keywords(tweet_neighbor['text'],keywords):\n",
    "                                    if parse_date(tweet_neighbor['created_at']) < parse_date(date2_neg) and parse_date(tweet_neighbor['created_at']) > parse_date(date2_neg):\n",
    "                                        #print(n, 'posted negkeyword in', parse_date(tweet_neighbor['created_at']))\n",
    "                                        #print('1entrou!!')\n",
    "                                        countneg = countneg+1\n",
    "                                        break\n",
    "                            dic[user]=((float(countpos)/float(len(neighbors))),float(countneg)/float(len(neighbors)))\n",
    "                #print('diccc',dic)\n",
    "                # print('ps=%d pe=%d ns=%d ne=%d' % (pos_start, pos_end, neg_start, neg_end))\n",
    "                #print('testeee',lines[0][pos_start:pos_end])\n",
    "                testepos = \" \".join(l[0] for l in lines[pos_start:pos_end])\n",
    "                testeneg = \" \".join(l[0] for l in lines[neg_start:neg_end])\n",
    "                \n",
    "                yield (testepos, 1, user, float(countpos)/float(len(neighbors)))\n",
    "                yield (testeneg, 0, user,float(countneg)/float(len(neighbors)))\n",
    "                countneg=0\n",
    "                countpos=0\n",
    "                break\n",
    "            lines.append((js['text'],js['created_at']))\n",
    "    #print(dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import text\n",
    "\n",
    "toker = Tokenizer(collapse_digits=False, collapse_mentions=True)\n",
    "\n",
    "stopwords = set(read_stopwords(DIR + '/stopwords.txt'))    \n",
    "my_stop_words = text.ENGLISH_STOP_WORDS.union(stopwords)\n",
    "\n",
    "vectorizer = TfidfVectorizer(binary=False, decode_error='ignore',ngram_range=(1,2),\n",
    "                                 max_df=1.0, min_df=2, use_idf=True,\n",
    "                                 tokenizer=toker.tokenize,\n",
    "                                 norm='l2', stop_words=set(my_stop_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['foradilma', 'fora dilma', 'forapt', 'fora pt', 'vemprarua', 'vem pra rua']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def read_keywords(path):\n",
    "    return [s.strip().lower() for s in open(path)]\n",
    "    \n",
    "keywords = read_keywords(DIR + '/keywords.txt')\n",
    "keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def plot_probs(dictv,name):\n",
    "    %pylab inline\n",
    "    fig = plt.figure(figsize=(40,10))\n",
    " \n",
    "    for k, v in dictv.items():\n",
    "        i=1\n",
    "        y=[]\n",
    "        x=[]\n",
    "        t = 1/len(dictv[k])\n",
    "        for g in dictv[k]:\n",
    "            y.append(g)\n",
    "            x.append(i*t)\n",
    "            i = i+1\n",
    "        plt.xlabel('% of total tweets')\n",
    "        plt.ylabel('Probability')\n",
    "        plt.plot(x,y)\n",
    "    plt.savefig(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"files_test=[]\\nfor _, _, arquivos in os.walk(DIR + '/Timeline/'): arquivos#list of all files in the directory\\nfor f in arquivos:\\n    if f not in set(files_train):\\n        files_test.append(f)\\n\\nprint(len(arquivos))\\nprint(len(set(files_test)))\\nprint(len(files_train))\\niterator1 = iterate_Testing_instances(DIR + '/Timeline/', keywords, negative_window, files_test)\\nX_ = vectorizer.transform(x[0] for x in iterator1 if not users.append(x[1]))\\nprint('read %d instances into X matrix with shape %s' % (len(users), str(X_.shape)))\\n#print('label distribution=', Counter(y))\\n#y = np.array(y)\\n#users = np.array(users)\\n#print(type(y))\\niterator1 = iterate_Testing_instances(DIR + '/Timeline/', keywords, negative_window,files_test)\\niter1=list(iterator1)\""
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def iterate_Testing_instances(path,files_test):\n",
    "    \"\"\"\n",
    "    Return an iterator over tuples containing:\n",
    "    (concatenated tweet text, label, username)\n",
    "    For each user in path, we find the first tweet containing one of the specified keywords.\n",
    "    We then create one positive instance, containing all tweets prior to the matched tweet.\n",
    "    We also create one negative instance, which is the same as the positive instance, except\n",
    "    the N most recent tweets are removed (where N is set by the negative_window parameter).\n",
    "    We additionally filter users if they use one of the keywords in one of their first `negative_window`\n",
    "    tweets. This is to we have enough tweets to make a negative example.\n",
    "    \"\"\" \n",
    "    for _, _, arquivos in os.walk(path): arquivos\n",
    "        \n",
    "    for fname in files_test:#glob.glob(path + '/*.txt'):\n",
    "        \n",
    "        user = filename2user(arquivos[fname])\n",
    "        \n",
    "        lines = []\n",
    "        var=None\n",
    "        #weetlist=[]\n",
    "        count=0\n",
    "        for i, line in enumerate(open(path+arquivos[fname])):\n",
    "            js = json.loads(line)\n",
    "            #ines.append(js['text'])\n",
    "            # exclude people who use keyword within first `window` of tweets.\n",
    "            \"\"\"if i <= negative_window and matches_keywords(js['text'], keywords):\n",
    "                #print('skipping', fname, 'because uses keyword in first', negative_window, 'tweets')\n",
    "                break\n",
    "            if i > negative_window and matches_keywords(js['text'], keywords):\n",
    "                #print(user)\n",
    "                var=''\n",
    "                for l in lines[-100:][::-1]:\n",
    "                    var = var + l\n",
    "                    yield(var,user)\n",
    "                var=None\n",
    "                break\"\"\"\n",
    "            if matches_keywords(js['text'], keywords):\n",
    "                #print('test',user)\n",
    "                v=''\n",
    "                count=0\n",
    "                #print('len of lines', len(lines))\n",
    "                for k,l in enumerate(lines[:][::-1]):\n",
    "                    #print(l)\n",
    "                    v=v+l\n",
    "                    count=count+1\n",
    "                    if count==20:\n",
    "                        #print('count',count)\n",
    "                        count=0\n",
    "                        yield(v,user)\n",
    "                        v=None\n",
    "                        v=\"\"\n",
    "                    elif(count<20 and k == (len(lines)-1)):  #PS.: if first tweet of user is with keyword this user is not counted!\n",
    "                        #print('count',count)\n",
    "                        count=0\n",
    "                        yield(v,user)\n",
    "                        v=None\n",
    "                        v=\"\"\n",
    "                break\n",
    "            lines.append(js['text'])\n",
    "            \n",
    "#y = []\n",
    "#users = []\n",
    "#negative_window = 10\n",
    "# The loop below iterates over each instance and vectorizes the text.\n",
    "# Simulataneously, we append to the y (labels) and users lists.\n",
    "# We do this to avoid having to store all the text in memory at once and to \n",
    "# only require one loop through the files.\n",
    "\"\"\"files_test=[]\n",
    "for _, _, arquivos in os.walk(DIR + '/Timeline/'): arquivos#list of all files in the directory\n",
    "for f in arquivos:\n",
    "    if f not in set(files_train):\n",
    "        files_test.append(f)\n",
    "\n",
    "print(len(arquivos))\n",
    "print(len(set(files_test)))\n",
    "print(len(files_train))\n",
    "iterator1 = iterate_Testing_instances(DIR + '/Timeline/', keywords, negative_window, files_test)\n",
    "X_ = vectorizer.transform(x[0] for x in iterator1 if not users.append(x[1]))\n",
    "print('read %d instances into X matrix with shape %s' % (len(users), str(X_.shape)))\n",
    "#print('label distribution=', Counter(y))\n",
    "#y = np.array(y)\n",
    "#users = np.array(users)\n",
    "#print(type(y))\n",
    "iterator1 = iterate_Testing_instances(DIR + '/Timeline/', keywords, negative_window,files_test)\n",
    "iter1=list(iterator1)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "288 33\n",
      "read 362 instances into X training matrix with shape (362, 6851)\n",
      "label distribution= Counter({0: 181, 1: 181})\n",
      "read 1068 instances into X_ testing matrix with shape (1068, 6850)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "X has 6850 features per sample; expecting 6851",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-48-56688c594c27>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m     \u001b[0mmodel_mod\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtreino_X\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtreino_y\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 37\u001b[1;33m     \u001b[0mpreds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel_mod\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     38\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/lib64/python3.4/site-packages/sklearn/linear_model/logistic.py\u001b[0m in \u001b[0;36mpredict_proba\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m   1093\u001b[0m             \u001b[0mwhere\u001b[0m \u001b[0mclasses\u001b[0m \u001b[0mare\u001b[0m \u001b[0mordered\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mthey\u001b[0m \u001b[0mare\u001b[0m \u001b[1;32min\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1094\u001b[0m         \"\"\"\n\u001b[1;32m-> 1095\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_predict_proba_lr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1096\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1097\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mpredict_log_proba\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/lib64/python3.4/site-packages/sklearn/linear_model/base.py\u001b[0m in \u001b[0;36m_predict_proba_lr\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    235\u001b[0m         \u001b[0mmulticlass\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mhandled\u001b[0m \u001b[0mby\u001b[0m \u001b[0mnormalizing\u001b[0m \u001b[0mthat\u001b[0m \u001b[0mover\u001b[0m \u001b[0mall\u001b[0m \u001b[0mclasses\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    236\u001b[0m         \"\"\"\n\u001b[1;32m--> 237\u001b[1;33m         \u001b[0mprob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecision_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    238\u001b[0m         \u001b[0mprob\u001b[0m \u001b[1;33m*=\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    239\u001b[0m         \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprob\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprob\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/lib64/python3.4/site-packages/sklearn/linear_model/base.py\u001b[0m in \u001b[0;36mdecision_function\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    202\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mn_features\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    203\u001b[0m             raise ValueError(\"X has %d features per sample; expecting %d\"\n\u001b[1;32m--> 204\u001b[1;33m                              % (X.shape[1], n_features))\n\u001b[0m\u001b[0;32m    205\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    206\u001b[0m         scores = safe_sparse_dot(X, self.coef_.T,\n",
      "\u001b[1;31mValueError\u001b[0m: X has 6850 features per sample; expecting 6851"
     ]
    }
   ],
   "source": [
    "# cross validation process\n",
    "\n",
    "dictv = defaultdict(list)\n",
    "newdictv = defaultdict(list)\n",
    "\n",
    "model_mod = LogisticRegression(penalty='l2', C=1)\n",
    "\n",
    "treino_y=[]\n",
    "plots=[]\n",
    "t=0\n",
    "allusers=[]\n",
    "\n",
    "for _, _, arquivos in os.walk(DIR + '/Timeline/'): arquivos\n",
    "\n",
    "cv = KFold(len(arquivos), 10,shuffle=False)\n",
    "preds=None\n",
    "for train_ind, test_ind in cv: \n",
    "    print(len(train_ind), len(test_ind))\n",
    "    #files, keywords, window_sz, gap_sz,files_train\n",
    "    iterator = iterate_instances_changed(DIR + '/Timeline/', keywords, 20,100, train_ind)\n",
    "    treino_y=[]\n",
    "    users=[]\n",
    "    neighbors=[]\n",
    "    treino_X = vectorizer.fit_transform(x[0] for x in iterator if not users.append(x[2]) and not treino_y.append(x[1]) and not neighbors.append(x[3]))\n",
    "    treino_X = hstack((treino_X, np.array([neighbors]).T)).tocsr()\n",
    "    print('read %d instances into X training matrix with shape %s' % (len(users), str(treino_X.shape)))\n",
    "    print('label distribution=', Counter(treino_y))\n",
    "\n",
    "    iterator1 = iterate_Testing_instances(DIR + '/Timeline/', test_ind)\n",
    "    users1=[]\n",
    "    X_ = vectorizer.transform(x[0] for x in iterator1 if not users1.append(x[1]))\n",
    "    print('read %d instances into X_ testing matrix with shape %s' % (len(users1), str(X_.shape)))\n",
    "\n",
    "    treino_y = np.array(treino_y)\n",
    "    \n",
    "    model_mod.fit(treino_X,treino_y)   \n",
    "    preds = model_mod.predict_proba(X_)\n",
    "    print(len(preds))\n",
    "    \n",
    "    #iterator1 = iterate_Testing_instances(DIR + '/Timeline/', test_ind)\n",
    "    #iter1=list(iterator1)\n",
    "    #print(users1)\n",
    "    for o in zip(users1, preds):\n",
    "        #print(o[1][1])\n",
    "        allusers.append(o[0])\n",
    "        dictv[(o[0])].append(o[1][1])\n",
    "        newdictv[(o[0])].append(o[1][1])\n",
    "\n",
    "    t=t+1\n",
    "    print('users1',len(users1))\n",
    "\n",
    "    print('dictv',len(dictv))\n",
    "    name = ['a','b','c','d','e','f','g','h','i','j']#names for each plot\n",
    "    name_sep = ['aa','bb','cc','dd','ee','ff','gg','hh','ii','jj']\n",
    "    plot_probs(dictv,name[t-1])\n",
    "    plot_probs(newdictv,name_sep[t-1])\n",
    "    print('newdcit', len(newdictv))\n",
    "    newdictv = defaultdict(list)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_files = list(glob.glob(DIR + '/Timeline/*.txt'))\n",
    "window_sz=20  \n",
    "gap_sz=100\n",
    "users = []\n",
    "y=[]\n",
    "\n",
    "iterator = iterate_instances(all_files, keywords, window_sz, gap_sz)\n",
    "X = vectorizer.fit_transform(x[0] for x in iterator if not users.append(x[2]) and not y.append(x[1]))\n",
    "y = np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average 10-fold cross validation accuracy=0.5784 (std=0.05)\n",
      "accuracy on training data=0.9382\n"
     ]
    }
   ],
   "source": [
    "#y = np.array(y)\n",
    "model_mod = LogisticRegression(penalty='l2', C=1)\n",
    "model_mod.fit(X, y)\n",
    "\n",
    "\n",
    "# Compute accuracy\n",
    "def accuracy(truth, predicted):\n",
    "    return (1. * len([1 for tr, pr in zip(truth, predicted) if tr == pr]) / len(truth))\n",
    "\n",
    "# 10 Cross-validation accuracy\n",
    "cv = KFold(len(y), 10, shuffle=False)  # Don't shuffle b/c we don't want a user in both training and testing set.\n",
    "accuracies = []\n",
    "for train_ind, test_ind in cv:\n",
    "    model_mod.fit(X[train_ind],y[train_ind])   \n",
    "    accuracies.append(accuracy_score(y[test_ind], model_mod.predict(X[test_ind])))\n",
    "    \n",
    "print('Average 10-fold cross validation accuracy=%.4f (std=%.2f)' % (np.mean(accuracies), np.std(accuracies)))\n",
    "\n",
    "predicted = model_mod.predict(X)\n",
    "print('accuracy on training data=%.4f' % accuracy(y, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os, io, json, codecs\n",
    "\n",
    "def matches_keywords(text, keywords):\n",
    "    \"\"\" Return true if any keyword is a substring of this text, ignoring case. \"\"\"\n",
    "    text = text.lower()\n",
    "    for kw in keywords:\n",
    "        if kw in text:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def filename2user(fname):\n",
    "    \"\"\"Convert filename like this\n",
    "      /data/2/protest/Timeline/MandinhaSimone.txt.txt\n",
    "    into a username like\n",
    "      MandinhaSimone\n",
    "    \"\"\"\n",
    "    return re.sub(r'^([^\\.]+)\\..+', r'\\1', os.path.basename(fname))\n",
    "\n",
    "def iterate_instances(files, keywords, window_sz, gap_sz,files_train):\n",
    "    \"\"\"\n",
    "    Return an iterator over tuples containing:\n",
    "    (concatenated tweet text, label, username)\n",
    "    For each user in path, we find the first tweet containing one of the specified keywords.\n",
    "    We then create one positive instance, containing all tweets prior to the matched tweet.\n",
    "    We also create one negative instance, which is the same as the positive instance, except\n",
    "    the N most recent tweets are removed (where N is set by the negative_window parameter).\n",
    "    We additionally filter users if they use one of the keywords in one of their first `negative_window`\n",
    "    tweets. This is to we have enough tweets to make a negative example.\n",
    "    \"\"\"\n",
    "    for _, _, arquivos in os.walk(path): arquivos\n",
    "        \n",
    "    for fname in files_train:#glob.glob(path + '/*.txt'):\n",
    "        \n",
    "        user = filename2user(arquivos[fname])\n",
    "        lines = []\n",
    "        #print('treino',user)\n",
    "        for i, line in enumerate(open(path+arquivos[fname])):\n",
    "            js = json.loads(line)\n",
    "            # exclude people who use keyword within first `window` of tweets.\n",
    "            if i <= (2*window_sz + gap_sz) and matches_keywords(js['text'], keywords):\n",
    "                #print('skipping', fname, 'because uses keyword in first', negative_window, 'tweets')\n",
    "                break\n",
    "            if i > (2*window_sz + gap_sz) and matches_keywords(js['text'], keywords):\n",
    "                #yield (' '.join(lines), 1, user)\n",
    "                # yield (' '.join(lines[:-negative_window]), 0, user)\n",
    "                pos_start = max(0, i - window_sz)\n",
    "                pos_end = i+1\n",
    "                neg_start = max(0, i - gap_sz - (2 * window_sz))\n",
    "                neg_end = i - gap_sz - window_sz\n",
    "                # print('ps=%d pe=%d ns=%d ne=%d' % (pos_start, pos_end, neg_start, neg_end))\n",
    "                yield (' '.join(lines[pos_start:pos_end]), 1, user)\n",
    "                yield (' '.join(lines[neg_start:neg_end]), 0, user)\n",
    "                break\n",
    "            lines.append(js['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([ 0.94527363,  0.95431472]), array([ 0.95477387,  0.94472362]), array([ 0.95      ,  0.94949495]), array([199, 199]))\n"
     ]
    }
   ],
   "source": [
    "#precision, recall and F1 measures for both classes\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "y_true = y\n",
    "y_pred = model_mod.predict(X)\n",
    "print(precision_recall_fscore_support(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getRandomFile(path):\n",
    "  \"\"\"\n",
    "  Returns a random filename, chosen among the files of the given path.\n",
    "  \"\"\"\n",
    "  files = os.listdir(path)\n",
    "  index = random.randrange(0, len(files))\n",
    "  return files[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "myn\n",
      "2\n",
      "3\n",
      "4\n",
      "mlp\n",
      "5\n",
      "6\n",
      "7\n",
      "gob\n",
      "8\n",
      "9\n",
      "10\n",
      "less\n"
     ]
    }
   ],
   "source": [
    "ab=['d','v','b','o','g','p','l','m','n','y','m']\n",
    "v=''\n",
    "count=0\n",
    "for k,l in enumerate(ab[::-1]):\n",
    "    #print(l)\n",
    "    v=v+l\n",
    "    count=count+1\n",
    "    if count==3:\n",
    "        count=0\n",
    "        print(v)\n",
    "        v=None\n",
    "        v=\"\"\n",
    "    print(k)\n",
    "    if (count<3 and k==(len(ab)-1)):\n",
    "        print('less')\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['marcocanavarro',\n",
       " 'marcocanavarro',\n",
       " 'marcocanavarro',\n",
       " 'marcocanavarro',\n",
       " 'marcocanavarro',\n",
       " 'marcocanavarro',\n",
       " 'marcocanavarro',\n",
       " 'marcocanavarro',\n",
       " 'AugustoCampos3',\n",
       " 'AugustoCampos3',\n",
       " 'AugustoCampos3',\n",
       " 'AugustoCampos3',\n",
       " 'AugustoCampos3',\n",
       " 'edsondomingues2',\n",
       " 'edsondomingues2',\n",
       " 'edsondomingues2',\n",
       " 'edsondomingues2',\n",
       " 'edsondomingues2',\n",
       " 'edsondomingues2',\n",
       " 'edsondomingues2',\n",
       " 'edsondomingues2',\n",
       " 'edsondomingues2',\n",
       " 'edsondomingues2',\n",
       " 'edsondomingues2',\n",
       " 'edsondomingues2',\n",
       " 'edsondomingues2',\n",
       " 'edsondomingues2',\n",
       " 'edsondomingues2',\n",
       " 'edsondomingues2',\n",
       " 'edsondomingues2',\n",
       " 'edsondomingues2',\n",
       " 'edsondomingues2',\n",
       " 'edsondomingues2',\n",
       " 'edsondomingues2',\n",
       " 'edsondomingues2',\n",
       " 'edsondomingues2',\n",
       " 'edsondomingues2',\n",
       " 'edsondomingues2',\n",
       " 'edsondomingues2',\n",
       " 'edsondomingues2',\n",
       " 'edsondomingues2',\n",
       " 'edsondomingues2',\n",
       " 'edsondomingues2',\n",
       " 'edsondomingues2',\n",
       " 'ClaraCamargoL',\n",
       " 'ClaraCamargoL',\n",
       " 'ClaraCamargoL',\n",
       " 'ClaraCamargoL',\n",
       " 'ClaraCamargoL',\n",
       " 'ClaraCamargoL',\n",
       " 'walisonsr',\n",
       " 'walisonsr',\n",
       " 'walisonsr',\n",
       " 'walisonsr',\n",
       " 'walisonsr',\n",
       " 'walisonsr',\n",
       " 'walisonsr',\n",
       " 'walisonsr',\n",
       " 'walisonsr',\n",
       " 'walisonsr',\n",
       " 'walisonsr',\n",
       " 'walisonsr',\n",
       " 'walisonsr',\n",
       " 'walisonsr',\n",
       " 'walisonsr',\n",
       " 'walisonsr',\n",
       " 'walisonsr',\n",
       " 'walisonsr',\n",
       " 'walisonsr',\n",
       " 'walisonsr',\n",
       " 'walisonsr',\n",
       " 'walisonsr',\n",
       " 'walisonsr',\n",
       " 'walisonsr',\n",
       " 'walisonsr',\n",
       " 'walisonsr',\n",
       " 'walisonsr',\n",
       " 'walisonsr',\n",
       " 'walisonsr',\n",
       " 'walisonsr',\n",
       " 'walisonsr',\n",
       " 'walisonsr',\n",
       " 'walisonsr',\n",
       " 'walisonsr',\n",
       " 'walisonsr',\n",
       " 'walisonsr',\n",
       " 'walisonsr',\n",
       " 'walisonsr',\n",
       " 'walisonsr',\n",
       " 'walisonsr',\n",
       " 'walisonsr',\n",
       " 'walisonsr',\n",
       " 'walisonsr',\n",
       " 'walisonsr',\n",
       " 'walisonsr',\n",
       " 'walisonsr',\n",
       " 'walisonsr',\n",
       " 'walisonsr',\n",
       " 'walisonsr',\n",
       " 'walisonsr',\n",
       " 'walisonsr',\n",
       " 'walisonsr',\n",
       " 'walisonsr',\n",
       " 'walisonsr',\n",
       " 'walisonsr',\n",
       " 'walisonsr',\n",
       " 'martinhagatosa',\n",
       " 'martinhagatosa',\n",
       " 'martinhagatosa',\n",
       " 'martinhagatosa',\n",
       " 'martinhagatosa',\n",
       " 'martinhagatosa',\n",
       " 'martinhagatosa',\n",
       " 'martinhagatosa',\n",
       " 'martinhagatosa',\n",
       " 'martinhagatosa',\n",
       " 'martinhagatosa',\n",
       " 'martinhagatosa',\n",
       " 'martinhagatosa',\n",
       " 'martinhagatosa',\n",
       " 'martinhagatosa',\n",
       " 'martinhagatosa',\n",
       " 'martinhagatosa',\n",
       " 'martinhagatosa',\n",
       " 'martinhagatosa',\n",
       " 'martinhagatosa',\n",
       " 'martinhagatosa',\n",
       " 'martinhagatosa',\n",
       " 'martinhagatosa',\n",
       " 'martinhagatosa',\n",
       " 'martinhagatosa',\n",
       " 'martinhagatosa',\n",
       " 'martinhagatosa',\n",
       " 'martinhagatosa',\n",
       " 'martinhagatosa',\n",
       " 'IgorYamada',\n",
       " 'IgorYamada',\n",
       " 'IgorYamada',\n",
       " 'IgorYamada',\n",
       " 'IgorYamada',\n",
       " 'IgorYamada',\n",
       " 'IgorYamada',\n",
       " 'IgorYamada',\n",
       " 'IgorYamada',\n",
       " 'IgorYamada',\n",
       " 'IgorYamada',\n",
       " 'IgorYamada',\n",
       " 'IgorYamada',\n",
       " 'IgorYamada',\n",
       " 'IgorYamada',\n",
       " 'IgorYamada',\n",
       " 'IgorYamada',\n",
       " 'IgorYamada',\n",
       " 'IgorYamada',\n",
       " 'IgorYamada',\n",
       " 'IgorYamada',\n",
       " 'IgorYamada',\n",
       " 'IgorYamada',\n",
       " 'IgorYamada',\n",
       " 'IgorYamada',\n",
       " 'IgorYamada',\n",
       " 'IgorYamada',\n",
       " 'IgorYamada',\n",
       " 'IgorYamada',\n",
       " 'IgorYamada',\n",
       " 'IgorYamada',\n",
       " 'IgorYamada',\n",
       " 'IgorYamada',\n",
       " 'IgorYamada',\n",
       " 'IgorYamada',\n",
       " 'IgorYamada',\n",
       " 'IgorYamada',\n",
       " 'IgorYamada',\n",
       " 'IgorYamada',\n",
       " 'IgorYamada',\n",
       " 'IgorYamada',\n",
       " 'IgorYamada',\n",
       " 'IgorYamada',\n",
       " 'IgorYamada',\n",
       " 'IgorYamada',\n",
       " 'IgorYamada',\n",
       " 'IgorYamada',\n",
       " 'IgorYamada',\n",
       " 'IgorYamada',\n",
       " 'IgorYamada',\n",
       " 'IgorYamada',\n",
       " 'IgorYamada',\n",
       " 'IgorYamada',\n",
       " 'IgorYamada',\n",
       " 'IgorYamada',\n",
       " 'IgorYamada',\n",
       " 'IgorYamada',\n",
       " 'IgorYamada',\n",
       " 'IgorYamada',\n",
       " 'IgorYamada',\n",
       " 'IgorYamada',\n",
       " 'IgorYamada',\n",
       " 'IgorYamada',\n",
       " 'IgorYamada',\n",
       " 'IgorYamada',\n",
       " 'IgorYamada',\n",
       " 'IgorYamada',\n",
       " 'IgorYamada',\n",
       " 'IgorYamada',\n",
       " 'IgorYamada',\n",
       " 'IgorYamada',\n",
       " 'IgorYamada',\n",
       " 'IgorYamada',\n",
       " 'IgorYamada',\n",
       " 'IgorYamada',\n",
       " 'IgorYamada',\n",
       " 'mcberinguy',\n",
       " 'mcberinguy',\n",
       " 'mcberinguy',\n",
       " 'mcberinguy',\n",
       " 'mcberinguy',\n",
       " 'mcberinguy',\n",
       " 'mcberinguy',\n",
       " 'mcberinguy',\n",
       " 'mcberinguy',\n",
       " 'mcberinguy',\n",
       " 'mcberinguy',\n",
       " 'mcberinguy',\n",
       " 'mcberinguy',\n",
       " 'mcberinguy',\n",
       " 'mcberinguy',\n",
       " 'mcberinguy',\n",
       " 'mcberinguy',\n",
       " 'mcberinguy',\n",
       " 'mcberinguy',\n",
       " 'mcberinguy',\n",
       " 'mcberinguy',\n",
       " 'mcberinguy',\n",
       " 'mcberinguy',\n",
       " 'mcberinguy',\n",
       " 'mcberinguy',\n",
       " 'mcberinguy',\n",
       " 'mcberinguy',\n",
       " 'mcberinguy',\n",
       " 'mcberinguy',\n",
       " 'mcberinguy',\n",
       " 'mcberinguy',\n",
       " 'mcberinguy',\n",
       " 'mcberinguy',\n",
       " 'mcberinguy',\n",
       " 'mcberinguy',\n",
       " 'mcberinguy',\n",
       " 'mcberinguy',\n",
       " 'mcberinguy',\n",
       " 'mcberinguy',\n",
       " 'mcberinguy',\n",
       " 'mcberinguy',\n",
       " 'mcberinguy',\n",
       " 'mcberinguy',\n",
       " 'mcberinguy',\n",
       " 'mcberinguy',\n",
       " 'mcberinguy',\n",
       " 'mcberinguy',\n",
       " 'mcberinguy',\n",
       " 'mcberinguy',\n",
       " 'mcberinguy',\n",
       " 'mcberinguy',\n",
       " 'mcberinguy',\n",
       " 'mcberinguy',\n",
       " 'mcberinguy',\n",
       " 'mcberinguy',\n",
       " 'mcberinguy',\n",
       " 'mcberinguy',\n",
       " 'mcberinguy',\n",
       " 'mcberinguy',\n",
       " 'mcberinguy',\n",
       " 'mcberinguy',\n",
       " 'mcberinguy',\n",
       " 'mcberinguy',\n",
       " 'mcberinguy',\n",
       " 'mcberinguy',\n",
       " 'mcberinguy',\n",
       " 'mcberinguy',\n",
       " 'mcberinguy',\n",
       " 'mcberinguy',\n",
       " 'mcberinguy',\n",
       " 'mcberinguy',\n",
       " 'mcberinguy',\n",
       " 'mcberinguy',\n",
       " 'mcberinguy',\n",
       " 'mcberinguy',\n",
       " 'mcberinguy',\n",
       " 'mcberinguy',\n",
       " 'mcberinguy',\n",
       " 'mcberinguy',\n",
       " 'mcberinguy',\n",
       " 'mcberinguy',\n",
       " 'mcberinguy',\n",
       " 'mcberinguy',\n",
       " 'mcberinguy',\n",
       " 'mcberinguy',\n",
       " 'mcberinguy',\n",
       " 'mcberinguy',\n",
       " 'mcberinguy',\n",
       " 'mcberinguy',\n",
       " 'mcberinguy',\n",
       " 'mcberinguy',\n",
       " 'mcberinguy',\n",
       " 'mcberinguy',\n",
       " 'mcberinguy',\n",
       " 'mcberinguy',\n",
       " 'mcberinguy',\n",
       " 'mcberinguy',\n",
       " 'mcberinguy',\n",
       " 'mcberinguy',\n",
       " 'mcberinguy',\n",
       " 'mcberinguy',\n",
       " 'mcberinguy',\n",
       " 'mcberinguy',\n",
       " 'mcberinguy',\n",
       " 'mcberinguy',\n",
       " 'mcberinguy',\n",
       " 'MandinhaSimone',\n",
       " 'MandinhaSimone',\n",
       " 'MandinhaSimone',\n",
       " 'MandinhaSimone',\n",
       " 'MandinhaSimone',\n",
       " 'MandinhaSimone',\n",
       " 'MandinhaSimone',\n",
       " 'MandinhaSimone',\n",
       " 'MandinhaSimone',\n",
       " 'MandinhaSimone',\n",
       " 'MandinhaSimone',\n",
       " 'MandinhaSimone',\n",
       " 'MandinhaSimone',\n",
       " 'MandinhaSimone',\n",
       " 'MandinhaSimone',\n",
       " 'MandinhaSimone',\n",
       " 'MandinhaSimone',\n",
       " 'MandinhaSimone',\n",
       " 'MandinhaSimone',\n",
       " 'MandinhaSimone',\n",
       " 'MandinhaSimone',\n",
       " 'MandinhaSimone',\n",
       " 'MandinhaSimone',\n",
       " 'MandinhaSimone',\n",
       " 'MandinhaSimone',\n",
       " 'MandinhaSimone',\n",
       " 'MandinhaSimone',\n",
       " 'MandinhaSimone',\n",
       " 'MandinhaSimone',\n",
       " 'MandinhaSimone',\n",
       " 'MandinhaSimone',\n",
       " 'MandinhaSimone',\n",
       " 'MandinhaSimone',\n",
       " 'MandinhaSimone',\n",
       " 'MandinhaSimone',\n",
       " 'MandinhaSimone',\n",
       " 'MandinhaSimone',\n",
       " 'MandinhaSimone',\n",
       " 'MandinhaSimone',\n",
       " 'MandinhaSimone',\n",
       " 'MandinhaSimone',\n",
       " 'MandinhaSimone',\n",
       " 'MandinhaSimone',\n",
       " 'MandinhaSimone',\n",
       " 'MandinhaSimone',\n",
       " 'MandinhaSimone',\n",
       " 'MandinhaSimone',\n",
       " 'MandinhaSimone',\n",
       " 'MandinhaSimone',\n",
       " 'MandinhaSimone',\n",
       " 'MandinhaSimone',\n",
       " 'MandinhaSimone',\n",
       " 'MandinhaSimone',\n",
       " 'MandinhaSimone',\n",
       " 'MandinhaSimone',\n",
       " 'MandinhaSimone',\n",
       " 'MandinhaSimone',\n",
       " 'MandinhaSimone',\n",
       " 'MandinhaSimone',\n",
       " 'MandinhaSimone',\n",
       " 'MandinhaSimone',\n",
       " 'MandinhaSimone',\n",
       " 'MandinhaSimone',\n",
       " 'MandinhaSimone',\n",
       " 'MandinhaSimone',\n",
       " 'MandinhaSimone',\n",
       " 'MandinhaSimone',\n",
       " 'MandinhaSimone',\n",
       " 'MandinhaSimone',\n",
       " 'MandinhaSimone',\n",
       " 'MandinhaSimone',\n",
       " 'Verazattar',\n",
       " 'Verazattar',\n",
       " 'Verazattar',\n",
       " 'Sandoval73',\n",
       " 'erasmo7silva',\n",
       " 'erasmo7silva',\n",
       " 'erasmo7silva',\n",
       " 'erasmo7silva',\n",
       " 'erasmo7silva',\n",
       " 'erasmo7silva',\n",
       " 'erasmo7silva',\n",
       " 'erasmo7silva',\n",
       " 'erasmo7silva',\n",
       " 'erasmo7silva',\n",
       " 'erasmo7silva',\n",
       " 'erasmo7silva',\n",
       " 'erasmo7silva',\n",
       " 'erasmo7silva',\n",
       " 'erasmo7silva',\n",
       " 'erasmo7silva',\n",
       " 'erasmo7silva',\n",
       " 'erasmo7silva',\n",
       " 'erasmo7silva',\n",
       " 'erasmo7silva',\n",
       " 'erasmo7silva',\n",
       " 'erasmo7silva',\n",
       " 'erasmo7silva',\n",
       " 'erasmo7silva',\n",
       " 'erasmo7silva',\n",
       " 'erasmo7silva',\n",
       " 'erasmo7silva',\n",
       " 'erasmo7silva',\n",
       " 'erasmo7silva',\n",
       " 'erasmo7silva',\n",
       " 'erasmo7silva',\n",
       " 'erasmo7silva',\n",
       " 'erasmo7silva',\n",
       " 'erasmo7silva',\n",
       " 'erasmo7silva',\n",
       " 'erasmo7silva',\n",
       " 'erasmo7silva',\n",
       " 'erasmo7silva',\n",
       " 'erasmo7silva',\n",
       " 'erasmo7silva',\n",
       " 'erasmo7silva',\n",
       " 'erasmo7silva',\n",
       " 'erasmo7silva',\n",
       " 'erasmo7silva',\n",
       " 'erasmo7silva',\n",
       " 'erasmo7silva',\n",
       " 'erasmo7silva',\n",
       " 'erasmo7silva',\n",
       " 'erasmo7silva',\n",
       " 'erasmo7silva',\n",
       " 'erasmo7silva',\n",
       " 'erasmo7silva',\n",
       " 'erasmo7silva',\n",
       " 'erasmo7silva',\n",
       " 'erasmo7silva',\n",
       " 'erasmo7silva',\n",
       " 'erasmo7silva',\n",
       " 'SANDRO_A',\n",
       " 'SANDRO_A',\n",
       " 'SANDRO_A',\n",
       " 'SANDRO_A',\n",
       " 'SANDRO_A',\n",
       " 'SANDRO_A',\n",
       " 'SANDRO_A',\n",
       " 'SANDRO_A',\n",
       " 'SANDRO_A',\n",
       " 'SANDRO_A',\n",
       " 'SANDRO_A',\n",
       " 'SANDRO_A',\n",
       " 'SANDRO_A',\n",
       " 'SANDRO_A',\n",
       " 'SANDRO_A',\n",
       " 'SANDRO_A',\n",
       " 'SANDRO_A',\n",
       " 'SANDRO_A',\n",
       " 'SANDRO_A',\n",
       " 'SANDRO_A',\n",
       " 'SANDRO_A',\n",
       " 'SANDRO_A',\n",
       " 'SANDRO_A',\n",
       " 'SANDRO_A',\n",
       " 'SANDRO_A',\n",
       " 'SANDRO_A',\n",
       " 'SANDRO_A',\n",
       " 'SANDRO_A',\n",
       " 'SANDRO_A',\n",
       " 'SANDRO_A',\n",
       " 'SANDRO_A',\n",
       " 'SANDRO_A',\n",
       " 'SANDRO_A',\n",
       " 'SANDRO_A',\n",
       " 'SANDRO_A',\n",
       " 'SANDRO_A',\n",
       " 'SANDRO_A',\n",
       " 'rafavnt',\n",
       " 'rafavnt',\n",
       " 'rafavnt',\n",
       " 'rafavnt',\n",
       " 'rafavnt',\n",
       " 'rafavnt',\n",
       " 'rafavnt',\n",
       " 'rafavnt',\n",
       " 'rafavnt',\n",
       " 'rafavnt',\n",
       " 'rafavnt',\n",
       " 'rafavnt',\n",
       " 'rafavnt',\n",
       " 'rafavnt',\n",
       " 'rafavnt',\n",
       " 'rafavnt',\n",
       " 'rafavnt',\n",
       " 'rafavnt',\n",
       " 'rafavnt',\n",
       " 'rafavnt',\n",
       " 'rafavnt',\n",
       " 'rafavnt',\n",
       " 'rafavnt',\n",
       " 'rafavnt',\n",
       " 'rafavnt',\n",
       " 'rafavnt',\n",
       " 'rafavnt',\n",
       " 'rafavnt',\n",
       " 'rafavnt',\n",
       " 'rafavnt',\n",
       " 'rafavnt',\n",
       " 'rafavnt',\n",
       " 'rafavnt',\n",
       " 'rafavnt',\n",
       " 'rafavnt',\n",
       " 'rafavnt',\n",
       " 'rafavnt',\n",
       " 'rafavnt',\n",
       " 'rafavnt',\n",
       " 'rafavnt',\n",
       " 'rafavnt',\n",
       " 'rafavnt',\n",
       " 'rafavnt',\n",
       " 'rafavnt',\n",
       " 'rafavnt',\n",
       " 'rafavnt',\n",
       " 'rafavnt',\n",
       " 'rafavnt',\n",
       " 'rafavnt',\n",
       " 'rafavnt',\n",
       " 'rafavnt',\n",
       " 'rafavnt',\n",
       " 'rafavnt',\n",
       " 'rafavnt',\n",
       " 'rafavnt',\n",
       " 'rafavnt',\n",
       " 'rafavnt',\n",
       " 'rafavnt',\n",
       " 'rafavnt',\n",
       " 'rafavnt',\n",
       " 'rafavnt',\n",
       " 'rafavnt',\n",
       " 'rafavnt',\n",
       " 'rafavnt',\n",
       " 'rafavnt',\n",
       " 'rafavnt',\n",
       " 'rafavnt',\n",
       " 'rafavnt',\n",
       " 'rafavnt',\n",
       " 'rafavnt',\n",
       " 'rafavnt',\n",
       " 'rafavnt',\n",
       " 'rafavnt',\n",
       " 'rafavnt',\n",
       " 'rafavnt',\n",
       " 'rafavnt',\n",
       " 'rafavnt',\n",
       " 'rafavnt',\n",
       " 'rafavnt',\n",
       " 'rafavnt',\n",
       " 'rafavnt',\n",
       " 'rafavnt',\n",
       " 'rafavnt',\n",
       " 'rafavnt',\n",
       " 'rafavnt',\n",
       " 'rafavnt',\n",
       " 'rafavnt',\n",
       " 'rafavnt',\n",
       " 'rafavnt',\n",
       " 'rafavnt',\n",
       " 'rafavnt',\n",
       " 'rafavnt',\n",
       " 'rafavnt',\n",
       " 'rafavnt',\n",
       " 'rafavnt',\n",
       " 'rafavnt',\n",
       " 'rafavnt',\n",
       " 'rafavnt',\n",
       " 'samanthaeandre',\n",
       " 'samanthaeandre',\n",
       " 'samanthaeandre',\n",
       " 'samanthaeandre',\n",
       " 'samanthaeandre',\n",
       " 'samanthaeandre',\n",
       " 'samanthaeandre',\n",
       " 'samanthaeandre',\n",
       " 'samanthaeandre',\n",
       " 'samanthaeandre',\n",
       " 'samanthaeandre',\n",
       " 'samanthaeandre',\n",
       " 'samanthaeandre',\n",
       " 'samanthaeandre',\n",
       " 'samanthaeandre',\n",
       " 'samanthaeandre',\n",
       " 'samanthaeandre',\n",
       " 'samanthaeandre',\n",
       " 'samanthaeandre',\n",
       " 'samanthaeandre',\n",
       " 'samanthaeandre',\n",
       " 'samanthaeandre',\n",
       " 'samanthaeandre',\n",
       " 'samanthaeandre',\n",
       " 'samanthaeandre',\n",
       " 'samanthaeandre',\n",
       " 'samanthaeandre',\n",
       " 'samanthaeandre',\n",
       " 'samanthaeandre',\n",
       " 'samanthaeandre',\n",
       " 'samanthaeandre',\n",
       " 'samanthaeandre',\n",
       " 'samanthaeandre',\n",
       " 'samanthaeandre',\n",
       " 'samanthaeandre',\n",
       " 'samanthaeandre',\n",
       " 'samanthaeandre',\n",
       " 'samanthaeandre',\n",
       " 'samanthaeandre',\n",
       " 'samanthaeandre',\n",
       " 'samanthaeandre',\n",
       " 'samanthaeandre',\n",
       " 'samanthaeandre',\n",
       " 'samanthaeandre',\n",
       " 'samanthaeandre',\n",
       " 'samanthaeandre',\n",
       " 'samanthaeandre',\n",
       " 'samanthaeandre',\n",
       " 'samanthaeandre',\n",
       " 'samanthaeandre',\n",
       " 'samanthaeandre',\n",
       " 'samanthaeandre',\n",
       " 'samanthaeandre',\n",
       " 'samanthaeandre',\n",
       " 'samanthaeandre',\n",
       " 'samanthaeandre',\n",
       " 'samanthaeandre',\n",
       " 'samanthaeandre',\n",
       " 'samanthaeandre',\n",
       " 'samanthaeandre',\n",
       " 'samanthaeandre',\n",
       " 'samanthaeandre',\n",
       " 'samanthaeandre',\n",
       " 'samanthaeandre',\n",
       " 'samanthaeandre',\n",
       " 'samanthaeandre',\n",
       " 'samanthaeandre',\n",
       " 'samanthaeandre',\n",
       " 'samanthaeandre',\n",
       " 'samanthaeandre',\n",
       " 'samanthaeandre',\n",
       " 'samanthaeandre',\n",
       " 'samanthaeandre',\n",
       " 'samanthaeandre',\n",
       " 'samanthaeandre',\n",
       " 'samanthaeandre',\n",
       " 'samanthaeandre',\n",
       " 'barroschitao',\n",
       " 'barroschitao',\n",
       " 'barroschitao',\n",
       " 'barroschitao',\n",
       " 'barroschitao',\n",
       " 'barroschitao',\n",
       " 'barroschitao',\n",
       " 'barroschitao',\n",
       " 'barroschitao',\n",
       " 'barroschitao',\n",
       " 'barroschitao',\n",
       " 'barroschitao',\n",
       " 'barroschitao',\n",
       " 'barroschitao',\n",
       " 'barroschitao',\n",
       " 'barroschitao',\n",
       " 'barroschitao',\n",
       " 'barroschitao',\n",
       " 'bangsppk',\n",
       " 'bangsppk',\n",
       " 'bangsppk',\n",
       " 'bangsppk',\n",
       " 'bangsppk',\n",
       " 'bangsppk',\n",
       " 'bangsppk',\n",
       " 'bangsppk',\n",
       " 'bangsppk',\n",
       " 'bangsppk',\n",
       " 'bangsppk',\n",
       " 'bangsppk',\n",
       " 'bangsppk',\n",
       " 'bangsppk',\n",
       " 'bangsppk',\n",
       " 'bangsppk',\n",
       " 'bangsppk',\n",
       " 'bangsppk',\n",
       " 'bangsppk',\n",
       " 'bangsppk',\n",
       " 'bangsppk',\n",
       " 'bangsppk',\n",
       " 'bangsppk',\n",
       " 'bangsppk',\n",
       " 'bangsppk',\n",
       " 'ggauchinho',\n",
       " 'ggauchinho',\n",
       " 'ggauchinho',\n",
       " 'ggauchinho',\n",
       " 'ggauchinho',\n",
       " 'ggauchinho',\n",
       " 'ggauchinho',\n",
       " 'ggauchinho',\n",
       " 'ggauchinho',\n",
       " 'ggauchinho',\n",
       " 'ggauchinho',\n",
       " 'ggauchinho',\n",
       " 'ggauchinho',\n",
       " 'ggauchinho',\n",
       " 'ggauchinho',\n",
       " 'ggauchinho',\n",
       " 'ggauchinho',\n",
       " 'ggauchinho',\n",
       " 'ggauchinho',\n",
       " 'ggauchinho',\n",
       " 'ggauchinho',\n",
       " 'ggauchinho',\n",
       " 'ggauchinho',\n",
       " 'ggauchinho',\n",
       " 'ggauchinho',\n",
       " 'ggauchinho',\n",
       " 'ggauchinho',\n",
       " 'ggauchinho',\n",
       " 'ggauchinho',\n",
       " 'ggauchinho',\n",
       " 'ggauchinho',\n",
       " 'ggauchinho',\n",
       " 'ggauchinho',\n",
       " 'ggauchinho',\n",
       " 'ggauchinho',\n",
       " 'ggauchinho',\n",
       " 'ggauchinho',\n",
       " 'ggauchinho',\n",
       " 'ggauchinho',\n",
       " 'ggauchinho',\n",
       " 'ggauchinho',\n",
       " 'ggauchinho',\n",
       " 'ggauchinho',\n",
       " 'ggauchinho',\n",
       " 'ggauchinho',\n",
       " 'ggauchinho',\n",
       " 'ggauchinho',\n",
       " 'ggauchinho',\n",
       " 'ggauchinho',\n",
       " 'ggauchinho',\n",
       " 'ggauchinho',\n",
       " 'ggauchinho',\n",
       " 'ggauchinho',\n",
       " 'ggauchinho',\n",
       " 'GusttavoGrossi',\n",
       " 'mariavaltenice',\n",
       " 'mariavaltenice',\n",
       " 'mariavaltenice',\n",
       " 'FredSteca',\n",
       " 'FredSteca',\n",
       " 'FredSteca',\n",
       " 'FredSteca',\n",
       " 'FredSteca',\n",
       " 'casa_dos_pobres',\n",
       " 'casa_dos_pobres',\n",
       " 'casa_dos_pobres',\n",
       " 'casa_dos_pobres',\n",
       " 'bemdeboavsf',\n",
       " 'bemdeboavsf',\n",
       " 'bemdeboavsf',\n",
       " 'bemdeboavsf',\n",
       " 'bemdeboavsf',\n",
       " 'bemdeboavsf',\n",
       " 'bemdeboavsf',\n",
       " 'bemdeboavsf',\n",
       " 'bemdeboavsf',\n",
       " 'bemdeboavsf',\n",
       " 'bemdeboavsf',\n",
       " 'bemdeboavsf',\n",
       " 'bemdeboavsf',\n",
       " 'bemdeboavsf',\n",
       " 'bemdeboavsf',\n",
       " 'bemdeboavsf',\n",
       " 'bemdeboavsf',\n",
       " 'bemdeboavsf',\n",
       " 'bemdeboavsf',\n",
       " 'bemdeboavsf',\n",
       " 'bemdeboavsf',\n",
       " 'bemdeboavsf',\n",
       " 'bemdeboavsf',\n",
       " 'bemdeboavsf',\n",
       " 'bemdeboavsf',\n",
       " 'bemdeboavsf',\n",
       " 'bemdeboavsf',\n",
       " 'bemdeboavsf',\n",
       " 'bemdeboavsf',\n",
       " 'bemdeboavsf',\n",
       " 'bemdeboavsf',\n",
       " 'bemdeboavsf',\n",
       " 'bemdeboavsf',\n",
       " 'bemdeboavsf',\n",
       " 'bemdeboavsf',\n",
       " 'bemdeboavsf',\n",
       " 'bemdeboavsf',\n",
       " 'bemdeboavsf',\n",
       " 'bemdeboavsf',\n",
       " 'bemdeboavsf',\n",
       " 'bemdeboavsf',\n",
       " 'bemdeboavsf',\n",
       " 'bemdeboavsf',\n",
       " 'bemdeboavsf',\n",
       " 'bemdeboavsf',\n",
       " 'bemdeboavsf',\n",
       " 'bemdeboavsf',\n",
       " 'bemdeboavsf',\n",
       " 'bemdeboavsf',\n",
       " 'bemdeboavsf',\n",
       " 'bemdeboavsf',\n",
       " 'bemdeboavsf',\n",
       " 'bemdeboavsf',\n",
       " 'bemdeboavsf',\n",
       " 'bemdeboavsf',\n",
       " 'bemdeboavsf',\n",
       " 'bemdeboavsf',\n",
       " 'bemdeboavsf',\n",
       " 'bemdeboavsf',\n",
       " 'bemdeboavsf',\n",
       " 'bemdeboavsf',\n",
       " 'bemdeboavsf',\n",
       " 'bemdeboavsf',\n",
       " 'bemdeboavsf',\n",
       " 'bemdeboavsf',\n",
       " 'bemdeboavsf',\n",
       " 'bemdeboavsf',\n",
       " 'bemdeboavsf',\n",
       " 'bemdeboavsf',\n",
       " 'bemdeboavsf',\n",
       " 'bemdeboavsf',\n",
       " 'bemdeboavsf',\n",
       " 'bemdeboavsf',\n",
       " 'bemdeboavsf',\n",
       " 'bemdeboavsf',\n",
       " 'bemdeboavsf',\n",
       " 'bemdeboavsf',\n",
       " 'bemdeboavsf',\n",
       " 'bemdeboavsf',\n",
       " 'bemdeboavsf',\n",
       " 'bemdeboavsf',\n",
       " 'bemdeboavsf',\n",
       " 'bemdeboavsf',\n",
       " 'bemdeboavsf',\n",
       " 'bemdeboavsf',\n",
       " 'bemdeboavsf',\n",
       " 'bemdeboavsf',\n",
       " 'bemdeboavsf',\n",
       " 'bemdeboavsf',\n",
       " 'bemdeboavsf',\n",
       " 'bemdeboavsf',\n",
       " 'bemdeboavsf',\n",
       " 'bemdeboavsf',\n",
       " 'bemdeboavsf',\n",
       " 'bemdeboavsf',\n",
       " 'bemdeboavsf',\n",
       " 'bemdeboavsf',\n",
       " 'bemdeboavsf',\n",
       " 'bemdeboavsf',\n",
       " 'bemdeboavsf',\n",
       " 'bemdeboavsf',\n",
       " 'bemdeboavsf',\n",
       " 'bemdeboavsf',\n",
       " 'bemdeboavsf',\n",
       " 'bemdeboavsf',\n",
       " 'bemdeboavsf',\n",
       " 'bemdeboavsf',\n",
       " 'bemdeboavsf',\n",
       " 'bemdeboavsf',\n",
       " 'bemdeboavsf',\n",
       " 'bemdeboavsf',\n",
       " 'bemdeboavsf',\n",
       " 'bemdeboavsf',\n",
       " 'bemdeboavsf',\n",
       " 'bemdeboavsf',\n",
       " 'bemdeboavsf',\n",
       " 'bemdeboavsf',\n",
       " 'bemdeboavsf',\n",
       " 'bemdeboavsf',\n",
       " 'bemdeboavsf',\n",
       " 'bemdeboavsf',\n",
       " 'bemdeboavsf',\n",
       " 'bemdeboavsf',\n",
       " 'bemdeboavsf',\n",
       " 'bemdeboavsf',\n",
       " 'SONIARMALHEIROS',\n",
       " 'SONIARMALHEIROS',\n",
       " 'SONIARMALHEIROS',\n",
       " 'fernandasdn',\n",
       " 'fernandasdn',\n",
       " 'fernandasdn',\n",
       " 'fernandasdn',\n",
       " 'fernandasdn',\n",
       " 'fernandasdn',\n",
       " 'fernandasdn',\n",
       " 'fernandasdn',\n",
       " 'fernandasdn',\n",
       " 'fernandasdn',\n",
       " 'fernandasdn',\n",
       " 'fernandasdn',\n",
       " 'fernandasdn',\n",
       " 'fernandasdn',\n",
       " 'fernandasdn',\n",
       " 'fernandasdn',\n",
       " 'fernandasdn',\n",
       " 'fernandasdn',\n",
       " 'fernandasdn',\n",
       " 'fernandasdn',\n",
       " 'fernandasdn',\n",
       " 'fernandasdn',\n",
       " 'fernandasdn',\n",
       " 'fernandasdn',\n",
       " 'fernandasdn',\n",
       " 'fernandasdn',\n",
       " 'fernandasdn',\n",
       " 'fernandasdn',\n",
       " 'fernandasdn',\n",
       " 'fernandasdn',\n",
       " 'fernandasdn',\n",
       " 'fernandasdn',\n",
       " 'fernandasdn',\n",
       " 'fernandasdn',\n",
       " 'fernandasdn',\n",
       " 'fernandasdn',\n",
       " 'fernandasdn',\n",
       " 'fernandasdn',\n",
       " 'fernandasdn',\n",
       " 'fernandasdn',\n",
       " 'fernandasdn',\n",
       " 'fernandasdn',\n",
       " 'fernandasdn',\n",
       " 'fernandasdn',\n",
       " 'fernandasdn',\n",
       " 'fernandasdn',\n",
       " 'fernandasdn',\n",
       " 'fernandasdn',\n",
       " 'fernandasdn',\n",
       " 'fernandasdn',\n",
       " 'fernandasdn',\n",
       " 'fernandasdn',\n",
       " 'fernandasdn',\n",
       " 'fernandasdn',\n",
       " 'fernandasdn',\n",
       " 'fernandasdn',\n",
       " 'fernandasdn',\n",
       " 'fernandasdn',\n",
       " 'fernandasdn',\n",
       " 'fernandasdn',\n",
       " 'fernandasdn',\n",
       " 'fernandasdn',\n",
       " 'fernandasdn',\n",
       " 'fernandasdn',\n",
       " 'fernandasdn',\n",
       " 'fernandasdn',\n",
       " 'fernandasdn',\n",
       " 'fernandasdn',\n",
       " 'fernandasdn',\n",
       " 'fernandasdn',\n",
       " 'fernandasdn',\n",
       " 'fernandasdn',\n",
       " 'fernandasdn',\n",
       " 'fernandasdn',\n",
       " 'fernandasdn',\n",
       " 'fernandasdn',\n",
       " 'fernandasdn',\n",
       " 'fernandasdn',\n",
       " 'fernandasdn',\n",
       " 'fernandasdn',\n",
       " 'fernandasdn',\n",
       " 'fernandasdn',\n",
       " 'fernandasdn',\n",
       " 'fernandasdn',\n",
       " 'fernandasdn',\n",
       " 'fernandasdn',\n",
       " 'fernandasdn',\n",
       " 'fernandasdn',\n",
       " 'fernandasdn',\n",
       " 'fernandasdn',\n",
       " 'fernandasdn',\n",
       " 'fernandasdn',\n",
       " 'fernandasdn',\n",
       " 'fernandasdn',\n",
       " 'fernandasdn',\n",
       " 'fernandasdn',\n",
       " 'fernandasdn',\n",
       " 'fernandasdn',\n",
       " 'fernandasdn',\n",
       " 'fernandasdn',\n",
       " 'fernandasdn',\n",
       " ...]"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "allusers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
